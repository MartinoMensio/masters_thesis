<html>
 <head>
  <meta content="text/html; charset=utf-8" http-equiv="content-type"/>
 </head>
 <body>
  <h1 id="h.r79qs0qeni3s">
   <span>
    Introduction
   </span>
  </h1>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <hr style="page-break-before:always;display:none;"/>
  <p>
   <span>
   </span>
  </p>
  <h1 id="h.15cgqq29u8bm">
   <span>
    State of the Art
   </span>
  </h1>
  <h2 id="h.43fer8dd1rjh">
   <span>
    Bots and autonomous systems
   </span>
  </h2>
  <p>
   <span>
    “Turing’s red flag”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Problem: AI mistaken for human
    </span>
   </li>
   <li>
    <span>
     Proposition:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     autonomous system should be designed in a way to make clear that is not a human
    </span>
   </li>
   <li>
    <span>
     should identify as autonomous system at the start of new interactions
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Cases:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Self-driving car: should be recognizable in order to avoid accidents and to allow other drivers (autonomous or not) to behave in proper way
    </span>
   </li>
   <li>
    <span>
     Virtual assistants: pretending to be human is a dangerous precedent. Now it is clear that they are AI, but with technological progress the difference could become unnoticed
    </span>
   </li>
   <li>
    <span>
     Online games: bots can have some advantages and disadvantages, but user should know what kind of player they belong to
    </span>
   </li>
   <li>
    <span>
     Computer-generated text: depending on the domain, can impact emotions of reader
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “The rise of social bots”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Problem: social bots can be harmful. Beyond the problem of veracity of information, they can gain influence and become popular
    </span>
   </li>
   <li>
    <span>
     Cases:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Political influence: audience is artificially enlarged
    </span>
   </li>
   <li>
    <span>
     Market influence: fake informations are amplified without fact-checking
    </span>
   </li>
   <li>
    <span>
     Exposing private informations: this makes people not to trust social media
    </span>
   </li>
   <li>
    <span>
     Manipulate emotions: on socials, they are contagious
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Proposition: detect bots
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Auto-reposting: easy to spot with posts/tweets without sense
    </span>
   </li>
   <li>
    <span>
     More advanced bots: emulating human behavior, filling profiles with data that seems legit, interacting actively with other users. In some cases they also clone profiles of real users
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Implementation types:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Based on social media information (graph-based): “sybil” accounts are prone to have more connections with other sybils -&gt; groups. Based on assumption that legitimate users refuse to interact with unknown accounts, but this is not so true
    </span>
   </li>
   <li>
    <span>
     Based on crowdsourcing: humans analyze profiles to detect bots, and majority voting is applied (same profiles shown to different workers)
    </span>
   </li>
   <li>
    <span>
     Machine-learning: find features that are significant for discrimination. The system analyzes a set of features (network, user, friends, timing, content, sentiment) to evaluate a score using cross validation. Not able to detect cyborgs (mixture of human and bot) and hacked accounts
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Is that a bot running the social media feed?”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: understand if users perceive differently a bot agent or a human agent
    </span>
   </li>
   <li>
    <span>
     Results: bots are perceived credible, attractive and competent as humans. The cause is that users use the same way of interacting with bots and with humans. However the attraction to the human agent is higher
    </span>
   </li>
   <li>
    <span>
     Limitation: very restricted context
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Towards the implementation of a Topic specific dialogue based Natural Language Chatbot as an Undergraduate Advisor”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: develop a chatbot to help students in University with admission and course information (FAQ) using AIML. Weights of state transitions are tuned to make the conversation stay on topic.
    </span>
   </li>
   <li>
    <span>
     Metrics for performance:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Satisfaction: user vote if the answer is appropriate in the context
    </span>
   </li>
   <li>
    <span>
     Topic switching rate: how much the dialogue is switched from on-topic to off-topic
    </span>
   </li>
   <li>
    <span>
     Correction rate: responses that are corrected by the user
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Can we control it? Autonomous robots threaten human identity, uniqueness, safety, and resources”
   </span>
   <sup>
    <a href="#ftnt1" id="ftnt_ref1">
     [1]
    </a>
   </sup>
  </p>
  <ul>
   <li>
    <span>
     Proposition: analyze the impact of autonomous systems on society, how they are perceived
    </span>
   </li>
   <li>
    <span>
     Problems of autonomous system:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Humans want to always have control and power. The system’s autonomy should always be bounded: only automatically perform specific tasks as requested. However in some cases there can be conflicts between what the user asks and the principle the system was designed with. This should be the only situation in which the system must disregard a command. If the system decides things on his own, also without conflicting with commands, this is seen as a threat to human superiority on machines. The hierarchy of society is threatened
    </span>
   </li>
   <li>
    <span>
     Realistic threats: menacing safety, wellbeing or material resources
    </span>
   </li>
   <li>
    <span>
     Symbolic threats: maintain the difference between human and machine. The concept of identity and distinctiveness are threatened
    </span>
   </li>
   <li>
    <span>
     Both kinds of threat lead to negative attitudes towards robots and robotics research
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “An Ergonomics Evaluation to Chatbot Equipped with Knowledge-Rich Mind”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: how to incorporate knowledge in chatbot systems
    </span>
   </li>
   <li>
    <span>
     Implementation:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Combining more approaches together. AIML (common dialog questions, containing a lot of stop words), Regular Matcher Component (for simple commands), FAQ (translating sentences into queries, selecting best match), NLP
    </span>
   </li>
  </ul>
  <h3 id="h.d7tjvkxrgg7m">
   <span>
    Guidelines
   </span>
  </h3>
  <p>
   <span>
    Define boundaries on:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Autonomy of the system: smart system should be able to decide things in its domain because a lot of information can be used to take decision, doing work for the users. Some limits need not to be overtaken, in order not to threat the hierarchy of society. Machines are tools for human well being
    </span>
   </li>
   <li>
    <span>
     Distinguishability: according to “red flag law”, system should clearly be recognizable as robot. However for usability the interaction with users should be similar to human-to-human
    </span>
   </li>
   <li>
    <span>
     Personality: having an artificial intelligence with some personality makes interactions more interesting and seamless for users. A boundary needs to be defined in order to avoid emotional attachment and other inappropriate feelings towards machines. From social point of view, people need to keep their life in real world with real people
    </span>
   </li>
  </ul>
  <hr style="page-break-before:always;display:none;"/>
  <p>
   <span>
   </span>
  </p>
  <h2 id="h.1t8dpluzlftb">
   <span>
    Types of bot
   </span>
  </h2>
  <p>
   <span>
    Dimensions to consider:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Long or short conversations
    </span>
   </li>
   <li>
    <span>
     Open or closed domain. Closed is the only feasible
    </span>
   </li>
  </ul>
  <p>
   <span>
    Important features / challenges:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Context: keep track of informations given by user
    </span>
   </li>
   <li>
    <span>
     Coherent personality: since training is done on multiple users data (see “A persona-based neural conversation model”)
    </span>
   </li>
   <li>
    <span>
     Evaluation (see “How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation”). May use recall@k
    </span>
   </li>
   <li>
    <span>
     Intention and diversity: easy to fall into generic responses (see “A Diversity-Promoting Objective Function for Neural Conversation Models”) especially for open-domain systems
    </span>
   </li>
  </ul>
  <h3 id="h.13wjijx5nwux">
   <span>
    Retrieval-based models
   </span>
  </h3>
  <p>
   <span>
    This kind of bots use a set of predefined response templates and apply some heuristics in order to select the most suitable one. The selection criterion can be:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Simple rule-based (as in AIML)
    </span>
   </li>
   <li>
    <span>
     More complex with machine learning classifiers
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Advantages:
   </span>
  </p>
  <ul>
   <li>
    <span>
     No grammatical mistakes
    </span>
   </li>
  </ul>
  <p>
   <span>
    Disadvantages:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Unseen cases cannot be handled
    </span>
   </li>
  </ul>
  <h4 id="h.lxbz31wv66qa">
   <span>
    AIML
   </span>
  </h4>
  <p>
   <span>
    Is a language that describes how the bot replies to certain inputs. Inputs are evaluated against a set of patterns and the best match (category) is chosen. The actions performed by the category can be a simple response, or can also set variables and call other categories.
   </span>
  </p>
  <p>
   <span>
    Bot creation:
   </span>
  </p>
  <p>
   <span>
    Cyclical process called
   </span>
   <span>
    targeting
   </span>
   <span>
    : client inputs that find no match are collected using logs and the botmaster creates suitable responses. Targeting interface can also be exposed to clients, but with risks: not controlled by the botmaster
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    AIML can be used to build bots that simply do a pattern-matching job, it is only a stimulus-response system.
   </span>
  </p>
  <p>
   <span>
    Main disadvantages:
   </span>
  </p>
  <ul>
   <li>
    <span>
     A big set of AIML rules need to be built: time consuming. A lot of rules are also needed to perform reduction
    </span>
   </li>
   <li>
    <span>
     Difficult to reply to complex queries
    </span>
   </li>
   <li>
    <span>
     Not really understanding the language.
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Where it is used:
   </span>
  </p>
  <ul>
   <li>
    <span>
     ALICE based bots: Pandorabots is a service to host an AIML processor. Users can create files that contain the rules and test the bot
    </span>
   </li>
   <li>
    <span>
     <a href="https://github.com/keiffster/program-y">
      https://github.com/keiffster/program-y
     </a>
    </span>
    <span>
     is an implementation of AIML 2.0 using python 3
    </span>
   </li>
  </ul>
  <h4 id="h.cfauph7t5emx">
   <span>
    Machine learning
   </span>
  </h4>
  <p>
   <span>
    Data sets:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Ubuntu dialogue corpus:
    </span>
    <span>
     <a href="https://github.com/rkadlec/ubuntu-ranking-dataset-creator">
      https://github.com/rkadlec/ubuntu-ranking-dataset-creator
     </a>
    </span>
    <span>
    </span>
   </li>
   <li>
    <span>
     Twitter
    </span>
   </li>
  </ul>
  <p>
   <span>
    Predictor:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Random. Very poor performance
    </span>
   </li>
   <li>
    <span>
     Using tf-idf: how important a word is important in a document. Documents with similar content have similar tf-idf vectors. Responses don’t need to be similar to the context to be correct
    </span>
   </li>
   <li>
    <span>
     Deep learning: dual-encoder LSTM network / seq2seq. Both are recurrent neural networks
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Examples:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Google smart reply
    </span>
    <span>
     <a href="http://arxiv.org/abs/1606.04870">
      http://arxiv.org/abs/1606.04870
     </a>
    </span>
    <span>
    </span>
   </li>
  </ul>
  <h3 id="h.3y4hjed0zgjs">
   <span>
    Generative models
   </span>
  </h3>
  <p>
   <span>
    Real artificial intelligence, understanding the language. They don’t use pre-defined responses, but generate them from scratch. The technology is based on translation techniques.
   </span>
  </p>
  <p>
   <span>
    Advantages:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Can refer back to entities mentioned in the conversation
    </span>
   </li>
  </ul>
  <p>
   <span>
    Disadvantages:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Grammatical mistakes can occur
    </span>
   </li>
   <li>
    <span>
     Require huge amounts of training data
    </span>
   </li>
   <li>
    <span>
     Early stage of development
    </span>
   </li>
   <li>
    <span>
     Can go off the rails (example of Microsoft Tay)
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <h2 id="h.n06dhtaupqp">
   <span>
    NLP
   </span>
  </h2>
  <p>
   <span>
    The most important part is to understand the user message. Instead to do a simple rule-matching, there are libraries that can process sentences and extract their structure, together with named entities.
   </span>
  </p>
  <p>
   <span>
    The pipeline on input should contain:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Spell check
    </span>
   </li>
   <li>
    <span>
     Tokenize (sentences, words)
    </span>
   </li>
   <li>
    <span>
     POS recognition
    </span>
   </li>
   <li>
    <span>
     Lemmatize (reconduct to base form) and reduce synonyms
    </span>
   </li>
   <li>
    <span>
     Entity recognition
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Once the input is processed, there should be a logic that based on the informations extracted (entities and relationships) understands the
   </span>
   <span>
    root-level intent
   </span>
   <span>
    of the person and decides what actions need to be performed:
   </span>
  </p>
  <ol start="1">
   <li>
    <span>
     Question classification
    </span>
   </li>
   <li>
    <span>
     Information retrieval
    </span>
   </li>
   <li>
    <span>
     Answer extraction
    </span>
   </li>
  </ol>
  <p>
   <span>
    NLP frameworks:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Stanford CoreNLP
    </span>
    <span>
     <a href="http://stanfordnlp.github.io/CoreNLP/">
      http://stanfordnlp.github.io/CoreNLP/
     </a>
    </span>
    <span>
     : java
    </span>
   </li>
   <li>
    <span>
     NLTK: wide variety of algorithms and stemmers. Can finely customize the model. String based
    </span>
   </li>
   <li>
    <span>
     <a href="https://github.com/sloria/TextBlob">
      https://github.com/sloria/TextBlob
     </a>
    </span>
    <span>
     : based on NLTK
    </span>
   </li>
   <li>
    <span>
     <a href="https://github.com/explosion/spaCy">
      https://github.com/explosion/spaCy
     </a>
    </span>
    <span>
     : faster, but a bit lower quality. A single stemmer. Object oriented
    </span>
   </li>
   <li>
    <span>
     Google syntaxnet
    </span>
    <span>
     <a href="https://github.com/tensorflow/models/tree/master/syntaxnet">
      https://github.com/tensorflow/models/tree/master/syntaxnet
     </a>
    </span>
    <span>
     . A tensorflow model that is the most accurate parser. Also able to parse correctly some garden-paths
    </span>
   </li>
  </ul>
  <p>
   <span>
    Comparison between NLTK and spaCy
   </span>
   <span>
    <a href="https://gist.github.com/rschroll/61b20c41e984a963df2870cfc9e628ed">
     https://gist.github.com/rschroll/61b20c41e984a963df2870cfc9e628ed
    </a>
   </span>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    A concept of the bot has been developed using wit.ai for doing intent detection and entity extraction. This can be used as a reference.
   </span>
  </p>
  <h3 id="h.7bar53uylr2x">
   <span>
    Beyond NLP: NLU
   </span>
  </h3>
  <p>
   <span>
    Capturing also the semantics and pragmatics is important
   </span>
  </p>
  <hr style="page-break-before:always;display:none;"/>
  <h2 id="h.bhhmsohw8kph">
   <span>
    Personalization/Recommendation
   </span>
  </h2>
  <p>
   <span>
    “Computer-based personality judgments are more accurate than those made by humans”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: compare accuracy of personality judgment done by computers against those made by humans. Using likes to judge five traits: openness, agreeableness, extraversion, conscientiousness and neuroticism
    </span>
   </li>
   <li>
    <span>
     Criterions:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Self-other agreement: how much external judger (computer or human) agrees with self-rating
    </span>
   </li>
   <li>
    <span>
     Interjudge agreement: similarity of rating given by two external judger (human or computer-&gt;training set divided in two parts)
    </span>
   </li>
   <li>
    <span>
     External validity: measuring the prediction on life outcomes (facts that can be verified) based on the traits of personality computed (computer) or self-assigned (human)
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Results: computer are more accurate. But there are some differences:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Computer-based judgments can take into account a very big quantity of data and use statistical modeling
    </span>
   </li>
   <li>
    <span>
     Humans can capture more subtle cues
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Private traits and attributes are predictable from digital records of human behavior”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: using data of social network profiles (user likes) analyze the prediction rate of non-published personal data -&gt; 10-fold cross-validation
    </span>
   </li>
   <li>
    <span>
     Results: the majority of informations are accurately predicted. Especially the ethnicity, gender, age. The accuracy increases with the number of likes available, but even with a small data set, the prediction is accurate for some attributes
    </span>
   </li>
   <li>
    <span>
     Implications of being highly predictive:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Recommendation services can be improved, without explicitly asking some information but inferring them
    </span>
   </li>
   <li>
    <span>
     Unwilling use of details that user wanted to hide, considered as personal. This implies a decrease in trust of online services
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Share Like Recommend”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: analyze news as social activity
    </span>
   </li>
   <li>
    <span>
     Results:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     users receive news from friends on social networks, a lot more than directly from news agencies. Friends act like a filter, by sharing news.
    </span>
   </li>
   <li>
    <span>
     News as a shared social experience is appealing for customers. Main reason for using social networks is sharing content: events, news
    </span>
   </li>
   <li>
    <span>
     Journalists on social network seem to be important for users
    </span>
   </li>
   <li>
    <span>
     Using social media for retrieving news is not done at expenses of mainstream media outlets
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Tag-Based User Profiling for Social Media Recommendation”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: use tags to support user profiling. Tags connect entities directly, while collaborative filtering must search for relationships between user preferences and item attribute
    </span>
   </li>
   <li>
    <span>
     Implementation:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Types of tags (have different weights):
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Personal view: tags specified by the user
    </span>
   </li>
   <li>
    <span>
     Social view: tags specified by the friends on the user contents
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Tag-to-Tag Matrix: correlations between person-attribute set (tags on user) and content-attribute set (tags on contents)
    </span>
   </li>
   <li>
    <span>
     User-feature vector: for each idea mentioned in a specific content, the probability that the user has to like it. If it is above a threshold, it is recommended
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Experiment (using social bookmarking site del.icio.us):
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     A long-tail model applies (URLs are bookmarked by few people). Very low precision (2.77%) because there are not central topics to be defined. Also caused by dependence on order of visit (if user visits a better site before, decision about bookmarking a site on the same topic is lower)
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Cross Social Media Recommendation”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: merge data from different social networks (Twitter and Weibo) to remove biasing of models based on a single social network. Recommend twitter hashtag to weibo
    </span>
   </li>
   <li>
    <span>
     Implementation:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     “Pseudo Global Social Media Network (PGSMN) model to interconnect users (with similar interests, not the same user because the sets are separate) and topics
    </span>
   </li>
   <li>
    <span>
     Three layers: Twitter, Weibo and Wikipedia. The last one is a bridge. Each layer contains intra-layer links (users to tags or page to category) and inter-layer links (tags to pages/category)
    </span>
   </li>
   <li>
    <span>
     Explicit Semantic Path Mining (ESPM), derived from Explicit Semantic Analysis (ESA), identifies semantic paths from Wikipedia hierarchical relationships
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Measurements of ranking performance:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Precision
    </span>
   </li>
   <li>
    <span>
     Mean average precision
    </span>
   </li>
   <li>
    <span>
     Normalized discounted cumulative gain
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <h3 id="h.tm5ee3a05f8l">
   <span>
    Accessing user data on facebook (discarding it for the moment)
   </span>
  </h3>
  <p>
   <span>
    The data that is available directly from the messenger API is the following: first_name,last_name,profile_pic,locale,timezone,gender,is_payment_enabled
   </span>
  </p>
  <p>
   <span>
    In order to be able to do personalization, there exists a way to link the messenger id (page-scoped) to other ids (also facebook id indirectly):
   </span>
   <span>
    <a href="https://developers.facebook.com/docs/messenger-platform/account-linking">
     https://developers.facebook.com/docs/messenger-platform/account-linking
    </a>
   </span>
  </p>
  <p>
   <span>
    Once the facebook id is retrieved, it is possible to use the facebook graph API.
   </span>
  </p>
  <p>
   <span>
    To the external account the messenger platform sends a request to an url specified by the developer, adding account_linking_token and redirect_uri (page to redirect user after login). The external site, if login successful, redirects to redirect_uri with authentication_code custom (maybe put there the id in order to allow the bot to do the join)
   </span>
  </p>
  <p>
   <span>
    From the other side, the oauth flow:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Redirect user to facebook oauth with client_id (the app id) and redirect_uri (where facebook will send the user)
    </span>
   </li>
   <li>
    <span>
     After that user gives permissions, fb will redirect to redirect_uri with other parameters
    </span>
   </li>
  </ul>
  <p>
   <span>
    The procedure seems quite complicated and requires a web server component that interacts on one side with messenger platform and on the other side handles the facebook login.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h3 id="h.gjp0hmp20g33">
   <span>
    Cold start
   </span>
  </h3>
  <p>
   <span>
    Another possible approach, without using the facebook data, would be to use a bootstrap in the
   </span>
   <span>
    cold start
   </span>
   <span>
    phase: a few questions to build a first model of the user. Then more informations can be
   </span>
   <span>
    extracted as the conversation flows.
   </span>
  </p>
  <p>
   <span>
    An explicit preference elicitation at the beginning, not too long, that then becomes implicit.
   </span>
  </p>
  <p>
   <span>
    Other systems apply this procedure (e.g. netflix)
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Facing the cold start problem in recommender systems”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: find a good solution for the user-side cold start
    </span>
   </li>
   <li>
    <span>
     Other approaches to the problem:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Content-based: requires ratings by user. Cold start problem
    </span>
   </li>
   <li>
    <span>
     Collaborative-filtering: requires other users ratings
    </span>
   </li>
   <li>
    <span>
     Explicit interview to new user about items (adapting new questions to the answers provided)
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Implementation: three phases (Collaborative-filtering)
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Classification of the new user in a specific group (based on demographic data): using C4.5 algorithm (decision tree) and Naive Bayes
    </span>
   </li>
   <li>
    <span>
     Find “neighbours” of the new user inside the group: weighted average of demographic data
    </span>
   </li>
   <li>
    <span>
     Calculation of outcome: prediction techniques
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Dealing with the new user cold-start problem in recommender systems: A comparative review”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: compare existing algorithms (collaborative filtering) on the cold-start problem. Types of systems:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Uses additional data sources (e.g. demographic data): a limitation of these systems is that sometimes data is not available (because user did not associate social profile)
    </span>
   </li>
   <li>
    <span>
     Selects a group of analogous users (without additional data sources): construct a decision tree where nodes are questions. NHSM also takes into account the global preference of user behaviors, using three factors of similarity: Proximity (how much two ratings are near), Significance (how much distant from the median) and Singularity (how the two ratings are different from others). Limitations: how to choose the optimal number of groups and splitting criteria. Must have some rating from new user (bootstrap)
    </span>
   </li>
   <li>
    <span>
     Hybrid methods:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     FARAMS: using multiple approaches (fuzzy sets and association rules).
    </span>
   </li>
   <li>
    <span>
     HU-FCF: combines analysis on demographic data (fuzzy similarity matrix) and on rating data (hard similarity matrix)
    </span>
   </li>
   <li>
    <span>
     Limitations: irrelevant users are still included in the computation of similarities
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Results: NHSM is the best one, both for accuracy and for computational time
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Learning multiple-question decision trees for cold-start recommendation”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: find a solution to the cold-start recommendation problem that maximizes accuracy and minimizes user efforts. Problem of classic bootstrap is that user usually does not know items in the first interactions. The idea is to build a tree with multiple questions at each node.
    </span>
   </li>
   <li>
    <span>
     Results:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Accuracy is better than single-question based system (because for each node/page more informations are extracted) and also better than linear-combination of multiple trees
    </span>
   </li>
   <li>
    <span>
     Time increment to answer more questions per screen is sublinear
    </span>
   </li>
   <li>
    <span>
     Time difference between rating scale and binary answer doesn’t have strong dependency on the number of questions
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Matrix Factorization Techniques for Recommender Systems”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Collaborative filtering Approaches:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Neighborhood methods:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     User-oriented: find similar users to the target user, and from them find the items that they like
    </span>
   </li>
   <li>
    <span>
     Product-oriented: find items similar to the ones liked by the target user
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Latent factor models: characterize items and users on some factors (user/item vector) inferred from rating patterns (explicit or implicit feedback). Predicted rating is the dot product between the two vectors
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Learning algorithms for extracting the factor vectors:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Stochastic gradient descent: easier and faster
    </span>
   </li>
   <li>
    <span>
     Alternating least squares: can be parallelized, better on sparse data
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Temporal dynamics: the model should be dynamic because some terms vary over time (item biases, user biases and user preferences)
    </span>
   </li>
  </ul>
  <hr style="page-break-before:always;display:none;"/>
  <p>
   <span>
   </span>
  </p>
  <h1 id="h.ral1n0tuv49">
   <span>
    Approach
   </span>
  </h1>
  <p>
   <span>
    Main components:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Core
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     NLP: extraction of intent and entities
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     User intents:
    </span>
   </li>
  </ul>
  <ol start="1">
   <li>
    <span>
     Search a bike
    </span>
   </li>
   <li>
    <span>
     Search a free slot
    </span>
   </li>
   <li>
    <span>
     Plan a trip (1+2)
    </span>
   </li>
   <li>
    <span>
     Search other information (restaurant, cafe, …)
    </span>
   </li>
  </ol>
  <ul>
   <li>
    <span>
     Entities:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     location to/from
    </span>
   </li>
   <li>
    <span>
     Recognise some values (home/work)
    </span>
   </li>
   <li>
    <span>
     Disambiguation: retrieve the right entity if it is not uniquely identified (contextualize with current city)
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Data retrieval:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     bike sharing (easy part)
    </span>
   </li>
   <li>
    <span>
     Context of the city: events, places. Possible source of data:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Foursquare
    </span>
    <span>
     <a href="https://developer.foursquare.com/docs/venues/explore">
      https://developer.foursquare.com/docs/venues/explore
     </a>
    </span>
    <span>
     can easily ask for suggestion on venues with specific category/property (120.000 requests per day, bigger coverage)
    </span>
   </li>
   <li>
    <span>
     Google places API
    </span>
    <span>
     <a href="https://developers.google.com/places/web-service/search">
      https://developers.google.com/places/web-service/search
     </a>
    </span>
    <span>
     provides similar features (1.000 requests per day)
    </span>
   </li>
   <li>
    <span>
     <a href="https://www.quora.com/What-are-the-pros-and-cons-of-each-Places-API">
      https://www.quora.com/What-are-the-pros-and-cons-of-each-Places-API
     </a>
    </span>
    <span>
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Weather forecasts: if bad weather is expected, tell user
    </span>
    <span>
     <a href="https://www.wunderground.com/weather/api/">
      https://www.wunderground.com/weather/api/
     </a>
    </span>
    <span>
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Personalization:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Model:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Manifest variables: gender, age, profession, … &lt;- bootstrap/social
    </span>
   </li>
   <li>
    <span>
     Habits variables: recurrent locations with roles &lt;- interaction
    </span>
   </li>
   <li>
    <span>
     Latent/hidden variables:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     health/sportive
    </span>
   </li>
   <li>
    <span>
     Arts
    </span>
   </li>
   <li>
    <span>
     Big five personality traits (feasible?)
    </span>
   </li>
   <li>
    <span>
     Pre-defined or internally created by the model
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Extract data to build user model (manifest variables):
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Bootstrap questions (explicit or by social media) to extract relevant informations in cold start phase:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Age
    </span>
   </li>
   <li>
    <span>
     Gender
    </span>
   </li>
   <li>
    <span>
     Profession
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Habits (can be derived from interactions):
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Home location
    </span>
   </li>
   <li>
    <span>
     Work/school location
    </span>
   </li>
   <li>
    <span>
     Other recurring places
    </span>
   </li>
   <li>
    <span>
     Timing informations
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Feedback from suggestions:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Attitude to recommendations (important)
    </span>
   </li>
   <li>
    <span>
     Interests
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Suggestion engine:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Trip suggestion and reminders (must not be invasive):
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Previously used stations
    </span>
   </li>
   <li>
    <span>
     Previous schemes/timetables
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Provide informations on the city context (not strictly related to domain)
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     User relevant places on the way
    </span>
   </li>
   <li>
    <span>
     Time relevant (suggested place should be open)
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Messaging interface (towards which messenger platform?)
    </span>
   </li>
   <li>
    <span>
     External profile data interface (consider technical difficulties) for more personalization and less explicit interview. If present, should be optional (not every user are on facebook, and also if they are, they may not want to share their profile)
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Recommendation phases:
   </span>
  </p>
  <ol start="1">
   <li>
    <span>
     SSH (social sciences and humanities):
    </span>
   </li>
  </ol>
  <ol start="1">
   <li>
    <span>
     Input: bootstrap/social data
    </span>
   </li>
   <li>
    <span>
     Output: user features (can be described and are kind of explicit dimensions)
    </span>
   </li>
  </ol>
  <ol start="1">
   <li>
    <span>
     Recommender system:
    </span>
   </li>
  </ol>
  <ol start="1">
   <li>
    <span>
     Input: user features
    </span>
   </li>
   <li>
    <span>
     Output: recommendation
    </span>
   </li>
   <li>
    <span>
     Internally using user profile vector that contains model-generated dimensions (can match or not the user features)
    </span>
   </li>
  </ol>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <hr style="page-break-before:always;display:none;"/>
  <p>
   <span>
   </span>
  </p>
  <h1 id="h.xrkimbz3muul">
   <span>
    Implementation
   </span>
  </h1>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <hr style="page-break-before:always;display:none;"/>
  <p>
   <span>
   </span>
  </p>
  <h1 id="h.9ldkmut5kq6f">
   <span>
    Validation
   </span>
  </h1>
  <p>
   <span>
    Have a look at “How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation”
   </span>
  </p>
  <hr style="page-break-before:always;display:none;"/>
  <h1 id="h.b8i4k637cy5o">
   <span>
    Conclusion
   </span>
  </h1>
  <div>
   <p>
    <span>
    </span>
   </p>
  </div>
  <hr/>
  <div>
   <p>
    <a href="#ftnt_ref1" id="ftnt1">
     [1]
    </a>
    <span>
    </span>
   </p>
  </div>
 </body>
</html>