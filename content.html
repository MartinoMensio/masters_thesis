<html>
 <head>
  <meta content="text/html; charset=utf-8" http-equiv="content-type"/>
 </head>
 <body>
  <div>
   <p>
    <span>
    </span>
   </p>
  </div>
  <h1 id="h.k60jpmft2v71">
   <span>
    Introduction
   </span>
  </h1>
  <p>
   <span>
    Introduce the problem and motivation
   </span>
  </p>
  <p>
   <span>
    Brief summary of previous works
   </span>
  </p>
  <p>
   <span>
    Specific objectives
   </span>
  </p>
  <p>
   <span>
    Roadmap
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <hr style="page-break-before:always;display:none;"/>
  <p>
   <span>
   </span>
  </p>
  <h1 id="h.hohs5u6vo8mq">
   <span>
    State of the Art
   </span>
  </h1>
  <p>
   <span>
    In this section I am presenting the related works on the topic.
   </span>
  </p>
  <h2 id="h.8cfue3eizy9l">
   <span>
    Social bots
   </span>
  </h2>
  <p>
   <span>
    The term can be used for different purposes:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Fake user profiles
    </span>
   </li>
   <li>
    <span>
     Crawlers that analyze social profiles
    </span>
   </li>
   <li>
    <span>
     Autonomous agents that provide some kind of service
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Bots and autonomous systems papers
   </span>
  </p>
  <p>
   <span>
    “Turing’s red flag”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Problem: AI mistaken for human
    </span>
   </li>
   <li>
    <span>
     Proposition:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     autonomous system should be designed in a way to make clear that is not a human
    </span>
   </li>
   <li>
    <span>
     should identify as autonomous system at the start of new interactions
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Cases:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Self-driving car: should be recognizable in order to avoid accidents and to allow other drivers (autonomous or not) to behave in proper way
    </span>
   </li>
   <li>
    <span>
     Virtual assistants: pretending to be human is a dangerous precedent. Now it is clear that they are AI, but with technological progress the difference could become unnoticed
    </span>
   </li>
   <li>
    <span>
     Online games: bots can have some advantages and disadvantages, but user should know what kind of player they belong to
    </span>
   </li>
   <li>
    <span>
     Computer-generated text: depending on the domain, can impact emotions of reader
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “The rise of social bots”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Problem: social bots can be harmful. Beyond the problem of veracity of information, they can gain influence and become popular
    </span>
   </li>
   <li>
    <span>
     Cases:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Political influence: audience is artificially enlarged
    </span>
   </li>
   <li>
    <span>
     Market influence: fake informations are amplified without fact-checking
    </span>
   </li>
   <li>
    <span>
     Exposing private informations: this makes people not to trust social media
    </span>
   </li>
   <li>
    <span>
     Manipulate emotions: on socials, they are contagious
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Proposition: detect bots
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Auto-reposting: easy to spot with posts/tweets without sense
    </span>
   </li>
   <li>
    <span>
     More advanced bots: emulating human behavior, filling profiles with data that seems legit, interacting actively with other users. In some cases they also clone profiles of real users
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Implementation types:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Based on social media information (graph-based): “sybil” accounts are prone to have more connections with other sybils -&gt; groups. Based on assumption that legitimate users refuse to interact with unknown accounts, but this is not so true
    </span>
   </li>
   <li>
    <span>
     Based on crowdsourcing: humans analyze profiles to detect bots, and majority voting is applied (same profiles shown to different workers)
    </span>
   </li>
   <li>
    <span>
     Machine-learning: find features that are significant for discrimination. The system analyzes a set of features (network, user, friends, timing, content, sentiment) to evaluate a score using cross validation. Not able to detect cyborgs (mixture of human and bot) and hacked accounts
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Is that a bot running the social media feed?”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: understand if users perceive differently a bot agent or a human agent
    </span>
   </li>
   <li>
    <span>
     Results: bots are perceived credible, attractive and competent as humans. The cause is that users use the same way of interacting with bots and with humans. However the attraction to the human agent is higher
    </span>
   </li>
   <li>
    <span>
     Limitation: very restricted context
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Towards the implementation of a Topic specific dialogue based Natural Language Chatbot as an Undergraduate Advisor”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: develop a chatbot to help students in University with admission and course information (FAQ) using AIML. Weights of state transitions are tuned to make the conversation stay on topic.
    </span>
   </li>
   <li>
    <span>
     Metrics for performance:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Satisfaction: user vote if the answer is appropriate in the context
    </span>
   </li>
   <li>
    <span>
     Topic switching rate: how much the dialogue is switched from on-topic to off-topic
    </span>
   </li>
   <li>
    <span>
     Correction rate: responses that are corrected by the user
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Can we control it? Autonomous robots threaten human identity, uniqueness, safety, and resources”
   </span>
   <sup>
    <a href="#ftnt1" id="ftnt_ref1">
     [1]
    </a>
   </sup>
  </p>
  <ul>
   <li>
    <span>
     Proposition: analyze the impact of autonomous systems on society, how they are perceived
    </span>
   </li>
   <li>
    <span>
     Problems of autonomous system:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Humans want to always have control and power. The system’s autonomy should always be bounded: only automatically perform specific tasks as requested. However in some cases there can be conflicts between what the user asks and the principle the system was designed with. This should be the only situation in which the system must disregard a command. If the system decides things on his own, also without conflicting with commands, this is seen as a threat to human superiority on machines. The hierarchy of society is threatened
    </span>
   </li>
   <li>
    <span>
     Realistic threats: menacing safety, wellbeing or material resources
    </span>
   </li>
   <li>
    <span>
     Symbolic threats: maintain the difference between human and machine. The concept of identity and distinctiveness are threatened
    </span>
   </li>
   <li>
    <span>
     Both kinds of threat lead to negative attitudes towards robots and robotics research
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “An Ergonomics Evaluation to Chatbot Equipped with Knowledge-Rich Mind”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: how to incorporate knowledge in chatbot systems
    </span>
   </li>
   <li>
    <span>
     Implementation:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Combining more approaches together. AIML (common dialog questions, containing a lot of stop words), Regular Matcher Component (for simple commands), FAQ (translating sentences into queries, selecting best match), NLP
    </span>
   </li>
  </ul>
  <h3 id="h.d7tjvkxrgg7m">
   <span>
    Guidelines
   </span>
  </h3>
  <p>
   <span>
    Define boundaries on:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Autonomy of the system: smart system should be able to decide things in its domain because a lot of information can be used to take decision, doing work for the users. Some limits need not to be overtaken, in order not to threat the hierarchy of society. Machines are tools for human well being
    </span>
   </li>
   <li>
    <span>
     Distinguishability: according to “red flag law”, system should clearly be recognizable as robot. However for usability the interaction with users should be similar to human-to-human
    </span>
   </li>
   <li>
    <span>
     Personality: having an artificial intelligence with some personality makes interactions more interesting and seamless for users. A boundary needs to be defined in order to avoid emotional attachment and other inappropriate feelings towards machines. From social point of view, people need to keep their life in real world with real people
    </span>
   </li>
  </ul>
  <hr style="page-break-before:always;display:none;"/>
  <p>
   <span>
   </span>
  </p>
  <h2 id="h.1t8dpluzlftb">
   <span>
    Types of bot
   </span>
  </h2>
  <p>
   <span>
    Dimensions to consider:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Long or short conversations
    </span>
   </li>
   <li>
    <span>
     Open or closed domain. Closed is the only feasible
    </span>
   </li>
  </ul>
  <p>
   <span>
    Important features / challenges:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Context: keep track of informations given by user
    </span>
   </li>
   <li>
    <span>
     Coherent personality: since training is done on multiple users data (see “A persona-based neural conversation model”)
    </span>
   </li>
   <li>
    <span>
     Evaluation (see “How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation”). May use recall@k
    </span>
   </li>
   <li>
    <span>
     Intention and diversity: easy to fall into generic responses (see “A Diversity-Promoting Objective Function for Neural Conversation Models”) especially for open-domain systems
    </span>
   </li>
  </ul>
  <p>
   <span>
    Scope of dialogue:
   </span>
  </p>
  <ul>
   <li>
    <span>
     General chat
    </span>
   </li>
   <li>
    <span>
     Task specific
    </span>
   </li>
  </ul>
  <h3 id="h.13wjijx5nwux">
   <span>
    Retrieval-based models
   </span>
  </h3>
  <p>
   <span>
    This kind of bots use a set of predefined response templates and apply some heuristics in order to select the most suitable one. The selection criterion can be:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Simple rule-based (as in AIML)
    </span>
   </li>
   <li>
    <span>
     More complex with machine learning classifiers
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Advantages:
   </span>
  </p>
  <ul>
   <li>
    <span>
     No grammatical mistakes
    </span>
   </li>
  </ul>
  <p>
   <span>
    Disadvantages:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Unseen cases cannot be handled
    </span>
   </li>
  </ul>
  <h4 id="h.lxbz31wv66qa">
   <span>
    AIML (rule based)
   </span>
  </h4>
  <p>
   <span>
    Is a language that describes how the bot replies to certain inputs. Inputs are evaluated against a set of patterns and the best match (category) is chosen. The actions performed by the category can be a simple response, or can also set variables and call other categories.
   </span>
  </p>
  <p>
   <span>
    Bot creation:
   </span>
  </p>
  <p>
   <span>
    Cyclical process called
   </span>
   <span>
    targeting
   </span>
   <span>
    : client inputs that find no match are collected using logs and the botmaster creates suitable responses. Targeting interface can also be exposed to clients, but with risks: not controlled by the botmaster
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    AIML can be used to build bots that simply do a pattern-matching job, it is only a stimulus-response system.
   </span>
  </p>
  <p>
   <span>
    Main disadvantages:
   </span>
  </p>
  <ul>
   <li>
    <span>
     A big set of AIML rules need to be built: time consuming. A lot of rules are also needed to perform reduction
    </span>
   </li>
   <li>
    <span>
     Difficult to reply to complex queries
    </span>
   </li>
   <li>
    <span>
     Not really understanding the language.
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Where it is used:
   </span>
  </p>
  <ul>
   <li>
    <span>
     ALICE based bots: Pandorabots is a service to host an AIML processor. Users can create files that contain the rules and test the bot
    </span>
   </li>
   <li>
    <span>
     <a href="https://github.com/keiffster/program-y">
      https://github.com/keiffster/program-y
     </a>
    </span>
    <span>
     is an implementation of AIML 2.0 using python 3
    </span>
   </li>
  </ul>
  <h4 id="h.cfauph7t5emx">
   <span>
    Machine learning (completely
   </span>
  </h4>
  <p>
   <span>
    Data sets:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Ubuntu dialogue corpus:
    </span>
    <span>
     <a href="https://github.com/rkadlec/ubuntu-ranking-dataset-creator">
      https://github.com/rkadlec/ubuntu-ranking-dataset-creator
     </a>
    </span>
    <span>
    </span>
   </li>
   <li>
    <span>
     Twitter
    </span>
   </li>
  </ul>
  <p>
   <span>
    Predictor:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Random. Very poor performance
    </span>
   </li>
   <li>
    <span>
     Using tf-idf: how important a word is important in a document. Documents with similar content have similar tf-idf vectors. Responses don’t need to be similar to the context to be correct
    </span>
   </li>
   <li>
    <span>
     Deep learning: dual-encoder LSTM network / seq2seq. Both are recurrent neural networks
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Examples:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Google smart reply
    </span>
    <span>
     <a href="http://arxiv.org/abs/1606.04870">
      http://arxiv.org/abs/1606.04870
     </a>
    </span>
    <span>
    </span>
   </li>
  </ul>
  <h3 id="h.3y4hjed0zgjs">
   <span>
    Generative models
   </span>
  </h3>
  <p>
   <span>
    Real artificial intelligence, understanding the language. They don’t use pre-defined responses, but generate them from scratch. The technology is based on translation techniques.
   </span>
  </p>
  <p>
   <span>
    Advantages:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Can refer back to entities mentioned in the conversation
    </span>
   </li>
  </ul>
  <p>
   <span>
    Disadvantages:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Grammatical mistakes can occur
    </span>
   </li>
   <li>
    <span>
     Require huge amounts of training data
    </span>
   </li>
   <li>
    <span>
     Early stage of development
    </span>
   </li>
   <li>
    <span>
     Can go off the rails (example of Microsoft Tay)
    </span>
   </li>
  </ul>
  <h2 id="h.kvxdevcml2at">
   <span>
    Challenges
   </span>
  </h2>
  <ul>
   <li>
    <span>
     Understanding the user queries: intent + slot
    </span>
   </li>
   <li>
    <span>
     Tracking the state of the dialogue
    </span>
   </li>
   <li>
    <span>
     Provide relatively good answers to non-task specific interactions
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <h2 id="h.n06dhtaupqp">
   <span>
    NLP and NLU
   </span>
  </h2>
  <p>
   <span>
    The most important part is to understand the user message. Instead to do a simple rule-matching, there are libraries that can process sentences and extract their structure, together with named entities.
   </span>
  </p>
  <p>
   <span>
    The pipeline on input should contain:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Spell check
    </span>
   </li>
   <li>
    <span>
     Tokenize (sentences, words)
    </span>
   </li>
   <li>
    <span>
     POS recognition
    </span>
   </li>
   <li>
    <span>
     Lemmatize (reconduct to base form) and reduce synonyms + stemming
    </span>
   </li>
   <li>
    <span>
     Entity recognition
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Once the input is processed, there should be a logic that based on the informations extracted (entities and relationships) understands the
   </span>
   <span>
    root-level intent
   </span>
   <span>
    of the person and decides what actions need to be performed:
   </span>
  </p>
  <ol start="1">
   <li>
    <span>
     Question classification -&gt; use
    </span>
   </li>
   <li>
    <span>
     Information retrieval
    </span>
   </li>
   <li>
    <span>
     Answer extraction
    </span>
   </li>
  </ol>
  <p>
   <span>
    NLP frameworks:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Stanford CoreNLP
    </span>
    <span>
     <a href="http://stanfordnlp.github.io/CoreNLP/">
      http://stanfordnlp.github.io/CoreNLP/
     </a>
    </span>
    <span>
     : java
    </span>
   </li>
   <li>
    <span>
     NLTK: wide variety of algorithms and stemmers. Can finely customize the model. String based
    </span>
   </li>
   <li>
    <span>
     <a href="https://github.com/sloria/TextBlob">
      https://github.com/sloria/TextBlob
     </a>
    </span>
    <span>
     : based on NLTK
    </span>
   </li>
   <li>
    <span>
     <a href="https://github.com/explosion/spaCy">
      https://github.com/explosion/spaCy
     </a>
    </span>
    <span>
     : faster, but a bit lower quality. A single stemmer. Object oriented
    </span>
   </li>
   <li>
    <span>
     Google syntaxnet
    </span>
    <span>
     <a href="https://github.com/tensorflow/models/tree/master/syntaxnet">
      https://github.com/tensorflow/models/tree/master/syntaxnet
     </a>
    </span>
    <span>
     . A tensorflow model that is the most accurate parser. Also able to parse correctly some garden-paths
    </span>
   </li>
  </ul>
  <p>
   <span>
    Comparison between NLTK and spaCy
   </span>
   <span>
    <a href="https://gist.github.com/rschroll/61b20c41e984a963df2870cfc9e628ed">
     https://gist.github.com/rschroll/61b20c41e984a963df2870cfc9e628ed
    </a>
   </span>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Those NLP framework can be used for the specific task of NLU (that corresponds to turning the sentences into intent+entities). The other NLP tasks (syntactical analysis, POS tagging) are not main interests in the field of bots
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    A concept of the bot has been developed using wit.ai for doing intent detection and entity extraction. This can be used as a reference. The training data can be downloaded from the platform, useful for our model training.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h3 id="h.xsg4ur5pp4av">
   <span>
    Goal
   </span>
  </h3>
  <p>
   <span>
    As said before, from the sentences we want to:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Classify the intent (what the user wants, against a set of pre-defined ones)
    </span>
   </li>
   <li>
    <span>
     Extract the entities mentioned (we may want to care about only a specific set of entity types that are relevant to the abilities of the bot)
    </span>
   </li>
  </ul>
  <p>
   <span>
    The first task corresponds to a classification of the user sentences to decide what is the intention of the user. Since a bot is usually designed to answer to different types of questions, this stage is responsible for finding the type of question. The slot filling task instead (also called NER) is a process that annotates some parts of the input sentences with the corresponding entity type. While the intent represents the type of question, the entities of a sentence are values that the bot must be able to extract from the sentences because they are used as parameters for the application logic.
   </span>
  </p>
  <p>
   <span>
    Making a metaphor with programming languages, intents correspond to methods while entities are the arguments.
   </span>
  </p>
  <p>
   <span>
    For intent classification different approaches can be used:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Key words
    </span>
   </li>
   <li>
    <span>
     Syntactic based: the input features are hand-crafted (for example verb + object are taken into account)
    </span>
   </li>
   <li>
    <span>
     Sequence modeling (see RNN)
    </span>
   </li>
  </ul>
  <p>
   <span>
    There are different approaches for entity extraction:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Syntactic-based: from the structure of the sentence (usually as a tree), a model is built to observe and learn where usually a certain type of entity is
    </span>
   </li>
   <li>
    <span>
     Statistical-based: instead of syntactically parsing the sentence, a model is built without hand-crafted features, only providing input sequences and output label sequences
    </span>
   </li>
   <li>
    <span>
     Semantic-based: the values of the entities are used to detect the entities. The entity lookup and linking is a key point in this approach
    </span>
   </li>
  </ul>
  <p>
   <span>
    The two tasks can be done independently, but some recent studies have shown that there can be benefits if they are performed together.
   </span>
  </p>
  <h3 id="h.m20m4wbp4ch1">
   <span>
    Recurrent Neural Networks for joint entity extraction and intent classification
   </span>
  </h3>
  <p>
   <span>
    A neural-network approach can be used for the tasks of named entity extraction and intent detection. The inputs to the network are the words and the outputs are the labels of the considered task(s): intent label and/or entity labels.
   </span>
  </p>
  <p>
   <span>
    As entity labels, a commonly used format is the IOB, where the O indicates “no entity”, the B is the beginning of an entity and the I is the label associated with a word that continues the entity of the previous word.
   </span>
  </p>
  <h4 id="h.iuw0sa6gdxrl">
   <span>
    Why RNN
   </span>
  </h4>
  <p>
   <span>
    Instead of using simple feed-forward networks, a lot of studies make use of networks with recurrent components. A recurrent cell is a type of cell that takes as input also its previous output. Those types of neural networks have been designed for problems where the order of inputs matters, and the length of input sequence can vary. For example, they are commonly used with sequence of words, characters, frames in a video and their goodness stands in modeling features that belong to the sequence. Unlike feed-forward nets, that consider the fixed set of inputs to generate the outputs, the recurrent nets are applied in different timesteps to elements belonging to a sequence and, thanks to the loops of their cells, keep informations from previous timesteps and the output depends on them too.
   </span>
  </p>
  <p>
   <span>
    Since the same network (and cells) are used in different timesteps, the analysis of RNN is usually performed on the unfolded version of the network: the single elements are considered different times and the looping links are therefore going from the element in the previous time to the next time.
   </span>
  </p>
  <p>
   <span>
    The way backpropagation is applied to RNN is the so called backpropagation through time BPTT, in which the network is unfolded in time but all the copies of the same element have shared parameters.
   </span>
  </p>
  <h4 id="h.wruksxks8hm">
   <span>
    Cell types (simple RNN, GRU, LSTM)
   </span>
  </h4>
  <p>
   <span>
    The simplest recurrent cell type is a block with two inputs and two outputs. One input is the actual input at the current timestep while the other one comes from the previous timestep (or from initialization on the first time). The two outputs of the cell are equivalent, and is simply to put emphasis on the fact that one of them will go as input to the next timestep (this corresponds to the loop in the not-unfolded representation) and the other one can be passed to the next layer or used as output after applying some other functions (usually a softmax in this case).
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 386.00px; height: 276.00px;">
    <img alt="rnn.png" src="https://lh6.googleusercontent.com/AZkM5BzOFANxoiegDMrhW79EXbl11EPFL08VkfsYqqU5cT2t3ZP2Kp156ps6-6kU0vjZ2sbbxBNtgw1F2-JkAzN9hBkYgmFMFBctYGdfGVVPkc8IxAbYWdSRTXe8mmzcnfralGp-" style="width: 386.00px; height: 276.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    The two inputs are concatenated and passed through a single feed-forward layer, that corresponds to a linear transformation plus a non-linear function (e.g. tanh, sigma).
   </span>
  </p>
  <p>
   <span>
    Since the same cell is applied many times in time, and the recurrence loop feeds back the output as inputs, there can easily be two kinds of problem due to the fact that the weight matrix coefficients are multiplied at each timestep:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Exploding gradient: if some coefficients are greater than 1, the output values can become soon very big, making the network insensible to new inputs because is in some way saturated. The solution to this problem is using some non-linear function that limit the values
    </span>
   </li>
   <li>
    <span>
     Vanishing gradient: if some coefficients are near to 0, the network will quickly forget previous inputs and the output won’t depend on them
    </span>
   </li>
  </ul>
  <p>
   <span>
    Those problem have the same origin: the simple RNN is not able to manage long-term dependencies. This problem has been analyzed in detail by Bengio, et al. (1994)
   </span>
   <sup>
    <a href="#ftnt2" id="ftnt_ref2">
     [2]
    </a>
   </sup>
   <span>
    and other types of cells have been proposed.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    LSTM
   </span>
   <sup>
    <a href="#ftnt3" id="ftnt_ref3">
     [3]
    </a>
   </sup>
   <span>
    is a solution that came out in 1997 in which a more complex cell is considered. The main idea is to have some gates that decide how much of the previous cell state to keep, and how much of the current input to consider for the calculation of the current state and current output.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 556.00px; height: 286.00px;">
    <img alt="rnn (2).png" src="https://lh4.googleusercontent.com/VmRxc0aSR7AZOeJulPfTqmsqy4bm3SZLc1AMIBH8sxs6bxEIbgiFa3w0CSoWJnF9i4mSjSaplDp_60BRsJmOoHxooaMNtjqEkGSFdCJlXRMMvG5wfbvcY1BDY0HC-M927lvi9NTL" style="width: 556.00px; height: 286.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    The gates are ft it and ot, that are:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Forget gate: decides how much of the previous hidden state to keep
    </span>
   </li>
   <li>
    <span>
     Input gate: decides how much of the current input to consider
    </span>
   </li>
   <li>
    <span>
     Output gate: decides how much of the hidden state is exposed to the output
    </span>
   </li>
  </ul>
  <p>
   <span>
    All those gates are implemented with single layer feedforward networks. This type of RNN is able to manage better the long-term dependencies, at the expenses of having four times the parameters. But with sufficient training examples, the network is able to learn how to output the correct values and how to mix the different inputs.
   </span>
  </p>
  <h4 id="h.e8p21ryw643v">
   <span>
    Word embeddings
   </span>
  </h4>
  <h4 id="h.qo6n6nf5uz8d">
   <span>
    Intent classification
   </span>
  </h4>
  <h4 id="h.c1d8k7r13p0l">
   <span>
    Sequence to sequence models for slot tagging
   </span>
  </h4>
  <p>
   <span>
    Simple RNN, bidirectional
   </span>
  </p>
  <ul>
   <li>
    <span>
     Simple encoder-decoder
    </span>
   </li>
   <li>
    <span>
     Context vector to each decoding timestep
    </span>
   </li>
   <li>
    <span>
     Attention (different context vector to each decoding timestep)
    </span>
   </li>
   <li>
    <span>
     Modeling output dependencies (local choice, feed the previous output together with the current input to decoding timestep, linear-chain CRF)
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Considering context (previous interactions) and multi-turn interactions
   </span>
  </p>
  <hr style="page-break-before:always;display:none;"/>
  <h2 id="h.bhhmsohw8kph">
   <span>
    Personalization/Recommendation
   </span>
  </h2>
  <p>
   <span>
    “Computer-based personality judgments are more accurate than those made by humans”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: compare accuracy of personality judgment done by computers against those made by humans. Using likes to judge five traits: openness, agreeableness, extraversion, conscientiousness and neuroticism
    </span>
   </li>
   <li>
    <span>
     Criterions:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Self-other agreement: how much external judger (computer or human) agrees with self-rating
    </span>
   </li>
   <li>
    <span>
     Interjudge agreement: similarity of rating given by two external judger (human or computer-&gt;training set divided in two parts)
    </span>
   </li>
   <li>
    <span>
     External validity: measuring the prediction on life outcomes (facts that can be verified) based on the traits of personality computed (computer) or self-assigned (human)
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Results: computer are more accurate. But there are some differences:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Computer-based judgments can take into account a very big quantity of data and use statistical modeling
    </span>
   </li>
   <li>
    <span>
     Humans can capture more subtle cues
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Private traits and attributes are predictable from digital records of human behavior”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: using data of social network profiles (user likes) analyze the prediction rate of non-published personal data -&gt; 10-fold cross-validation
    </span>
   </li>
   <li>
    <span>
     Results: the majority of informations are accurately predicted. Especially the ethnicity, gender, age. The accuracy increases with the number of likes available, but even with a small data set, the prediction is accurate for some attributes
    </span>
   </li>
   <li>
    <span>
     Implications of being highly predictive:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Recommendation services can be improved, without explicitly asking some information but inferring them
    </span>
   </li>
   <li>
    <span>
     Unwilling use of details that user wanted to hide, considered as personal. This implies a decrease in trust of online services
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Share Like Recommend”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: analyze news as social activity
    </span>
   </li>
   <li>
    <span>
     Results:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     users receive news from friends on social networks, a lot more than directly from news agencies. Friends act like a filter, by sharing news.
    </span>
   </li>
   <li>
    <span>
     News as a shared social experience is appealing for customers. Main reason for using social networks is sharing content: events, news
    </span>
   </li>
   <li>
    <span>
     Journalists on social network seem to be important for users
    </span>
   </li>
   <li>
    <span>
     Using social media for retrieving news is not done at expenses of mainstream media outlets
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Tag-Based User Profiling for Social Media Recommendation”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: use tags to support user profiling. Tags connect entities directly, while collaborative filtering must search for relationships between user preferences and item attribute
    </span>
   </li>
   <li>
    <span>
     Implementation:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Types of tags (have different weights):
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Personal view: tags specified by the user
    </span>
   </li>
   <li>
    <span>
     Social view: tags specified by the friends on the user contents
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Tag-to-Tag Matrix: correlations between person-attribute set (tags on user) and content-attribute set (tags on contents)
    </span>
   </li>
   <li>
    <span>
     User-feature vector: for each idea mentioned in a specific content, the probability that the user has to like it. If it is above a threshold, it is recommended
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Experiment (using social bookmarking site del.icio.us):
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     A long-tail model applies (URLs are bookmarked by few people). Very low precision (2.77%) because there are not central topics to be defined. Also caused by dependence on order of visit (if user visits a better site before, decision about bookmarking a site on the same topic is lower)
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Cross Social Media Recommendation”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: merge data from different social networks (Twitter and Weibo) to remove biasing of models based on a single social network. Recommend twitter hashtag to weibo
    </span>
   </li>
   <li>
    <span>
     Implementation:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     “Pseudo Global Social Media Network (PGSMN) model to interconnect users (with similar interests, not the same user because the sets are separate) and topics
    </span>
   </li>
   <li>
    <span>
     Three layers: Twitter, Weibo and Wikipedia. The last one is a bridge. Each layer contains intra-layer links (users to tags or page to category) and inter-layer links (tags to pages/category)
    </span>
   </li>
   <li>
    <span>
     Explicit Semantic Path Mining (ESPM), derived from Explicit Semantic Analysis (ESA), identifies semantic paths from Wikipedia hierarchical relationships
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Measurements of ranking performance:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Precision
    </span>
   </li>
   <li>
    <span>
     Mean average precision
    </span>
   </li>
   <li>
    <span>
     Normalized discounted cumulative gain
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <h3 id="h.tm5ee3a05f8l">
   <span>
    Accessing user data on facebook (discarding it for the moment)
   </span>
  </h3>
  <p>
   <span>
    The data that is available directly from the messenger API is the following: first_name,last_name,profile_pic,locale,timezone,gender,is_payment_enabled
   </span>
  </p>
  <p>
   <span>
    In order to be able to do personalization, there exists a way to link the messenger id (page-scoped) to other ids (also facebook id indirectly):
   </span>
   <span>
    <a href="https://developers.facebook.com/docs/messenger-platform/account-linking">
     https://developers.facebook.com/docs/messenger-platform/account-linking
    </a>
   </span>
  </p>
  <p>
   <span>
    Once the facebook id is retrieved, it is possible to use the facebook graph API.
   </span>
  </p>
  <p>
   <span>
    To the external account the messenger platform sends a request to an url specified by the developer, adding account_linking_token and redirect_uri (page to redirect user after login). The external site, if login successful, redirects to redirect_uri with authentication_code custom (maybe put there the id in order to allow the bot to do the join)
   </span>
  </p>
  <p>
   <span>
    From the other side, the oauth flow:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Redirect user to facebook oauth with client_id (the app id) and redirect_uri (where facebook will send the user)
    </span>
   </li>
   <li>
    <span>
     After that user gives permissions, fb will redirect to redirect_uri with other parameters
    </span>
   </li>
  </ul>
  <p>
   <span>
    The procedure seems quite complicated and requires a web server component that interacts on one side with messenger platform and on the other side handles the facebook login.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h3 id="h.gjp0hmp20g33">
   <span>
    Cold start
   </span>
  </h3>
  <p>
   <span>
    Another possible approach, without using the facebook data, would be to use a bootstrap in the
   </span>
   <span>
    cold start
   </span>
   <span>
    phase: a few questions to build a first model of the user. Then more informations can be
   </span>
   <span>
    extracted as the conversation flows.
   </span>
  </p>
  <p>
   <span>
    An explicit preference elicitation at the beginning, not too long, that then becomes implicit.
   </span>
  </p>
  <p>
   <span>
    Other systems apply this procedure (e.g. netflix)
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Facing the cold start problem in recommender systems”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: find a good solution for the user-side cold start
    </span>
   </li>
   <li>
    <span>
     Other approaches to the problem:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Content-based: requires ratings by user. Cold start problem
    </span>
   </li>
   <li>
    <span>
     Collaborative-filtering: requires other users ratings
    </span>
   </li>
   <li>
    <span>
     Explicit interview to new user about items (adapting new questions to the answers provided)
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Implementation: three phases (Collaborative-filtering)
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Classification of the new user in a specific group (based on demographic data): using C4.5 algorithm (decision tree) and Naive Bayes
    </span>
   </li>
   <li>
    <span>
     Find “neighbours” of the new user inside the group: weighted average of demographic data
    </span>
   </li>
   <li>
    <span>
     Calculation of outcome: prediction techniques
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Dealing with the new user cold-start problem in recommender systems: A comparative review”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: compare existing algorithms (collaborative filtering) on the cold-start problem. Types of systems:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Uses additional data sources (e.g. demographic data): a limitation of these systems is that sometimes data is not available (because user did not associate social profile)
    </span>
   </li>
   <li>
    <span>
     Selects a group of analogous users (without additional data sources): construct a decision tree where nodes are questions. NHSM also takes into account the global preference of user behaviors, using three factors of similarity: Proximity (how much two ratings are near), Significance (how much distant from the median) and Singularity (how the two ratings are different from others). Limitations: how to choose the optimal number of groups and splitting criteria. Must have some rating from new user (bootstrap)
    </span>
   </li>
   <li>
    <span>
     Hybrid methods:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     FARAMS: using multiple approaches (fuzzy sets and association rules).
    </span>
   </li>
   <li>
    <span>
     HU-FCF: combines analysis on demographic data (fuzzy similarity matrix) and on rating data (hard similarity matrix)
    </span>
   </li>
   <li>
    <span>
     Limitations: irrelevant users are still included in the computation of similarities
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Results: NHSM is the best one, both for accuracy and for computational time
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Learning multiple-question decision trees for cold-start recommendation”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: find a solution to the cold-start recommendation problem that maximizes accuracy and minimizes user efforts. Problem of classic bootstrap is that user usually does not know items in the first interactions. The idea is to build a tree with multiple questions at each node.
    </span>
   </li>
   <li>
    <span>
     Results:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Accuracy is better than single-question based system (because for each node/page more informations are extracted) and also better than linear-combination of multiple trees
    </span>
   </li>
   <li>
    <span>
     Time increment to answer more questions per screen is sublinear
    </span>
   </li>
   <li>
    <span>
     Time difference between rating scale and binary answer doesn’t have strong dependency on the number of questions
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Matrix Factorization Techniques for Recommender Systems”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Collaborative filtering Approaches:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Neighborhood methods:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     User-oriented: find similar users to the target user, and from them find the items that they like
    </span>
   </li>
   <li>
    <span>
     Product-oriented: find items similar to the ones liked by the target user
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Latent factor models: characterize items and users on some factors (user/item vector) inferred from rating patterns (explicit or implicit feedback). Predicted rating is the dot product between the two vectors
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Learning algorithms for extracting the factor vectors:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Stochastic gradient descent: easier and faster
    </span>
   </li>
   <li>
    <span>
     Alternating least squares: can be parallelized, better on sparse data
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Temporal dynamics: the model should be dynamic because some terms vary over time (item biases, user biases and user preferences)
    </span>
   </li>
  </ul>
  <hr style="page-break-before:always;display:none;"/>
  <p>
   <span>
   </span>
  </p>
  <h1 id="h.ral1n0tuv49">
   <span>
    Approach
   </span>
  </h1>
  <h2 id="h.cof2o0hc8q7h">
   <span>
    Main components
   </span>
  </h2>
  <ul>
   <li>
    <span>
     Core
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     NLU: extraction of intent and entities (transforming language into actionable data)
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     User intents: (
    </span>
    <span>
     <a href="https://github.com/ryankiros/skip-thoughts">
      https://github.com/ryankiros/skip-thoughts
     </a>
    </span>
    <span>
     )
    </span>
   </li>
  </ul>
  <ol start="1">
   <li>
    <span>
     Search a bike
    </span>
   </li>
   <li>
    <span>
     Search a free slot
    </span>
   </li>
   <li>
    <span>
     Plan a trip (1+2)
    </span>
   </li>
   <li>
    <span>
     Communicate location
    </span>
   </li>
   <li>
    <span>
     Get informations about a station
    </span>
   </li>
   <li>
    <span>
     Find closest station to a certain point
    </span>
   </li>
   <li>
    <span>
     Search other information (restaurant, cafe, …)
    </span>
   </li>
   <li>
    <span>
     Basic interactions (greet/thank)
    </span>
   </li>
   <li>
    <span>
     Feedback (on recommendation/place/system)
    </span>
   </li>
   <li>
    <span>
     Informations on the bot (who are you? What can you do?)
    </span>
   </li>
   <li>
    <span>
     Setting preferences:
    </span>
   </li>
  </ol>
  <ol start="1">
   <li>
    <span>
     User favorite places (role → place)
    </span>
   </li>
   <li>
    <span>
     Enable/disable unsolicited messages
    </span>
   </li>
   <li>
    <span>
     Customization
    </span>
   </li>
  </ol>
  <ul>
   <li>
    <span>
     Entities:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     location to/from
    </span>
   </li>
   <li>
    <span>
     time
    </span>
   </li>
   <li>
    <span>
     Recognise some values (home/work)
    </span>
   </li>
   <li>
    <span>
     Disambiguation: retrieve the right entity if it is not uniquely identified (contextualize with current city)
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Data retrieval:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     bike sharing (easy part)
    </span>
   </li>
   <li>
    <span>
     Context of the city: events, places. Possible source of data:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Foursquare
    </span>
    <span>
     <a href="https://developer.foursquare.com/docs/venues/explore">
      https://developer.foursquare.com/docs/venues/explore
     </a>
    </span>
    <span>
     can easily ask for suggestion on venues with specific category/property (120.000 requests per day, bigger coverage)
    </span>
   </li>
   <li>
    <span>
     Google places API
    </span>
    <span>
     <a href="https://developers.google.com/places/web-service/search">
      https://developers.google.com/places/web-service/search
     </a>
    </span>
    <span>
     provides similar features (1.000 requests per day)
    </span>
   </li>
   <li>
    <span>
     <a href="https://www.quora.com/What-are-the-pros-and-cons-of-each-Places-API">
      https://www.quora.com/What-are-the-pros-and-cons-of-each-Places-API
     </a>
    </span>
    <span>
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Weather forecasts: if bad weather is expected, tell user
    </span>
    <span>
     <a href="https://www.wunderground.com/weather/api/">
      https://www.wunderground.com/weather/api/
     </a>
    </span>
    <span>
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Personalization:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Model:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Manifest variables: gender, age, profession, … &lt;- may be irrelevant (bootstrap/social)
    </span>
   </li>
   <li>
    <span>
     Habits variables: recurrent locations with roles &lt;- interaction
    </span>
   </li>
   <li>
    <span>
     Latent/hidden variables:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     health/sportive
    </span>
   </li>
   <li>
    <span>
     Arts
    </span>
   </li>
   <li>
    <span>
     Big five personality traits (feasible?)
    </span>
   </li>
   <li>
    <span>
     Pre-defined or internally created by the model
    </span>
   </li>
   <li>
    <span>
     <a href="http://goodcitylife.org/happymaps/">
      http://goodcitylife.org/happymaps/
     </a>
    </span>
    <span>
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Extract data to build user model (manifest variables):
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Bootstrap questions (explicit or by social media) to extract relevant informations in cold start phase:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Age
    </span>
   </li>
   <li>
    <span>
     Gender
    </span>
   </li>
   <li>
    <span>
     Profession
    </span>
   </li>
   <li>
    <span>
     What the user wants to be recommended on
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Habits (can be derived from interactions):
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Home location
    </span>
   </li>
   <li>
    <span>
     Work/school location
    </span>
   </li>
   <li>
    <span>
     Other recurring places
    </span>
   </li>
   <li>
    <span>
     Timing informations
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Feedback from suggestions:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Attitude to recommendations (important)
    </span>
   </li>
   <li>
    <span>
     Interests
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Suggestion engine:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Trip suggestion and reminders (must not be invasive):
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Previously used stations
    </span>
   </li>
   <li>
    <span>
     Previous schemes/timetables
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Provide informations on the city context (not strictly related to domain)
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     User relevant places on the way
    </span>
   </li>
   <li>
    <span>
     Time relevant (suggested place should be open)
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Messaging interface (towards which messenger platform?)
    </span>
   </li>
  </ul>
  <p>
   <span>
    Conversational agent separate from the messaging platform. Using a generic message-proxy (see botkit) that will handle all the specific features of the platforms. The conversational agent should be indipendent
   </span>
  </p>
  <ul>
   <li>
    <span>
     External profile data interface (consider technical difficulties) for more personalization and less explicit interview. If present, should be optional (not every user are on facebook, and also if they are, they may not want to share their profile)
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <h3 id="h.j4qm78dpama7">
   <span>
    Language understanding process
   </span>
  </h3>
  <ol start="1">
   <li>
    <span>
     Reduction to patterns: NLP with spaCy
    </span>
   </li>
  </ol>
  <ol start="1">
   <li>
    <span>
     get the grammatical relevant structure
    </span>
   </li>
   <li>
    <span>
     Reduce synonyms, reducing separately the Parts Of Speech
    </span>
   </li>
  </ol>
  <ol start="2">
   <li>
    <span>
     From patterns to intents
    </span>
   </li>
  </ol>
  <ol start="1">
   <li>
    <span>
     Applying a classifier (RNN LSTM or simple NN)
    </span>
   </li>
  </ol>
  <ol start="3">
   <li>
    <span>
     Intent processing
    </span>
   </li>
  </ol>
  <ol start="1">
   <li>
    <span>
     Get entities
    </span>
   </li>
  </ol>
  <ol start="4">
   <li>
    <span>
     Contextualization
    </span>
   </li>
  </ol>
  <ol start="1">
   <li>
    <span>
     Disambiguate (non-global entities contextualized)
    </span>
   </li>
  </ol>
  <ol start="1">
   <li>
    <span>
     Incomplete names (e.g. a store name, where multiple stores with this name exist)
    </span>
   </li>
   <li>
    <span>
     Variables (e.g. home, work)
    </span>
   </li>
  </ol>
  <ol start="5">
   <li>
    <span>
     Checking required informations (do some questions if missing) and pass them to the modules that use them (bike-sharing, meteo, places, …)
    </span>
   </li>
  </ol>
  <p>
   <span>
   </span>
  </p>
  <h3 id="h.m202m0o42x2b">
   <span>
    Recommendation phases
   </span>
  </h3>
  <ol start="1">
   <li>
    <span>
     SSH (social sciences and humanities):
    </span>
   </li>
  </ol>
  <ol start="1">
   <li>
    <span>
     Input: bootstrap/social data, episodic KB
    </span>
   </li>
   <li>
    <span>
     Output: user features (can be described and are kind of explicit dimensions)
    </span>
   </li>
  </ol>
  <ol start="2">
   <li>
    <span>
     Recommender system:
    </span>
   </li>
  </ol>
  <ol start="1">
   <li>
    <span>
     Input: user features + stimulus (what type of recommendation)
    </span>
   </li>
   <li>
    <span>
     Output: recommendation
    </span>
   </li>
   <li>
    <span>
     Internally using user profile vector that contains model-generated dimensions (can match or not the user features)
    </span>
   </li>
  </ol>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Finding recurrent actions:
   </span>
  </p>
  <p>
   <span>
    The EpisodicKB of actions describes events like: “on day DD/MM/YYYY time HH:MM user XXX searched for a bike at place PPP”. The place is not a simple position but is an external key to a Place row. Those places contain info that come from the contextual knowledge derived from some PlacesAPI (externally stored), or are stations or other specific positions with a role (home/work/school) (internally stored)
   </span>
  </p>
  <p>
   <span>
    This table is monitored to find patterns that are then stored in the RecurrentAction table
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h2 id="h.6tunsp8x3u1l">
   <span>
    Data
   </span>
  </h2>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Request: {utterance: {userID, time, content}, NLP: {intent, entities:[type,value]}, context, info: {stations:[], places:[], meteo:[]}, recommendations: [type,value,confidence], decision: {}}
   </span>
  </p>
  <p>
   <span>
    Is filled along the pipeline. Each stage is adding an element to it
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    NLP
   </span>
  </p>
  <p>
   <span>
    From utterance:
   </span>
   <span>
    utteranceID
   </span>
   <span>
    , userID, time, content
   </span>
  </p>
  <p>
   <span>
    To: {intent,entities[type,value]}
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Entity resolver
   </span>
  </p>
  <p>
   <span>
    From user model:
   </span>
  </p>
  <ul>
   <li>
    <span>
     User city
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     User position
    </span>
   </li>
   <li>
    <span>
     User places: home, work/school
    </span>
   </li>
  </ul>
  <p>
   <span>
    The location entities become references to places in the DB (using places API):
   </span>
  </p>
  <ul>
   <li>
    <span>
     From name to place (the name in the utterance is used as keyword to do a search in the city proximity)
    </span>
   </li>
   <li>
    <span>
     From position to place (when user sends position, find a relevant place in the proximity)
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Core
   </span>
  </p>
  <p>
   <span>
    Internal state:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Previous questions / topic (field of dialogue): to understand messages without explicit intent and to resume interaction (stack of interactions: “i want a bike” PUSH “where are you?” -&gt; “i am at XXX” -&gt; “ok got it” POP “you can find 3 bikes at YYY”)
    </span>
   </li>
   <li>
    <span>
     Special topic of bootstrap: activated at the beginning
    </span>
   </li>
  </ul>
  <p>
   <span>
    Core steps:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Intent
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     If the intent is not specified (or generic intent “approve”/”disapprove”), look at the entity provided and if a previous state is saved. Based on these, derive the intent
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Preparation of parameters for functions (take from entities) and check requirements
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     If requirements are ok, proceed with steps
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     If previous state, pop it
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     If some requirements are missing, save current state (push) and ask for requirement
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Bike sharing:
   </span>
  </p>
  <p>
   <span>
    Station:
   </span>
   <span>
    stationID
   </span>
   <span>
    , name, description
   </span>
   <span>
    , (lat,long), free_bikes, free_slots
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Personalization:
   </span>
  </p>
  <p>
   <span>
    User:
   </span>
   <span>
    userID
   </span>
   <span>
    , Name, Surname, sex, age
   </span>
  </p>
  <p>
   <span>
    Episodic KB:
   </span>
   <span>
    userID, location, time
   </span>
   <span>
    , action (was at/took bike/left bike)
   </span>
  </p>
  <p>
   <span>
    Explicit rating: collecting user feedback after recommendation / bootstrap / unsolicited rating → needs to be investigated
   </span>
  </p>
  <p>
   <span>
    User features (built by the model): favorite/recurrent places, sportiveness, art interest
   </span>
  </p>
  <ul>
   <li>
    <span>
     RecurrentAction:
    </span>
    <span>
     ID
    </span>
    <span>
     , userID, type, frequency (days in week pattern), time of day, userPlaceID (external key to UserPlace)
    </span>
   </li>
   <li>
    <span>
     UserPlaces:
    </span>
    <span>
     userID, placeID
    </span>
    <span>
     , role (home/work/school/other)
    </span>
   </li>
  </ul>
  <p>
   <span>
    The stimulus link from the result provider to the recommender contains: trip information / direct question to the recommending system
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    User_feature:
   </span>
   <span>
    userID, featureID
   </span>
   <span>
    , value ← output of user modeler, input to recommender
   </span>
  </p>
  <p>
   <span>
    Place:
   </span>
   <span>
    placeID
   </span>
   <span>
    , name, description, (lat,long), category, subcategory
   </span>
  </p>
  <p>
   <span>
    Place_feature:
   </span>
   <span>
    placeID, featureID
   </span>
   <span>
    , value
   </span>
  </p>
  <h2 id="h.kqqgd8as7rlg">
   <span>
    Scenarios
   </span>
  </h2>
  <p>
   <span>
    Search for informations:
   </span>
  </p>
  <p>
   <span>
    The utterance is processed by the NLU module. The query understanding engine activates the contextual knowledge engine to get data that is related to the question, and in parallel records the event into the episodic knowledge. The contextual knowledge is composed of informations belonging to bike sharing, meteo, placesAPI. The results from the different sources are combined by the appropriate module. At this point the recommender is interrogated, in order to provide some personalized suggestions, if they have some metrics (telling that the suggestion is appropriate and relevant). The structured information of the result that is going to be provided needs some elaboration to provide a natural-language feeling: this is the role of the sentence generation engine that, based on templates, puts the response in human-like format.
   </span>
  </p>
  <p>
   <span>
    Expressing availability to talk (familiarization):
   </span>
  </p>
  <p>
   <span>
    The user may be sending some messages that don’t have some needs inside. Also the first interaction, in which the user sends some “/start” or “hi“ messages can activate this use case. The NLU processes them as usual, and the query understanding engine detects the situation. Instead of using contextual knowledge, it interacts with the personalization module to find some questions for the user: what he expects from the bot, or asking some features (interests/…).
   </span>
  </p>
  <p>
   <span>
    Sending messages without being asked:
   </span>
  </p>
  <p>
   <span>
    Warning: this breaks the principle that bots should only interact when explicitly asked. For this reason this feature should be kept under full control of the user (opt-in proposition vs one-time test + asking if this is liked), that in any moment can decide to turn it off. The reminder/suggestion, based on explicit request to do that or because a pattern has been detected, is generated and sent. The feedback must always be measured. The activation may be defined by the user (e.g. “tomorrow at 5 p.m. give me informations about bikes to go home”) or when the expected time, derived from the detected pattern, is coming (e.g: user always asks for a bike between 8:10 and 8:30 to go to university, now it is 8:00, could be the right time to send him the suggestion).
   </span>
  </p>
  <p>
   <span>
    Another useful activation is user asks for station X→ give results now on X and activate watcher on station X → estimate user-arrival-time to X → if station X is becoming useless (no/few bikes/slots depending on user need) send message to warn user
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Location can be inside the utterance as entity or can be the user position. In this case it must be up-to-date (define a timeout of validity and a way to ask user: “are you still there?”). If the dependency is not fulfilled, a way of skip
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <a id="t.2c97474fc09cd2758e0b91a19c5cfdca72f43631">
  </a>
  <a id="t.0">
  </a>
  <table>
   <tbody>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        intent
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        dependencies
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        steps
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        decision
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Search bike/free slot
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Location / updated position
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Find station informations, check meteo. Collect event. Activate watcher on station. Interrogate recommender
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Station informations + meteo warning* + recommendation*
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Plan trip
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Location A+B
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Same as above, but twice because both station must be watched / both events need to be collected
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        As above
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        If i will go home in 30 minutes, will I get wet?
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        OMG
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
       </span>
      </p>
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <hr style="page-break-before:always;display:none;"/>
  <p>
   <span>
   </span>
  </p>
  <h1 id="h.xrkimbz3muul">
   <span>
    Implementation
   </span>
  </h1>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <hr style="page-break-before:always;display:none;"/>
  <p>
   <span>
   </span>
  </p>
  <h1 id="h.9ldkmut5kq6f">
   <span>
    Validation
   </span>
  </h1>
  <p>
   <span>
    “How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation”
   </span>
  </p>
  <p>
   <span>
    This paper analyzes metrics for
   </span>
   <span>
    unsupervised
   </span>
   <span>
    dialogue systems -&gt; not task focused. Correlation between automatic metrics and human ratings
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h2 id="h.9u2lsbvmk08z">
   <span>
    System as a whole
   </span>
  </h2>
  <p>
   <span>
    Ground-truth based (comparing with a predetermined output. Using some already available datasets, but domain problems)
   </span>
  </p>
  <ul>
   <li>
    <span>
     Continuous sequences of correct actions from the beginning of the dialog
    </span>
   </li>
   <li>
    <span>
     Dialogue success rate
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Objective measures:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Duration of conversation
    </span>
   </li>
   <li>
    <span>
     Length of sentences
    </span>
   </li>
   <li>
    <span>
     Uptime of the bot
    </span>
   </li>
   <li>
    <span>
     Errors in time unit
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Detecting from chat:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Sentiment detection: not always applicable (“cold sentences”)
    </span>
   </li>
   <li>
    <span>
     User feedback: ok/thanks
    </span>
   </li>
   <li>
    <span>
     Thumbs up/down at the end of conversation → must be shown to user
    </span>
   </li>
   <li>
    <span>
     User repeats question → means the system didn’t catch it, but dangerous, maybe user wanted again updated data (can distinguish on time passed: short time could show that system didn’t catch)
    </span>
   </li>
   <li>
    <span>
     Not recognised intent→ could be
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Out of domain
    </span>
   </li>
   <li>
    <span>
     Into the domain but unforeseen
    </span>
   </li>
   <li>
    <span>
     Number of error messages
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     System turn duration (to generate a response)
    </span>
   </li>
   <li>
    <span>
     Task completion time (could compare against existing app)
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Survey-based:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Overall evaluation:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Usefulness
    </span>
   </li>
   <li>
    <span>
     Usability
    </span>
   </li>
   <li>
    <span>
     Relevance of results
    </span>
   </li>
   <li>
    <span>
     Missing features
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Per-interaction feedback (C. Chakrabarti, G.F. Luger / Expert Systems with Applications 42 (2015) 6878–6897):
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Grice’s maxims:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Quality: informations are good
    </span>
   </li>
   <li>
    <span>
     Quantity: appropriate quantity of informations
    </span>
   </li>
   <li>
    <span>
     Relation: relevant to context and to the topic of conversation
    </span>
   </li>
   <li>
    <span>
     Manner: direct and straightforward
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Solving problems
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     The bot asked the required information
    </span>
   </li>
   <li>
    <span>
     The bot kept conversation on-topic (coherence)
    </span>
   </li>
   <li>
    <span>
     The bot solved issue
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Evaluation of personalization: when giving a personalized response, check if user prefers the basic one or the customized one
    </span>
    <hr style="page-break-before:always;display:none;"/>
   </li>
  </ul>
  <h1 id="h.b8i4k637cy5o">
   <span>
    Conclusion
   </span>
  </h1>
  <div>
   <p>
    <span>
    </span>
   </p>
  </div>
  <hr/>
  <div>
   <p>
    <a href="#ftnt_ref1" id="ftnt1">
     [1]
    </a>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref2" id="ftnt2">
     [2]
    </a>
    <span>
     Bengio, et al. (1994) Learning long-term dependencies with gradient descent is difficult
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref3" id="ftnt3">
     [3]
    </a>
    <span>
     S Hochreiter, J Schmidhuber - Neural computation, 1997
    </span>
   </p>
  </div>
 </body>
</html>