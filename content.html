<html>
 <head>
  <meta content="text/html; charset=utf-8" http-equiv="content-type"/>
 </head>
 <body>
  <div>
   <p>
    <span>
    </span>
   </p>
  </div>
  <h1 id="h.k60jpmft2v71">
   <span>
    Introduction
   </span>
  </h1>
  <p>
   <span>
    Introduce the problem and the motivation
   </span>
  </p>
  <ul>
   <li>
    <span>
     Machine reads (NLU)
    </span>
   </li>
   <li>
    <span>
     Machine replies (retrieve informations, provide answer)
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Brief summary of previous works
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Specific objectives
   </span>
  </p>
  <p>
   <span>
    The goal of the thesis is two-fold:
   </span>
  </p>
  <ul>
   <li>
    <span>
     conduct a survey of technologies enabling to build a textual chatbot
    </span>
   </li>
   <li>
    <span>
     develop the system able to handle a basic conversation providing pre-computed answers
    </span>
   </li>
  </ul>
  <p>
   <span>
    depending on the user profile. For example, the system while informing about factual
   </span>
  </p>
  <p>
   <span>
    information, will offer additional and personalized information concerning the user's preferences
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Focus on:
   </span>
  </p>
  <ul>
   <li>
    <span>
     NLU
    </span>
   </li>
   <li>
    <span>
     Personalization?
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Roadmap
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h2 id="h.nv5rf5sxkhne">
   <span>
    Autonomous systems in the society
   </span>
  </h2>
  <p>
   <span>
    Skeleton (remove me) about autonomous systems:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Sci-fi literature of AI (not as section, but as introduction to this section)
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Topics
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     human vs machine fight
    </span>
   </li>
   <li>
    <span>
     humanity of machines (what makes the difference)
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    If we think about independent systems that live together with humans, a lot of science fiction stories may come to our minds. From this literature we can extract some major themes that may reflect how the society can see the development of autonomous systems.
   </span>
  </p>
  <p>
   <span>
    One of them is surely the problem of dominance and control. There is a fight (real or implicit) between the humans and the machines. Another very important topic that emerges is about self-awareness and what distinguishes humans from machines.
   </span>
  </p>
  <p>
   <span>
    TELL MORE? ADD EXAMPLES? WHICH ONES?
   </span>
  </p>
  <p>
   <span>
    All those are stories that, being part of science fiction, are mostly far from reality. But since technology is advancing faster and faster, an analysis should be done on the consequences it can have on the society, and how the existence of some principles could turn those advances into empowering tools for the humans and not something to be afraid of.
   </span>
  </p>
  <p>
   <span>
    These concepts have to be analyzed and some principles must be known and followed when designing autonomous systems. Also if the systems analyzed in this work are far from generic artificial intelligence (see next section), because they are designed to solve very narrow problems, those principles have to be analyzed at the beginning.
   </span>
  </p>
  <p>
   <span>
    For this reason, after giving a brief introduction to Artificial Intelligence and autonomous agents, the discussion will go on some guidelines that are of common sense and have been analyzed in scientific literature (papers).
   </span>
  </p>
  <h3 id="h.ana2zm6bt6x">
   <span>
    Artificial Intelligence and autonomous systems
   </span>
  </h3>
  <p>
   <span>
    History of AI.
   </span>
  </p>
  <p>
   <span>
    The development of artificial intelligence has its roots back in the 50s. The idea is to build an artificial brain, inspired by the human
   </span>
   <span>
    brain
   </span>
   <sup>
    <a href="#cmnt1" id="cmnt_ref1">
     [a]
    </a>
   </sup>
   <span>
    . Empowered by the studies carried in those years about neurons and synapses, imitating those elements and their connection became an active topic of research. The first machines trying to achieve this task didn’t use computers, but were completely controlled by analog circuitry. The first neural network machine was built by Marvin Minsky in 1951 [SNARC].
   </span>
  </p>
  <p>
   <span>
    With the goal of exploring how the human brain works, the neuroscience community has done a lot of progresses with the advance of years, and the studies are still open trying to find the biological basis of information processing (take a look at Towards general AI about the “human brain project”).
   </span>
  </p>
  <p>
   <span>
    While the first experiment were carried out in 50s, on the philosophical side Alan Turing published a paper
   </span>
   <sup>
    <a href="#ftnt1" id="ftnt_ref1">
     [1]
    </a>
   </sup>
   <span>
    in which he sustains that a “thinking machine” machine could be built. The criterion defined to distinguish the thinking process is based on human judgement: if the machine is not distinguishable from another human being in a conversation over a teleprinter, we can say that the machine is able to think. This setting is the so called “Turing Test”. In this test two players (a human and a machine) are tested against a human interrogator, that tries to understand their nature. A machine that passes this test is said to be intelligent.
   </span>
  </p>
  <p>
   <span>
    As we can see, all is based on imitation of humans: at detailed level, the artificial neural networks try to mimic the behaviour of the human brain. If the intelligence of a human is located in its brain, a good reproduction of it can potentially have the same intelligence of the original one. And the root of the abilities of the brain stands in its structure. At more higher level of abstraction, a system can be seen as intelligent if it can emulate a human characteristic (such as holding a conversation over a teleprinter) well enough to convince the interrogator.
   </span>
  </p>
  <p>
   <span>
    This can be seen as a
   </span>
   <span>
    first definition of artificial intelligence: the simulation of human intelligence by machines
   </span>
   <span>
    . This imitation also is reflected in the shapes of robots, because a more human-like appearance is a faster way to immediately feel more human. Having a face that can emulate some expressions is an active field of work in the robotics. As we know, the interaction is not bound to what is said in communication, but has its strength in multimodality, combining visual and audio channels.
   </span>
  </p>
  <p>
   <span>
    As humanoid examples of robots, we can start with Sophia, developed by Hanson Robotics, which has been granted citizenship by Saudi Arabia recently. This is an examples that is linked to many interesting topics. For some, this action “set a bad precedent for how we might treat robots in future”
   </span>
   <sup>
    <a href="#ftnt2" id="ftnt_ref2">
     [2]
    </a>
   </sup>
   <span>
    and this will be covered in the next paragraph [Guidelines]. Secondly, the questioning of how much of this robot is real Artificial Intelligence and how much is just the emblematic representation of AI hype, that will be covered in the section [AI of today] to examine the big question of Artificial General Intelligence.
   </span>
  </p>
  <p>
   <span>
    But is really the human intelligence the best an autonomous system can achieve? In the world of today, with the enormous quantity of data available, and with the increasing processing power of computers, an artificial system can be seen as smart because combines and extracts the informations available very fast and in a way that is useful to the humans using it.
   </span>
  </p>
  <p>
   <span>
    Having a good distinction between what is human intelligence and what artificial intelligence could be, can help building a roadmap about how the society wants/will like to be empowered by technology.
   </span>
  </p>
  <h3 id="h.xvzfv4qbd6au">
   <span>
    Guidelines
   </span>
  </h3>
  <p>
   <span>
    This paragraph is about some general guidelines that should be followed when designing autonomous systems that are interacting with humans.
   </span>
  </p>
  <p>
   <span>
    On the previous paragraph it has been said that keeping a good distinction between the human intelligence and the intelligence provided by artificial systems, and understanding the advantages of both, could help focusing on the desired scope of the technology. Following this vision, we can say that artificial intelligence can provide an improvement to the society of today by being a set of useful tools.
   </span>
  </p>
  <p>
   <span>
    Letting the machines doing the stuff they can do better, such as automatic and repetitive tasks as has been done in factories by substituting human employees with robots in tasks like assembling of products, is needed in a word where fast production is a crucial element. Fields where this substitution is happening is with call centers, where the work is most repetitive and can be automated. While the automation in previous years has been focused on mechanical fields, now it is shifting to conversational fields.
   </span>
  </p>
  <p>
   <span>
    While machines do this works, the humans can focus on works they like more and on more decisional processes.
   </span>
  </p>
  <p>
   <span>
    All where there is a distinction there is a point of contact, an interface, where the two sides meet and interact. This boundary should be present on different dimensions that are examined in this paragraph: distinguishability, autonomy and personality.
   </span>
  </p>
  <h4 id="h.2f4sbzb19wzh">
   <span>
    Distinguishability
   </span>
  </h4>
  <p>
   <span>
    Going in a direction that seems contrary to the human emulation that can be observed, we can put as a first general rule the distinguishability of autonomous systems. Following the “Turing’s red flag law” analyzed by Walsh 2016, an autonomous system should be designed in a way to make clear that is not a human, and identify itself at the start of any interaction with other agent
   </span>
   <sup>
    <a href="#ftnt3" id="ftnt_ref3">
     [3]
    </a>
   </sup>
   <span>
    . In this article, the author is taking the expression “red flag law” from the Red Flag Act contained in the Locomotive Act
   </span>
   <sup>
    <a href="#ftnt4" id="ftnt_ref4">
     [4]
    </a>
   </sup>
   <span>
    , that stated that a self-propelled vehicle had to be led by a pedestrian waving a red flag or carrying a lantern to warn bystanders of the vehicle's approach. The term is modified by adding the artificial intelligence topic by referencing the author of the so largely known test.
   </span>
  </p>
  <p>
   <span>
    Walsh stands this principle in two parts. In the first one, the design itself of the system should be done by keeping in mind that the product is unlikely to be mistaken for human. This applies to the case of self-driving vehicles, that should be recognizable so that other actors on the road can have a more precise knowledge of the surrounding environment. This is crucial because the behaviour of human drivers and autonomous ones can be very different: both sides can make errors but of different kinds, a human can be distracted or fall asleep while a bot can do mistakes in situations it has not been designed to work.
   </span>
  </p>
  <p>
   <span>
    The second part is about stating the nature of the agent at the beginning of every conversation. This has to be done to be distinguished and put the interlocutor in the right setting and mood. Knowing the source of words is very important.
   </span>
  </p>
  <p>
   <span>
    The article also reports some examples for this specific part of the law. First of all with virtual assistants, that nowadays are so popular. Walsh observes that this rule is not always respected: if you ask Siri if she is human or not, the answer is not so clear. The playwright did that in order to keep a funny and unpredictable character of the assistant, but pretending to be human is a dangerous precedent. Now it is clear that they are AI, but with technological progress the difference could become unnoticed. Another example is with online games: bots can have some advantages and disadvantages, but user should know what kind of player they are playing with. Also when reading computer-generated text it should be explicit that the writer was not human: depending on the domain, this can impact the emotions of the reader.
   </span>
  </p>
  <p>
   <span>
    This criterion of distinguishability is not itself limiting the expressive power that the autonomous systems can have. It’s simply asking to make clear what species belongs the interlocutor.
   </span>
  </p>
  <h4 id="h.vt0e19a42ak8">
   <span>
    Autonomy
   </span>
  </h4>
  <p>
   <span>
    Another dimension in which there should exist a fixed barrier is the decisional one. On this dimension there should be a limit on what can be decided autonomously by the artificial agents in a way to establish on the one side the control of humans and their safety, and on the other one to allow some smart actions that improve the experience for humans. Because the goal all systems should be of this nature. Then the problems falls on how to decide what is good in an environment composed of different people with different goals. This problem is intrinsic in the society, also when no autonomous systems exist. Adding those presences, their function of autonomy must be aligned with the values and rules of the society.
   </span>
  </p>
  <p>
   <span>
    If their autonomy is none, there is no risk for the humans but there may be no advantages at all. With some degrees of freedom, regulamented in the right way for example always letting human overrides and never hurting anyone, a good improvement can be done.
   </span>
  </p>
  <p>
   <span>
    A good regulamentation by the governments, maybe in a internationalized context, can help defining the boundaries in this dimension.
   </span>
  </p>
  <p>
   <span>
    As an example, let’s take again the self-driving car. In the majority of the world this technology is not allowed to drive independently. Advanced cruise controls with lane assist can operate only if a driver with a licence is sitting on the driving place, ready to intervene in danger situations. Those kind of rules are necessary because those technology may not be ready and in general can find unexpected situations where they may fail.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    However in some cases there can be conflicts between what the user asks and the principle the system was designed with. This should be the only situation in which the system must disregard a command. If the system decides things on his own, also without conflicting with commands, this can be seen as a threat to human superiority on machines. The autonomy can be seen as menacing the safety and wellbeing of humans or also material resources
   </span>
   <sup>
    <a href="#ftnt5" id="ftnt_ref5">
     [5]
    </a>
   </sup>
   <span>
    . It’s a hierarchy problem. The humans should stay in control. For this reason it would be better if autonomous systems in decisional processes are used only as advisors to empower humans to take smarter decision, not directly deciding on their own.
   </span>
  </p>
  <h4 id="h.9fg2s2vum9m4">
   <span>
    Personality
   </span>
  </h4>
  <p>
   <span>
    AI can be seen as a threat to humans not only in the autonomy dimension but also in more symbolic ways. Without establishing a strong discrimination between human and machine, the concept of identity and distinctiveness are threatened
   </span>
   <sup>
    <a href="#ftnt6" id="ftnt_ref6">
     [6]
    </a>
   </sup>
   <span>
    .
   </span>
  </p>
  <p>
   <span>
    Both kinds of threat lead to negative attitudes towards robots and robotics research.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Having an artificial intelligence with some personality makes interactions more interesting and seamless for users. A boundary needs to be defined in order to avoid emotional attachment and other inappropriate feelings towards machines. From social point of view, people need to keep their life in real world with real people.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    These guidelines should put the design of autonomous systems oriented towards the wellbeing of humans, helping with repetitive tasks and providing powerful services and enhancing the connection between humans. All this should interface in a easy natural way for the humans, using natural language both in spoken and in written forms.
   </span>
  </p>
  <h3 id="h.iw2zwj3y191q">
   <span>
    AI of today
   </span>
  </h3>
  <p>
   <span>
    In this subsection the topic of the evolution of Artificial Intelligence will be expanded a bit: starting from the good results it has achieved in some narrow tasks, a description of emerging Machine Learning techniques will open the path to the General Intelligence topic.
   </span>
  </p>
  <h4 id="h.sdw3y7hhmz3n">
   <span>
    What AI can do
   </span>
  </h4>
  <p>
   <span>
    TODO
   </span>
  </p>
  <p>
   <span>
    <a href="http://pages.cs.wisc.edu/~dyer/cs540/handouts/deep-learning-nature2015.pdf">
     http://pages.cs.wisc.edu/~dyer/cs540/handouts/deep-learning-nature2015.pdf
    </a>
   </span>
   <span>
    many citations
   </span>
   <span>
    contains some examples and methods
   </span>
  </p>
  <p>
   <span>
    explore some key fields, no technicism:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Image processing
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Object recognition
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Games
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Deep reinforcement learning (Human-level control through deep reinforcement learning
    </span>
    <span>
     <a href="http://www.davidqiu.com:8888/research/nature14236.pdf">
      http://www.davidqiu.com:8888/research/nature14236.pdf
     </a>
    </span>
    <span>
     ) learns to play large classes of simple video games from just frames of pixels and the game score
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     NLP
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Speech recognition
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     control
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <ul>
   <li>
    <span>
     “social bot” can be used for different purposes:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Fake user profiles
    </span>
   </li>
   <li>
    <span>
     Crawlers that analyze social profiles
    </span>
   </li>
   <li>
    <span>
     Autonomous agents that provide some kind of service
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Good results on tasks:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Element recognition in images (also segmentation)
    </span>
   </li>
   <li>
    <span>
     Translation
    </span>
   </li>
   <li>
    <span>
     Sentiment analysis
    </span>
   </li>
   <li>
    <span>
     Voice control
    </span>
   </li>
  </ul>
  <h4 id="h.vq46acbre51d">
   <span>
    The evolution of machine learning
   </span>
  </h4>
  <p>
   <span>
    All those results have been achieved thanks to the evolution of the Machine Learning techniques.
   </span>
  </p>
  <p>
   <span>
    A first division of them can be done by considering if the desired output is provided or not in the training set: the distinction is between supervised techniques (that have input-output pairs) and unsupervised ones (that have only inputs).
   </span>
  </p>
  <p>
   <span>
    The principle at the basis of supervised ML techniques is to show examples to a system that learns how to obtain the desired output. Different approaches exist: decision trees, association rules, neural networks, ...
   </span>
  </p>
  <p>
   <span>
    About Artificial Neural Networks, their power stands in being able to model non-linear relationships and for this reason are very general and applicable in different ways. They have tunable parameters that are learnt by using the Backpropagation algorithm. From the errors on the predictions with respect to the true values, the source of errors is found by evaluating the backpropagation of the gradients, and the tunable parameters are modified in order to reduce this error by using some Gradient Descent techniques.
   </span>
  </p>
  <p>
   <span>
    TOY EXAMPLE WITH SOME NUMBERS, SHOW BACKPROPAGATION
   </span>
  </p>
  <p>
   <span>
    Given this mechanism to automatically learn from examples, there has been an evolution considering the structure itself of the network (adding more and more layers), on the training algorithms and on the kind of input that are fed.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    From features to deep ML techniques
   </span>
  </p>
  <p>
   <span>
    Neural Networks come in the field of Artificial Intelligence as systems that can model some non-linear functions and substitute handcrafted rules in classification tasks. The first generation of this approach was based on a strong definition of input features, that were defined manually and inputs are annotated with a lot of them. This manual specification of input features requires a lot of effort in their design and in data annotation, and for this reason is not easily applicable to new problems. For some problems the features themselves can become quite complex and difficult to generate.
   </span>
  </p>
  <p>
   <span>
    Given those reasons, and also considering the advances of hardware that allow more computation power at reduced costs, the trend of Machine Learning has gone towards Deep Learning that uses several layers of computation in the networks. This increase in number of layers and in complexity of the computational graph allows to reduce the work done on input features, allowing rawer data to be fed into the networks. In Deep Learning techniques, features to more high level layers are extracted by the lower levels of the network. This reduces the work of feature engineering but on the other hand requires more training samples in order to understand how to extract the relevant features.
   </span>
  </p>
  <p>
   <span>
    This shift from simple networks with complex and elaborated features towards complex networks with simpler inputs can be observed also in the fields of Natural Language Understanding, that will be covered in
   </span>
   <span>
    <a href="#h.y0t2ac4xdn82">
     this section [link to nlu]
    </a>
   </span>
   <span>
    .
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Evolution of the approaches
   </span>
  </p>
  <p>
   <span>
    From ML to RL
   </span>
  </p>
  <p>
   <span>
    Having deep learning techniques however does not change the applied approach: it is always based on the observation of inputs and output pairs and learning a statistical model that will be able to predict something with sufficiently similar inputs. Random examples are presented to the computational graph that learns how to imitate them.
   </span>
  </p>
  <p>
   <span>
    Another approach that is relevant to mention is the Reinforcement Learning, that instead of statically seeing examples is more active also in the training epochs. Being able to act and observing the consequences on some rewarding functions it is able to learn what’s good and what’s not towards a certain goal. This is the field where computers recently have defeated human opponents in popular games, such as AlphaGo. In this example the reinforcement was applied also to matches between two computers.
   </span>
  </p>
  <p>
   <span>
    This dynamic training in which the machine is able to act and observe the consequences is also at the basis of Adversarial Training and other techniques that involve multiple agents that interact and each one of them has its own rewarding function to optimize.
   </span>
  </p>
  <p>
   <span>
    With Reinforcement Learning the systems involved are able to interact with the environment to obtain desired things. However what is unique to human intelligence is the ability to perform retrospective reasoning, in other words to truly understand the association between causes and consequences and being able to provide answer to associational questions.
   </span>
  </p>
  <p>
   <span>
    As [Pearl]
   </span>
   <sup>
    <a href="#ftnt7" id="ftnt_ref7">
     [7]
    </a>
   </sup>
   <span>
    analyzed, those three levels of reasoning (seeing, doing, reasoning) have strong dependencies between them, and currently the Machine Learning approaches only reach the second level.
   </span>
  </p>
  <p>
   <span>
    An overview of building machines that are able to reach the third level (reasoning like humans) is provided in the next paragraph, as is a key characteristics of Artificial General Intelligence.
   </span>
  </p>
  <h4 id="h.48lulb88tpcw">
   <span>
    Towards general AI
   </span>
  </h4>
  <p>
   <span>
    The term Artificial General Intelligence, with its meaning of being able to perform tasks like humans, “involves the ability to reason, plan, solve problems, think abstractly, comprehend complex ideas, learn quickly and learn from experience. It is not merely book learning, a narrow academic skill, or test-taking smarts. Rather, it reflects a broader and deeper capability for comprehending our surroundings—"catching on," "making sense" of things, or "figuring out" what to do”.
   </span>
   <sup>
    <a href="#ftnt8" id="ftnt_ref8">
     [8]
    </a>
   </sup>
  </p>
  <p>
   <span>
    Looking at the current situation of Artificial Intelligence, the gap with respect to General AI can be felt: the examples shown previously belong to a narrow field, with strong definitions of inputs and outputs, while this definition requires a very dynamic behaviour, learning what to do in contexts that were not analyzed.
   </span>
  </p>
  <p>
   <span>
    The main capabilities that are missing, as [Lake et al.]
   </span>
   <sup>
    <a href="#ftnt9" id="ftnt_ref9">
     [9]
    </a>
   </sup>
   <span>
    point out, are:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Building causal models that can support real understanding of causes and consequences
    </span>
   </li>
   <li>
    <span>
     Understanding of physics and psychology principles, in order to enrich the knowledge that is acquired through “senses” with general principles that rule the world
    </span>
   </li>
   <li>
    <span>
     Learning to learn in new environments, generalizing knowledge. → learn with small amounts of training data
    </span>
   </li>
  </ul>
  <p>
   <span>
    The first point is the same that has been analyzed by [Pearl]
   </span>
   <sup>
    <a href="#ftnt10" id="ftnt_ref10">
     [10]
    </a>
   </sup>
   <span>
    , and underlines the fact that mostly AI is applied to solving pattern recognition problems.
   </span>
  </p>
  <p>
   <span>
    Instead the second one describes how, having a prior knowledge of some general rules, can help understand better and faster the observation done on the domain. For example, learning to track an object knowing that it is solid and coherent, can help focusing on more important features, such as the shape of the its trajectory. Or for the psychological principles, observing a video gamer knowing that is trying to seek rewards while avoiding punishment can help focusing on his tactics and more advanced features.
   </span>
  </p>
  <p>
   <span>
    For the third point,
   </span>
   <span>
    between human learning and machine learning we know that the first one is more efficient and faster to learn from few examples. The ability of learning to learn comes from the causal models that give form to richly structured knowledge. This knowledge is not simply a set of patterns, but is a set of concepts that belong to a higher level of abstraction.
   </span>
  </p>
  <p>
   <span>
    From concepts, the human brain is able not only to do the pattern recognition task, but also to generate new examples and explain what are the major discriminator of the output classes.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    The scientific communities are quite divided in opinion about if and when AGI will be reached but from some surveys, completed by participants of conferences (Philosophy and Theory of AI 2011, AGI 12) and members of associations like EETN and “The 100 Top authors in artificial intelligence by citation in all years”, the opinion seems to be that it will be reached in this century
   </span>
   <sup>
    <a href="#ftnt11" id="ftnt_ref11">
     [11]
    </a>
   </sup>
   <span>
    .
   </span>
  </p>
  <p>
   <span>
    The opinions can be so positive about the reachability of this objective thanks to all the research that has been done in order to understand how the human brain works. Different projects exist today, both European
   </span>
   <sup>
    <a href="#ftnt12" id="ftnt_ref12">
     [12]
    </a>
   </sup>
   <span>
    and American
   </span>
   <sup>
    <a href="#ftnt13" id="ftnt_ref13">
     [13]
    </a>
   </sup>
   <span>
    that aim to explore this topic in neuroscience, computing and medicine. Models are built that can simulate brains and researchers can use them to do experiments in different fields: robotics, medicine, cognition.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <ul>
   <li>
    <span>
     Use the cambridge paper
    </span>
    <span>
     <a href="https://www.cambridge.org/core/services/aop-cambridge-core/content/view/A9535B1D745A0377E16C590E14B94993/S0140525X16001837a.pdf/div-class-title-building-machines-that-learn-and-think-like-people-div.pdf">
      https://www.cambridge.org/core/services/aop-cambridge-core/content/view/A9535B1D745A0377E16C590E14B94993/S0140525X16001837a.pdf/div-class-title-building-machines-that-learn-and-think-like-people-div.pdf
     </a>
    </span>
    <span>
    </span>
   </li>
   <li>
    <span>
     Mention the hype and some examples
    </span>
    <span>
     <a href="https://dl.acm.org/citation.cfm?id%3D2770869">
      https://dl.acm.org/citation.cfm?id=2770869
     </a>
    </span>
    <span>
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    As today, artificial intelligence has become a common expression.
   </span>
  </p>
  <p>
   <span>
    But a discussion has to be done on the usage of this term: from using it with anything that contains a machine learning approach (narrow/basic AI), to the Artificial General Intelligence. Those two different uses also reflect on the division of what has been already done in very narrow tasks (that reaches very good and measurable results) versus the fuzziness of AGI.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Towards generic AI?
   </span>
  </p>
  <p>
   <span>
    Deep learning systems only work on very limited narrow tasks. They are not general purpose. To make them more general the approach seems to be: imitate how the human brain works.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    <a href="https://medium.com/@Numenta/the-secret-to-strong-ai-61d153e26273">
     https://medium.com/@Numenta/the-secret-to-strong-ai-61d153e26273
    </a>
   </span>
   <span>
    :
   </span>
  </p>
  <p>
   <span>
    Similarities/differences between artificial neural networks and the brain. Neurons use most of their synapses to make prediction. Rich predictions. The brain learns by forming new synapses. In deep learning, the connections between artificial neurons are fixed. The brain, forming new synapses can quickly learn new things without affecting previous learnings.
   </span>
   <sup>
    <a href="#ftnt14" id="ftnt_ref14">
     [14]
    </a>
   </sup>
  </p>
  <p>
   <span>
    A deep learning network can learn patterns from a lot of examples, as also the human brain does. But there is one big difference: the human brain really understands and is more able to generalize. Let’s take the image recognition process as an example. If a deep learning network is trained on recognizing the objects contained with lots of examples, at inference time there can be some test samples that are not recognized simply because the image is rotated or comes from a different perspective. Instead the human brain is more likely to recognize it correctly anyway. This seems to be because the brain learns a structured representation of the world that has not yet been imitated by deep learning
   </span>
   <sup>
    <a href="#ftnt15" id="ftnt_ref15">
     [15]
    </a>
   </sup>
   <span>
    .
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    The Hype
   </span>
  </p>
  <p>
   <span>
    Artificial General Intelligence as simply the sum of narrow AI methods?
   </span>
  </p>
  <p>
   <span>
    Sophia is a combination of a wide number of AI methods: face tracking, emotion recognition, and robotic movements generated by deep neural networks
   </span>
   <sup>
    <a href="#ftnt16" id="ftnt_ref16">
     [16]
    </a>
   </sup>
   <span>
    . But anyway this is far from general AI, because it works only in the specific field it has been designed to. The advances have been done on expanding the things that can be done, but purists of AI state that in this way the human intelligence cannot be reached. Human intelligence is more dynamic and general because it learns how to do new things in unbounded fields.
   </span>
  </p>
  <p>
   <span>
    Learn from itself
   </span>
  </p>
  <p>
   <span>
    Also the news about reinforcement learning, that learns from itself how to do tasks, is misleading because the fields this is applied is always defined a priori and the learning always occurr following a predefined rule. For example on the task of playing a specific game, a rewarding function is defined and the machine learns how to play better by challenging itself.
   </span>
  </p>
  <p>
   <span>
    <a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/building-machines-that-learn-and-think-like-people/A9535B1D745A0377E16C590E14B94993">
     https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/building-machines-that-learn-and-think-like-people/A9535B1D745A0377E16C590E14B94993
    </a>
   </span>
   <span>
    Building machines that learn and think like people. Suggests “concrete challenges and promising routes toward these goals that can combine the strengths of recent neural network advances with more structured cognitive models”. Analyzes how “despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways” → READ IT, high number of citations
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <hr style="page-break-before:always;display:none;"/>
  <p>
   <span>
   </span>
  </p>
  <h1 id="h.hohs5u6vo8mq">
   <span>
    State of the Art
   </span>
  </h1>
  <p>
   <span>
    In this section I am presenting the related works on the topic of bots. Having done some considerations about how the interaction between autonomous agents and humans has been designed and which social implication can rise, the discussion will focus on a classification of chatbots. Then the focus will go deep into technologies that can help building bots that can interact with the user using natural language. The last part will cover the topic of personalization.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h2 id="h.eoxxi1a2h13n">
   <span>
    Chatbots and their classification
   </span>
   <sup>
    <a href="#cmnt2" id="cmnt_ref2">
     [b]
    </a>
   </sup>
  </h2>
  <p>
   <span>
    Focusing more on the interaction between humans and machines, starting from the formulation of the Turing test, the problem has been analyzed and different solution have been analyzed. The terms that are commonly used for them are chatbots, conversational interfaces, virtual assistants.
   </span>
  </p>
  <p>
   <span>
    Their evolution started long time ago, with first systems that were built to emulate a natural conversation, and has lead to today’s virtual assistants that live on our smartphones and are ready to complete tasks for us.
   </span>
  </p>
  <p>
   <span>
    The approaches that have been used have evolved through time to fit different needs and to overcome challenges that arise while developing such systems.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    The choices of the approach to use depend a lot on the purpose of the bot. Bots can be designed to entertain the user in a conversation or can be designed to provide informations on a specific field.
   </span>
  </p>
  <p>
   <span>
    The discussion will first cover the common purposes of chatbots and then will focus on the main approaches used.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Existing taxonomies of agents:
   </span>
  </p>
  <ul>
   <li>
    <span>
     <a href="https://link.springer.com/content/pdf/10.1007%252FBFb0013570.pdf">
      https://link.springer.com/content/pdf/10.1007%2FBFb0013570.pdf
     </a>
    </span>
    <span>
     page11 about autonomous agents
    </span>
   </li>
   <li>
    <span>
     <a href="http://ivizlab.sfu.ca/arya/Papers/ACM/AAMS-02/Workshop%2520on%2520Embodied%2520Conversational%2520Agents/ECA%2520Designand%2520Evaluation%2520Taxonomy.pdf">
      http://ivizlab.sfu.ca/arya/Papers/ACM/AAMS-02/Workshop%20on%20Embodied%20Conversational%20Agents/ECA%20Designand%20Evaluation%20Taxonomy.pdf
     </a>
    </span>
    <span>
     table1 Major categories of embodied conversational agent research
    </span>
   </li>
  </ul>
  <h3 id="h.k02dvyfqdk5k">
   <span>
    Different chatbots for different purposes of dialogue
   </span>
  </h3>
  <p>
   <span>
    There are fields in which using apps can be quite frustrating for users. User interface forces you to fill up informations in a form-like structure. When finally you try to submit, you find out that you missed one required field. A conversational interface could simplify a lot this process, even though you are simply doing slot-filling and asking one input after the other.
   </span>
   <sup>
    <a href="#cmnt3" id="cmnt_ref3">
     [c]
    </a>
   </sup>
  </p>
  <p>
   <span>
    From the point of view of the companies, relations with the customer is the main channel to acquire and maintain customers. It is where the effort to understand user's needs should be maximum, and in many cases opting for offshore call centers may not be good. In these situations, having an automated responder could provide the service in the way you want in a scalable way.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Chatbots are also used in the field of virtual assistants. Using voice or text, we can interact on mobile devices with something that can quickly do things for us: adding events to the calendar, making phone calls and searching informations on the web. Virtual assistants provide a fast way to interact with the device, useful in situations where the users can be impossibled to interact in the visual way, for example while driving.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    INSTEAD OF CALL CENTERS
   </span>
  </p>
  <p>
   <span>
    The advantages of having those systems instead of dedicated people in call centers for customer support are many. First of all, the companies can support more customers because of the high scalability of those systems. For the majority of support requests, an automated response can be helpful. In case the system does not understand well the requests, a backup human solution can be called into action: in this way, also some very specific responses that were not designed in the requirements can be provided. The second major advantage for companies is that in this way they can have a set of responses more aligned with their guidelines. Establishing how the system should answer some types of questions once and for all, instead of having possible disinformation and misalignment of human employees.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    I
   </span>
   <span>
    NSTEAD OF APPS
   </span>
  </p>
  <p>
   <span>
    From the point of view of customers, the main advantages of conversational interfaces can be found when interacting with structured data whose criterion of navigability and search are not well known. Having an interlocutor that progressively helps refining our search, instead of filling a large form in a app, can help being more productive. Moreover, voice commands can be used also without need to look and use hands, for situations where our attention is needed for other tasks.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    VIRTUAL ASSISTANTS
   </span>
  </p>
  <p>
   <span>
    Also virtual assistants mainly fall into this category. On pre-determined domains (such as agenda management, meteo, sending messages) or also with external domains integrated by third-party apps (such as Alexa Skills, Cortana integration, ...) the assistant can help on a set of intents.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    All those scenarios can be grouped together and three main needs can be extracted:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Maintain a general conversation with the user without the knowledge of a domain (chit-chat)
    </span>
   </li>
   <li>
    <span>
     Provide informations on a specific domain (domain specific).
    </span>
   </li>
   <li>
    <span>
     Be able to connect to different sources of information and combine them to provide rich answers (Encyclopedic Knowledge)
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Those three needs have been analyzed and different solutions have been proposed.
   </span>
  </p>
  <h4 id="h.xtuxn5hkhwl2">
   <span>
    Chit-chat
   </span>
  </h4>
  <p>
   <span>
    For the first need, that correspond to the native setting of the turing test because the goal is to entertain the user making him believe that the machine really understands the conversation, the history of chatbots is quite long.
   </span>
  </p>
  <p>
   <span>
    The first chatbot ELIZA, that was built in 1966, is of this kind. It was created mainly to demonstrate the superficiality of communications and the illusion to be understood by a system that is simply applying a set of pattern-matching rules and a substitution methodology.
   </span>
  </p>
  <p>
   <span>
    ELIZA simulates a psychotherapist and, thanks to the trick of representing to the interlocutor some contents that have been previously mentioned, keeps the conversation without having an understanding of what really is said. ELIZA can be easily tested inside the text editor Emacs with the command doctor (meta-x doctor). At the time when ELIZA came out, some people even attributed human-like feelings to the bot.
   </span>
  </p>
  <p>
   <span>
    A lot of other computer programs have been inspired by ELIZA. Even a markup language has been created to express the rules that drive the conversation (
   </span>
   <span>
    <a href="#h.lxbz31wv66qa">
     see AIML paragraph
    </a>
   </span>
   <span>
    ).
   </span>
  </p>
  <p>
   <span>
    A competition has been created to reward the progresses in this field: the Loebner Prize, an annual challenge that stands in the format of the Turing tests, but restricting the topics of conversation. Judges keep parallel conversations, one with a human and one with a computer program. Judges are chosen with the criterion that they “should respond naturally, as they would in a conversation with another person”, in a way to avoid excessive sophistication. At the end the winner is the computer program that mostly convinced the judges (even without passing the Turing test). There are two more prizes in this competition: one is for the Turing test in text only interactions, the last one (the biggest one) is for passing the Turing test including visual and auditory inputs. This competition was first held in 1991, and with the years advancing the challengers have became more and more complex.
   </span>
  </p>
  <p>
   <span>
    The Loebner Prize has been criticized by experts in the field because it rewards the usage of some tricks
   </span>
   <sup>
    <a href="#ftnt17" id="ftnt_ref17">
     [17]
    </a>
   </sup>
   <span>
    , pointing the focus on imitation instead of intelligence.
   </span>
  </p>
  <p>
   <span>
    The common tricks used are
   </span>
   <sup>
    <a href="#ftnt18" id="ftnt_ref18">
     [18]
    </a>
   </sup>
   <span>
    :
   </span>
  </p>
  <ul>
   <li>
    <span>
     Let the conversation to be driven by the interlocutor. This works because most people like to talk about themselves and only want someone that listens
    </span>
   </li>
   <li>
    <span>
     Give the illusion to be listening and understanding, by including substrings of the user input
    </span>
   </li>
   <li>
    <span>
     Changing the topic when not understanding, that sometimes can be seen as paranoid
    </span>
   </li>
   <li>
    <span>
     Simulated typing, delaying the responses
    </span>
   </li>
  </ul>
  <p>
   <span>
    The details of rule-based chatbots will be explained in more detail in
   </span>
   <span>
    <a href="#h.y6rpo5m0ow5g">
     this paragraph (rule based)
    </a>
   </span>
   <span>
    .
   </span>
  </p>
  <p>
   <span>
    The evolution of chit-chat bots has recently evolved from a rule-based one to a generative approach. The key idea is to imitate existing conversation, and generate a response in a way that reflects both the training corpus both the current user turn (used as a stimulus). This approach, that will be discussed deeper in
   </span>
   <span>
    <a href="#h.4yanc26b251y">
     this paragraph (generative models)
    </a>
   </span>
   <span>
    , has its roots in using Statistical Machine Translation. The importance of the dialogue corpus, that dynamically establishes how the responses are generated, makes their availability a key element for a successful bot. For this reason these models are trained on the few datasets publicly available: tweets, reddit discussions, ubuntu dialog corpus.
   </span>
  </p>
  <h4 id="h.lofizn2zbeuf">
   <span>
    Domain specific bot
   </span>
  </h4>
  <p>
   <span>
    Another completely different type of bots is the one that provides a service on a restricted domain. It can be a natural language interface to a set of static Frequently Asked Questions, or can be linked to some dynamic content using some defined APIs. Those systems need to define the list of things they can do, and in some way map the user requests to some actions.
   </span>
  </p>
  <p>
   <span>
    Using those systems, the user should be aware of what the system can do and what cannot. Letting the user know that the bot he is interacting with can provide informations only on the domain it is built for, should be a goal of the ideators. Responses for a little chit-chat conversation can also be added to the system, but they should be used only to avoid a general “I don’t know”/”I didn’t understand”.
   </span>
  </p>
  <p>
   <span>
    Those kind of bots can provide a more natural way of interacting with companies, both in the field of customer support and in search for informations. The informations can be statically defined as pairs of questions-answers. In this case the bot has to classify the user requests and provide the answer that mostly fits it. Or there can be some dynamic information that need to be extracted from the request. In this case, in addition to sentence classification, a parameter extraction needs to be performed. These tasks are the ones that this thesis will later focus on, and are the main part of the Natural Language Understanding process, that is introduced better in
   </span>
   <span>
    <a href="#h.3r2mfopy3uiq">
     NLU empowered
    </a>
   </span>
   <span>
    and analyzed in detail in
   </span>
   <span>
    <a href="#h.n06dhtaupqp">
     NLP and NLU
    </a>
   </span>
   <span>
    .
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    As will be seen in the following section, a commonly used approach for this kind of bot is NLU based. However some less handmade approaches, more generative ones, are coming also in this field, inspired by the chit-chat domain.
   </span>
  </p>
  <h4 id="h.gpa5vfh7ly2m">
   <span>
    Encyclopedic Knowledge
   </span>
  </h4>
  <p>
   <span>
    As last type of chatbots, we can consider a more rich version of the domain specific bot. Instead of limiting on a very restricted domain, and predetermining the abilities with a static design of intents, this kind of bots tries to provide a natural language interface towards a linked Knowledge Base. The fixed-structure intent based approach seems not to be optimal when the number of possible intents is very high and cannot be predetermined. If the system allows questions with a high level of complexity, requiring the linking of informations, the number of intents quickly can explode.
   </span>
  </p>
  <p>
   <span>
    Instead of doing a classification of sentences on some pre-defined intents, the goal of those systems is to transform the user sentence in a database query (for structured knowledge) or also to extract information from unstructured knowledge (such as documents expressed in natural language). This requires a strong relation between the NL module and the Information Retrieval module. Structured Knowledge can be composed of entities and relations between them. Informations are in the form of RDF (Resource description framework).
   </span>
  </p>
  <p>
   <span>
    The sentences are analyzed using some parsers and the sentence is mapped to a set of linguistic patterns.
   </span>
  </p>
  <p>
   <span>
    Main problem:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Lexical gap: the same meaning can be expressed in different ways
    </span>
   </li>
   <li>
    <span>
     Manually designed features: the linguistic patterns are created by a domain expert
    </span>
   </li>
  </ul>
  <p>
   <span>
    The systems represented are good for question answering systems where the questions are related to known facts and actors that are publicly known and retrievable using URIs.
   </span>
  </p>
  <p>
   <span>
    <a href="http://www.semantic-web-journal.net/content/survey-challenges-question-answering-semantic-web">
     http://www.semantic-web-journal.net/content/survey-challenges-question-answering-semantic-web
    </a>
   </span>
   <span>
   </span>
  </p>
  <h3 id="h.9cp2ldxeokmq">
   <span>
    Where handcrafted rules meet the automatic (the approaches)
   </span>
  </h3>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    THE CHALLENGES
   </span>
   <sup>
    <a href="#cmnt4" id="cmnt_ref4">
     [d]
    </a>
   </sup>
  </p>
  <p>
   <span>
    For the different purposes that have been analyzed, different challenges arise depending also on the type of bot that is going to be implemented. The focus will be on the bots specific to a certain domain.
   </span>
  </p>
  <p>
   <span>
    The most important point is to classify the user sentences and to reconduct them to a set of questions that have a response by design. This challenge of
   </span>
   <span>
    understanding
   </span>
   <span>
    the user is made up of two components: one is relative to the natural language understanding of the current sentence, while the other one is relative to the contextualization of the current sentence in the environment of the dialogue. While talking, humans tend to refer to previously mentioned things, and the understanding of sentences is often difficult without placing it into its
   </span>
   <span>
    context
   </span>
   <span>
    .
   </span>
  </p>
  <p>
   <span>
    Another challenge that arises, especially on generative approaches, is the
   </span>
   <span>
    coherence
   </span>
   <span>
    of the sentences that are said. Since the training of those models is done on dialogues between multiple users with different visions and different behaviour, it may be difficult to establish a coherent personality of the bot. This issue is not present if the approach chosen makes the response generation in a static way: in this case it is sufficient to keep a coherent personality among the people writing the response templates.
   </span>
  </p>
  <p>
   <span>
    Another big issue is how to deal with unexpected questions, for which a response has not been prepared. This problem is present in rule-based approaches, in NLU empowered bots and even in generative approaches.
   </span>
  </p>
  <p>
   <span>
    The approaches used are mainly three, and their first use belongs to different epochs.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    THE EVOLUTION
   </span>
  </p>
  <p>
   <span>
    The first wave of chatbot, as mentioned before, simply matched the input sentences with some keywords and responded in a way to make the conversation look natural. From this, a lot of other agents grew up with the same technique: pattern matching. With this approach, lots of bot have been built and even a specific markup language has been developed to express the patterns and link them to some responses.
   </span>
  </p>
  <p>
   <span>
    More recent approaches instead tend to abandon the handcrafted rules towards an automatic learning from a dialogue corpus. This can be applied both to the part of natural language understanding (turn the user sentences into structured data) and to the response generation (querying Knowledge Bases and formulate responses).
   </span>
  </p>
  <p>
   <span>
    Depending on how responses are generated, those approaches can be split into two categories: declarative (that are mostly used for domain specific bots) and generative (for more general conversations). In the declarative approaches the sentences of the user are categorized on a range of pre-defined intents and some key entities are extracted. This is done by applying NLP techniques and the results of this stage are applied to the conversation manager that manages the state of the conversation and provides back answers and questions.
   </span>
  </p>
  <p>
   <span>
    In the generative ones, sequence-to-sequence models are applied and therefore the responses depend on the training dataset heavily, instead of depending on the logic decided by the conversation manager.
   </span>
  </p>
  <p>
   <span>
    The two approaches can be used together and instead of doing a rough division there can be a tunable mix of the two. For a domain specific bot, for example, may be useful to understand to some question that were not designed: having a supporting chit-chat model could come in help instead of replying with a generic fallback response. Or the bot could use the history of interactions with the user in order to understand better the current sentence.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    We can do a classification of chatbot systems based on how much has to be specified and defined in a static way versus a dynamic approach that learns from the examples provided. The two main points where this choice can be done are: the first one on the understanding and classification of responses, the second one on the generation of responses.
   </span>
  </p>
  <p>
   <span>
    In the following table we can see how the decisions on those two points define the approach used.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <a id="t.8994946a0701c99ef4d27ec2380b5e74dbd8c7bb">
  </a>
  <a id="t.0">
  </a>
  <table>
   <tbody>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        approach
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        understanding
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        generation of responses
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Rule based
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        handcrafted
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        handcrafted
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        NLU empowered
       </span>
       <sup>
        <a href="#cmnt5" id="cmnt_ref5">
         [e]
        </a>
       </sup>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        From examples
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        handcrafted
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        generative
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        From examples
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        From examples
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Da esempi + retrieval based
       </span>
      </p>
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    AGGIUNGERE dimensione complexity (in aggiunta a contenuto e metodologia
   </span>
  </p>
  <p>
   <span>
    MENZIONARE il tema della lingua
   </span>
  </p>
  <p>
   <span>
    AGGIUNGERE citazioni per ogni asse
   </span>
  </p>
  <p>
   <span>
    The choice between those possibilities has to be done based on the available data (e.g. if transcripts of existing human conversations exists), on the purpose of the system (see above, between chit-chat, service-oriented, ...).
   </span>
  </p>
  <p>
   <span>
    For the generation of responses the main activities can include:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Knowledge Base interrogation, if the purpose is to provide information on some domains
    </span>
   </li>
   <li>
    <span>
     Natural Language Generation: from templates to be filled, to generative models
    </span>
   </li>
  </ul>
  <p>
   <span>
    If the Knowledge Base is interrogated in a fixed, handcrafted way, the model is said to be retrieval-based. No matter if the understanding part is rule based or NLU. The advantages of retrieval-based models is that it is simpler to design API for accessing the informations since they are fixed. The user questions will be mapped on a predefined set of intents, with the only exception of sentences that are not classified. The fixed set of intents is a limitation in this case.
   </span>
  </p>
  <h4 id="h.y6rpo5m0ow5g">
   <span>
    Rule based
   </span>
  </h4>
  <p>
   <span>
    The first approach has both the understanding part and the response generation specified as rules. This approach was firstly used for chit-chat systems, and the number of rules can be quite big. Those rules can define theirself a response or can refer to other rules in order to reduce the amount of different actions. However this strategy is not very good because a lot of rules must be written in order to span all the possible variations of sentences. Moreover, since the majority of the work is to write those rules, and the rules highly depend on the language analyzed, this approach can hardly be applied on different languages and the process of migration from one to another requires the rewriting of the whole set of rules.
   </span>
  </p>
  <p>
   <span>
    This approach was firstly used by ELIZA. Then when used by ALICE, an extended version of ELIZA, there has been the release of AIML, that is the markup language used for expressing the rules. With the release of AIML, lots of implementations of chatbots used this approach.
   </span>
  </p>
  <h5 id="h.lxbz31wv66qa">
   <span>
    AIML
   </span>
  </h5>
  <p>
   <span>
    AIML stands for Artificial Intelligence Markup Language, and is a language that describes how the bot replies to certain inputs. Inputs are evaluated against a set of patterns and the best match (category) is chosen. The actions performed by the category can be a simple response, or can also set variables and call other categories.
   </span>
  </p>
  <p>
   <span>
    Those rules determine together the classification of the sentences and the response generation.
   </span>
  </p>
  <p>
   <span>
    A rule (in the AIML jargon called category) is made up of a pattern and a template. The pattern is responsible to identify the sentences that activate the rule, while the template is the part that manages the response: a template can be both a sentence that will be sent to the user or can be a redirection towards another rule.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 184.00px;">
    <img alt="" src="https://lh6.googleusercontent.com/R-niRLWA_Roo9Poc5T2Bm7SPFog1jJew6o6AYHrozNpgn-L4vIhOw4tbs-ECfLM-S2to3t7C9XY_BTLt0fI1glEA5YBqGO-vH4bbMohJywbWRn58Q5VsJ2-zy009TSeMxT1PmBWd" style="width: 602.00px; height: 184.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
   <span>
    In addition to those two elements, some actions to manage the state of the conversation can be added: variables can be set and read to manage the FSM and to conditionally activate rules.
   </span>
  </p>
  <p>
   <span>
    While writing those rules can be quite easy, the problem is that a lot of them must be written to manage the variations on the natural language.
   </span>
  </p>
  <p>
   <span>
    In fact the strategy used for managing the development cycle of bots empowered by AIML is a cyclical process: starting with a model with some rules, the system is tested against a group of test users collecting the interactions; at the end of the test session, the logs are analyzed and the botmasters modify and add rules to provide suitable responses. This process is called
   </span>
   <span>
    targeting
   </span>
   <span>
    . The botmaster role can be separated from the role of tester, or testers can be given privileges to modify the rules on their own.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    The disadvantages of using AIML are many. First of all, being a rule based system, a big set of rules need to be built and so a big fraction of time is spent analyzing the possible variation of the sentences instead of leaving it for more important tasks, such as focusing on the data available. A lot of rules are also needed to perform reduction on other rules. Those additional rules make very difficult to understand what is wrong when multiple reduction rules have been applied. Other problems come when the sentences contains complex queries. In this situation, it’s possible that more parameters need to be extracted from a single sentence, and since every pattern can contain a maximum of one wild character some tricks must be used.
   </span>
  </p>
  <p>
   <span>
    In general this is an approach that is good only for toy examples, and does not scale well to more complex bots.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    AIML is used on bots based on ALICE. Being the markup language released under GNU GPL license, a lot of open source tools exist, only requiring the botmasters to produce a set of AIML rules. Even online services exist that can be used to deploy an AIML bot in very few steps (for example pandorabots).
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Also if AIML was born for generic chit-chat systems and the Loebner Prize, it has been used also in domain specific bots. For example, to provide answers to Frequently Asked Questions in the domain of student support in Universities
   </span>
   <sup>
    <a href="#ftnt19" id="ftnt_ref19">
     [19]
    </a>
   </sup>
   <span>
    . The information contained in the FAQ are relative to admissions and courses informations. The AIML rules for managing conversations on these topics have been added to a base ALICE bot, and a change in the state transition management has been done in order to make the conversation stay on the desired topics, using weighted transitions that make the bot prefer to stay on the university domain. This system has been tested on students, establishing some evaluations on their satisfaction and also on the topic switching rate, in order to have some measures on the state transition management.
   </span>
  </p>
  <h4 id="h.4fbixuxd9req">
   <span>
    NLU empowered
   </span>
  </h4>
  <p>
   <span>
    Another approach that can be used to develop chatbots, instead of manually matching sentences with patterns, is to have a Natural Language Understanding engine that performs this classification in a smarter way. In this approach, instead of building rules and analyzing the structure of the sentence in order to span the different variation of sentences that should be grouped together, the objective is to have a classifier that can be trained to make this task automatically using machine learning techniques (feeding training examples, letting the internal parameters to be adjusted automatically to obtain the desired outputs).
   </span>
  </p>
  <p>
   <span>
    Natural Language Understanding however applies only to the task of understanding the sentences (turn natural language sentences into structured data), and not to the response generation. NLU is composed of two tasks: sentence classification, also called intent detection, that finds for each input sentence a predetermined type that expresses what the user wants to communicate or ask; slot filling, also called entity extraction, that is responsible to obtain from sentences some parameters that have been previously defined in their type.
   </span>
  </p>
  <p>
   <span>
    This approach is born for domain specific bots and responds to a problem that is inborn to this kind of bots: exists a big difference between the natural language and the information with fixed structure stored in Knowledge Bases. Using Natural Language Understanding techniques, an intermediate representation can be built starting from the sentences in natural language and from this representation, composed of intent and entities, it is easier to interface with the fixed structure informations that are usually reachable using some static APIs (information retrieval) and also manage the state of the dialogue, recording which intents and entities have occurred (dialogue manager).
   </span>
  </p>
  <p>
   <span>
    NLU can also be used when the dialogue manager is not statically defined, but is trained to provide the correct responses and call the right APIs over a predefined set. In this solution there are not rules defining how to track the state of the conversation, because they are inferred from the training corpus. This technique is investigated in
   </span>
   <span>
    <a href="#h.5h19s2dnfiyp">
     End to End Goal-Oriented dialog
    </a>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Maybe some examples?
   </span>
  </p>
  <h4 id="h.ufp22pmcw3dd">
   <span>
    Generative models
   </span>
  </h4>
  <p>
   <span>
    Going further with automation techniques that learn from examples, if the management of the state of the dialogue and the information retrieval and the response generation are managed without writing static rules, the approach is said to be generative. Using this approach removes the intermediate point of contact between natural language and rule based systems that is used in NLU systems. Everything is trained together in a end-to-end fashion: from the input sentences to the output sentences.
   </span>
  </p>
  <p>
   <span>
    This approach was born in the chit-chat domain, and has its strength when combined with a huge corpus of dialogues. Given the dialogues, that are sequences of sentences said by different actors, the system is trained to generate the next sentence given the current dialogue history.
   </span>
  </p>
  <p>
   <span>
    The first implementation of this strategy was trained on tweets
   </span>
   <sup>
    <a href="#ftnt20" id="ftnt_ref20">
     [20]
    </a>
   </sup>
   <span>
    , but other works have been trained on Ubuntu Dialogue Corpus [CIT NEEDED] and many other datasets.
   </span>
  </p>
  <p>
   <span>
    The technology used is the one that is used to provide translations, with some modifications. The model for the generation is trained on the corpus and learns how to generate the next sentence from the previous ones with an encoder-decoder structure. This structure, as will be seen in detail when
   </span>
   <span>
    <a href="#h.l6k85hs6qcqb">
     recurrent neural networks will be analyzed (link to why RNN)
    </a>
   </span>
   <span>
    , uses LSTM cells, that can capture features of sequences adaptively using some kind of memory. The encoder network collects the useful features while the decoder, given an initial state from the encoder and given a stimulus, begins the generation of the new sentence and stops when appropriate.
   </span>
  </p>
  <p>
   <span>
    The main advantage of this approach is that responses are generated completely in automatic way and thanks to LSTM cells can contain elements that were previously mentioned in the conversation. Being generative, they always produce an output, also with unexpected inputs. This can be good, but is in principle unpredictable. This unpredictability is the main disadvantage, that also reflects on the grammatical structure of the output sentences: also with huge corpus errors can occur. Another big disadvantage is the quantity of dialogues required to have good performances.
   </span>
  </p>
  <p>
   <span>
    To overcome those problems, some reinforcement strategies can be applied, in order to learn in a online settings also from the dialogues that occur at runtime. This is a good strategy in general, but can lead to some unwanted effects of polarity of the responses if there isn’t a supervision: the example of Microsoft Tay, that after 16 hours on Twitter was shut down because of the offensive tweets that was generating.
   </span>
  </p>
  <p>
   <span>
    Having a model trained on a wide variety of dialogues also rises the problem of the coherence of the responses: it’s easy that, without filtering and post-processing the responses, the bot replies in different ways to questions inherent to the same topic. For this reason a persona-based model can be built in order to consider also some more inputs to the generator of the responses, that can model some features of the speaker, such as background informations and speaking style
   </span>
   <sup>
    <a href="#ftnt21" id="ftnt_ref21">
     [21]
    </a>
   </sup>
   <span>
    . Those features can be encoded into distributed embeddings and be provided to the generator both in the training and in the inference time.
   </span>
  </p>
  <p>
   <span>
    Another common problem of this approach is that it tends to generate commonplace responses, because is trained to maximize the likelihood of the output for the given input. It is like providing an average response that fits the current input. A proposed solution to reduce this effect is to use Maximum Mutual Information as objective function in the neural model. This will try “to take into account not only the dependency of responses on messages, but also the inverse, the likelihood that a message will be provided to a given response”
   </span>
   <sup>
    <a href="#ftnt22" id="ftnt_ref22">
     [22]
    </a>
   </sup>
   <span>
    . Using this solution, a clear decrease in the proportion of generic responses can be observed on the outputs.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    This approach has shown good results for non-specific task dialogues, because the goal is simply to continue the dialogue and there is no need to introduce some task-specific informations (KBs).
   </span>
  </p>
  <p>
   <span>
    As an example of this approach in action, Google Smart Reply applies it to generate short email responses that can selected and completed. The implementation takes in account the incoming email to generate responses that may be used to replay
   </span>
   <sup>
    <a href="#ftnt23" id="ftnt_ref23">
     [23]
    </a>
   </sup>
   <span>
    .
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    In the field of domain specific bots, usually the responses are not generated completely from sequence to sequence models. The approaches usually make use of templates, that can be chosen by rules or by a dynamic classifier. But in some way templates are always used to provide the answers to domain-specific questions.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    The next chapter will investigate the natural language techniques that are used for domain specific bots, focusing on the tasks of intent recognition and slot filling and how to manage a dialogue in a multi-turn environment.
   </span>
  </p>
  <h2 id="h.y0t2ac4xdn82">
   <span>
    Natural Language Understanding
   </span>
  </h2>
  <p>
   <span>
    As introduced previously, NLU is the process of turning sentences into structured informations.
   </span>
   <span>
    The two tasks that involves are intent classification and slot filling. Usually those tasks are performed in statistical way, not applying a static set of rules as was done with AIML and similars.
   </span>
  </p>
  <p>
   <span>
    Natural Language Understanding belongs to the NL Processing family. NLP components are involved in text processing in very different ways.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    NLP
   </span>
  </p>
  <p>
   <span>
    Starting from the easiest component, tokenizers are part of this family. The role of a tokenizer is to split texts into tokens, that correspond to words and also punctuation (if punctuation is relevant for the current problem, or can be removed by the tokenizer).
   </span>
  </p>
  <p>
   <span>
    Another important component of NLP is Part Of Speech recognizer and Dependency Parser, that is responsible to parse (this term is also used when elaborating source code; some approaches can be taken from source code parsing, but usually the natural language grammar is more complex and is better analyzed by statistical parsing) the sentence and individuate the grammatical roles of the words.
   </span>
  </p>
  <p>
   <span>
    Another commonly used component of NLP is the Lemmatizer: its role is to reconduct to the base form all the words that derive from others. This is used to reduce the cardinality of vocabularies and simplify some tasks, for example finding the topic of the conversation.
   </span>
  </p>
  <p>
   <span>
    Another thing that is done using NLP is Named Entity Recognition: this finds occurrences in the text of some entities that belong to some types: common types are Person, Organization, Country, Location, Date, Time, Numbers, Currencies. This component usually mixes an approach based on the POS combining them with a Knowledge Base that enumerates in some way what belongs to each entity type. The Slot Filling task is quite similar to NER, and the differences will be analyzed soon.
   </span>
  </p>
  <p>
   <span>
    Other components are more focused on the sentence as a whole: Sentiment Analysis that tells if a sentence is positive negative or neutral, Summarization, Sentence Classification to find topic/complexity level/…. The intent detection task can be put in this last group.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    For doing this wide range of NLP tasks, a lot of NLP frameworks exist, using different programming languages and different approaches.
   </span>
  </p>
  <p>
   <span>
    The most important ones are:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Stanford Core NLP: a java library that provides Tokenizer, NER, POS, Dependency Parsing, Coreference Resolution for languages: English, Arabic, Chinese, French, German, and Spanish
    </span>
   </li>
   <li>
    <span>
     NLTK: a python library that focuses on Tokenizers, n-grams, POS, NER
    </span>
   </li>
   <li>
    <span>
     Syntaxnet: a neural-network NLP framework for TensorFlow. Helps building POS and Parsers. It contains pre-built models for POS and parsing. It is the most accurate parser for english, able to parse correctly garden-paths
    </span>
   </li>
   <li>
    <span>
     SpaCy: an open source NLP framework that provides Tokenizer, NER, POS, Dependencies, Word Vectors, Parsing for different languages with the goal of being ready to use with pre-built models, and also fast
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Comparison between NLTK and spaCy
   </span>
   <span>
    <a href="https://gist.github.com/rschroll/61b20c41e984a963df2870cfc9e628ed">
     https://gist.github.com/rschroll/61b20c41e984a963df2870cfc9e628ed
    </a>
   </span>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    NLP techniques are also used for processing tweets (TALK HERE ABOUT SOCIAL BOTS, crawlers)
   </span>
  </p>
  <p>
   <span>
    With the explosion of social networks, and being tweets freely available, a lot of dialogue corpus exist and can be analyzed with NLP techniques.
   </span>
  </p>
  <p>
   <span>
    “The rise of social bots”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Problem: social bots can be harmful. Beyond the problem of veracity of information, they can gain influence and become popular
    </span>
   </li>
   <li>
    <span>
     Cases:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Political influence: audience is artificially enlarged
    </span>
   </li>
   <li>
    <span>
     Market influence: fake informations are amplified without fact-checking
    </span>
   </li>
   <li>
    <span>
     Exposing private informations: this makes people not to trust social media
    </span>
   </li>
   <li>
    <span>
     Manipulate emotions: on socials, they are contagious
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Proposition: detect bots
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Auto-reposting: easy to spot with posts/tweets without sense
    </span>
   </li>
   <li>
    <span>
     More advanced bots: emulating human behavior, filling profiles with data that seems legit, interacting actively with other users. In some cases they also clone profiles of real users
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Implementation types:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Based on social media information (graph-based): “sybil” accounts are prone to have more connections with other sybils -&gt; groups. Based on assumption that legitimate users refuse to interact with unknown accounts, but this is not so true
    </span>
   </li>
   <li>
    <span>
     Based on crowdsourcing: humans analyze profiles to detect bots, and majority voting is applied (same profiles shown to different workers)
    </span>
   </li>
   <li>
    <span>
     Machine-learning: find features that are significant for discrimination. The system analyzes a set of features (network, user, friends, timing, content, sentiment) to evaluate a score using cross validation. Not able to detect cyborgs (mixture of human and bot) and hacked accounts
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Is that a bot running the social media feed?”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: understand if users perceive differently a bot agent or a human agent
    </span>
   </li>
   <li>
    <span>
     Results: bots are perceived credible, attractive and competent as humans. The cause is that users use the same way of interacting with bots and with humans. However the attraction to the human agent is higher
    </span>
   </li>
   <li>
    <span>
     Limitation: very restricted context
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Those NLP framework can be used for the specific task of NLU (that corresponds to turning the sentences into intent+entities). The other NLP tasks (syntactical analysis, POS tagging) are not main interests in the field of text understanding. However other NLP tools, such as sentiment analysis, can be used to detect the mood of the writer, as will be discussed in the personalization
   </span>
   <span>
    <a href="#h.xsjwxr36vaqb">
     section
    </a>
   </span>
   <span>
    .
   </span>
  </p>
  <h3 id="h.xsg4ur5pp4av">
   <span>
    Goal
   </span>
  </h3>
  <p>
   <span>
    As said before, from the sentences we want to:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Classify the intent (what the user wants, against a set of predefined ones)
    </span>
   </li>
   <li>
    <span>
     Extract the entities mentioned (we may want to care about only a specific set of entity types that are relevant to the abilities of the bot). This task is somewhere called slot filling, because the words that we want to extract may also not be entities. “The slot filling task is to search a document collection to fill in values for predefined slots (attributes) for a given entity.” Moreover, slots in a sentence can be designed to have a role (in order to be able to discriminate between multiple instances of the same type of entity). This feature is not present in the Named Entity Recognition task because the objective is only to recognize the entities, not to give them a role as a parameter in a programming language. For this reason the name of this task can be both “slot filling” and “entity extraction” with preference for the first one that gives more the idea of roles, while “named entity recognition” is quite unappropriate.
    </span>
   </li>
  </ul>
  <p>
   <span>
    For example, for the sentence “is Turin a supported city?” we may want to categorize it as a question that asks if a city is supported (this is the intent) and extract the word “Torino” that is the city the user is looking for (this is a slot that corresponds to an entity of type CITY or simply LOCATION depending on the granularity of entities defined).
   </span>
  </p>
  <p>
   <span>
    The first task corresponds to a classification of the user sentences to decide what is the intention of the user. Since a bot is usually designed to answer to different types of questions, this stage is responsible for finding the type of question. The slot filling task instead is a process that annotates some parts of the input sentences with the name of the corresponding slot. A slot can be thought as a field in an online form. While the intent represents the type of question, the slots of a sentence are values that the bot must be able to extract from the sentences because they are used as parameters for the application logic.
   </span>
  </p>
  <p>
   <span>
    For intent classification different approaches can be used:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Key words
    </span>
   </li>
   <li>
    <span>
     Syntactic based: the input features are hand-crafted (for example verb + object are taken into account)
    </span>
   </li>
   <li>
    <span>
     Sequence modeling (see RNN)
    </span>
   </li>
  </ul>
  <p>
   <span>
    There are different approaches for slot filling:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Syntactic-based: from the structure of the sentence (usually as a tree), a model is built to observe and learn where usually a certain type of entity is
    </span>
   </li>
   <li>
    <span>
     Statistical-based: instead of
    </span>
    <span>
     syntactically
    </span>
    <span>
     parsing the sentence, a model is built without hand-crafted features, only providing input sequences and output label sequences
    </span>
   </li>
   <li>
    <span>
     Semantic-based: the values of the entities are used to detect the entities. The entity lookup and linking is a key point in this approach
    </span>
   </li>
  </ul>
  <p>
   <span>
    Once the intent and the slots have been analyzed on the current sentence, we can have some rules at the application level that can put some constraints. For example, for a certain type of question, one or more slots can be compulsory. In this case, the conversation should make the user provide the values by prompting him some questions. These types of constraint make the interaction model more complex and require an analysis over multiple user and machine turns, as will be seen in this section (reference to “Beyond the single sentence”)
   </span>
  </p>
  <p>
   <span>
    The two tasks can be done independently, but some recent studies [this
   </span>
   <sup>
    <a href="#ftnt24" id="ftnt_ref24">
     [24]
    </a>
   </sup>
   <span>
    and this
   </span>
   <sup>
    <a href="#ftnt25" id="ftnt_ref25">
     [25]
    </a>
   </sup>
   <span>
    ] have shown that there can be benefits if they are performed together.
   </span>
  </p>
  <h3 id="h.jot16v735fof">
   <span>
    Joint slot filling and intent classification
   </span>
  </h3>
  <p>
   <span>
    This section contains a description of the different structures of neural networks that have reached the State of the Art condition and have been described in literature.
   </span>
  </p>
  <p>
   <span>
    A neural-network approach can be used for the tasks of slot filling and intent detection. The first thing that needs to be done is to point out what are the inputs and outputs of the system. The inputs to the network are the words contained in the sentence and the outputs are the intent label (for the intent classification task) and the slot labels (for the slot filling task).
   </span>
  </p>
  <p>
   <span>
    As slot labels, the slot names can be used directly but, in order to better handle multi-word slots, a commonly used format is the IOB, where the O indicates “other”, the B is the beginning of a slot and the I is the label associated with a word that continues the slot of the previous word. The IOB labels are prepended to the slot name.
   </span>
  </p>
  <p>
   <span>
    For the previous example, the output annotations could be:
   </span>
  </p>
  <p>
   <span>
    “Is(O) Turin(B-city) a(O) supported(O) city(O)?” intent=supported_city
   </span>
  </p>
  <p>
   <span>
    Once defined the inputs and outputs, we can think of a component that implements the desired functionalities.
   </span>
  </p>
  <h4 id="h.l6k85hs6qcqb">
   <span>
    Why RNN
   </span>
  </h4>
  <p>
   <span>
    Instead of using simple feed-forward networks, a lot of studies make use of networks with recurrent components. The core idea behind RNN is to make use of sequential informations. In feed-forward networks the assumption is that each input-output pair is independent from the others. For RNN, the recurrence stays in performing the same task for every element of the sequence, making the output depend on the previous steps. This idea can be seen as the RNN having memory that keeps informations from the past iterations. Specifically on the ability of remember and use in the correct way their memory, a lot of studies have been done to develop particular cells (a cell is the basic building block for RNNs).
   </span>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 241.33px;">
    <img alt="" src="https://lh5.googleusercontent.com/HvnduF8wYNcp3zfwOqKoIaPhbvuY5XfOJp194XSBL6XAEHWrbAd5vpW_sRZNQ2AfURDFBLFs1dKuPyWiWxpxT-Awqq-5VJXo4Ssh-KeuQhoIOkgRJMis15RyfAQ7B4nikoAOTfJs" style="width: 602.00px; height: 241.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    A recurrent cell is a type of cell that takes as input also its previous output. Those types of neural networks have been designed for problems where the order of inputs matters, and the length of input sequence can vary. For example, they are commonly used with sequence of words, characters, frames in a video and their goodness stands in modeling features that belong to the sequence. Unlike feed-forward nets, that consider the fixed set of inputs to generate the outputs, the recurrent nets are applied in different timesteps to elements belonging to a sequence and, thanks to the loops of their cells, keep informations from previous timesteps and the output depends on them too.
   </span>
  </p>
  <p>
   <span>
    Since the same network (and cells) are used in different timesteps, the analysis of RNN is usually performed on the unfolded version of the network: the single elements are repeated different times (one for each timestep) and the looping links are now going from the element in the previous time to the next time. In this way a recurrent network is transformed into a multi-layer feedforward network, but keeping the same weights on the unfolded elements that generate from the same recurrent cell.
   </span>
  </p>
  <h4 id="h.2p7ylki847vn">
   <span>
    Backpropagation
   </span>
  </h4>
  <p>
   <span>
    Backpropagation is the algorithm used in neural networks to update the weights, that is used also for the training of recurrent networks, with a slight modification.
   </span>
  </p>
  <p>
   <span>
    The goal of the backpropagation training algorithm is to modify the weights of a neural network in order to minimize the error of the network outputs compared to some expected output in response to corresponding inputs.
   </span>
  </p>
  <p>
   <span>
    It is a supervised learning algorithm that allows the network to be corrected with regard to the specific errors made.
   </span>
  </p>
  <p>
   <span>
    The general algorithm is as follows:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Present a training input pattern and propagate it through the network to get an output
    </span>
   </li>
   <li>
    <span>
     Compare the predicted outputs to the expected outputs and calculate the error
    </span>
   </li>
   <li>
    <span>
     Calculate the derivatives of the error with respect to the network weights
    </span>
   </li>
   <li>
    <span>
     Adjust the weights to minimize the error
    </span>
   </li>
   <li>
    <span>
     Repeat
    </span>
   </li>
  </ul>
  <p>
   <span>
    MAKE AN EASY EXAMPLE WITH NUMBERS IN A EASY COMPUTATIONAL GRAPH (+-/*)
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    BPTT
   </span>
  </p>
  <p>
   <span>
    The name of the modified algorithm is BackPropagation Through Time (BPTT) and is basically the standard algorithm applied on the unrolled version of the network. The only difference is that, since the layers correspond to different timesteps of the same cell, the weight updates in each instance are summed together.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 201.33px;">
    <img alt="" src="https://lh6.googleusercontent.com/uF6S4ni9jiDhGj956-BgzLeW95wmEnPJ5Ob7IwWOhZNiNz_KvDybtef4dYCibFQ9k80qKe7ATBu-kFx7aI3oBLHdSMbIxVa-IXkmHAmATGD8ZYO02boF8FkUG_fkxRmllFtxN_OV" style="width: 602.00px; height: 201.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <h4 id="h.wp9j82dyzgo5">
   <span>
    Cell types (simple RNN, GRU, LSTM)
   </span>
  </h4>
  <p>
   <span>
    In this paragraph the mainly used cell/unit types are presented. Those cells are the basic unit used for the following architecture. As a layer in feedforward NN is composed of neurons, a layer in a RNN is composed of a recurrent cell.
   </span>
  </p>
  <h5 id="h.1oe97uu0zy2z">
   <span>
    Simple RNN
   </span>
  </h5>
  <p>
   <span>
    The simplest recurrent cell type is a block with two inputs and two outputs. One input is the actual input at the current timestep while the other one comes from the previous timestep (or from initialization on the first time). The two outputs of the cell are equivalent, and is simply to put emphasis on the fact that one of them will go as input to the next timestep (this corresponds to the loop in the not-unfolded representation) and the other one can be passed to the next layer or used as output after applying some other functions (usually a softmax) and/or other layers (recurrent or not).
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 386.00px; height: 276.00px;">
    <img alt="rnn.png" src="https://lh6.googleusercontent.com/AZkM5BzOFANxoiegDMrhW79EXbl11EPFL08VkfsYqqU5cT2t3ZP2Kp156ps6-6kU0vjZ2sbbxBNtgw1F2-JkAzN9hBkYgmFMFBctYGdfGVVPkc8IxAbYWdSRTXe8mmzcnfralGp-" style="width: 386.00px; height: 276.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    The two inputs are concatenated and passed through a single feed-forward layer, that corresponds to a linear transformation plus a non-linear function (e.g. tanh, sigma).
   </span>
  </p>
  <p>
   <span>
    MATH FORMULATION
   </span>
  </p>
  <p>
   <span>
    Since the same cell is applied many times in time, and the recurrence loop feeds back the output as inputs, there can easily be two kinds of problem due to the fact that the weight matrix coefficients are multiplied at each timestep:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Exploding gradient: if some coefficients are greater than 1, the output values can become soon very big, making the network insensible to new inputs because is in some way saturated. The solution to this problem is using some non-linear function that limits the values not to be over the value of 1
    </span>
   </li>
   <li>
    <span>
     Vanishing gradient: if some coefficients are near to 0, the network will quickly forget previous inputs and the output won’t depend on them
    </span>
   </li>
  </ul>
  <p>
   <span>
    There are two factors that can affect the magnitude of gradients - the weights and the activation functions (or more precisely, their derivatives) that the gradient passes through.
   </span>
  </p>
  <p>
   <span>
    If either of these factors is smaller than 1, then the gradients may vanish in time; if larger than 1, then exploding might happen. For example, the tanh derivative is smaller than 1 for all inputs except 0; sigmoid is even worse and is always ≤0.25.
   </span>
  </p>
  <p>
   <span>
    Those problem have the same origin: the simple RNN is not able to manage long-term dependencies. This problem has been analyzed in detail by Bengio, et al. (1994)
   </span>
   <sup>
    <a href="#ftnt26" id="ftnt_ref26">
     [26]
    </a>
   </sup>
   <span>
    , Hochreiter et al. (1998)
   </span>
   <sup>
    <a href="#ftnt27" id="ftnt_ref27">
     [27]
    </a>
   </sup>
   <span>
    and other types of cells have been proposed.
   </span>
  </p>
  <h5 id="h.blhs0tl2bxox">
   <span>
    LSTM
   </span>
  </h5>
  <p>
   <span>
    LSTM
   </span>
   <sup>
    <a href="#ftnt28" id="ftnt_ref28">
     [28]
    </a>
   </sup>
   <span>
    is a solution that came out in 1997 in which a more complex cell is considered. The main idea is to have some gates that decide how much of the previous cell state to keep, and how much of the current input to consider for the calculation of the current state and current output.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 556.00px; height: 286.00px;">
    <img alt="rnn (2).png" src="https://lh4.googleusercontent.com/VmRxc0aSR7AZOeJulPfTqmsqy4bm3SZLc1AMIBH8sxs6bxEIbgiFa3w0CSoWJnF9i4mSjSaplDp_60BRsJmOoHxooaMNtjqEkGSFdCJlXRMMvG5wfbvcY1BDY0HC-M927lvi9NTL" style="width: 556.00px; height: 286.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    The gates are ft it and ot, that are:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Forget gate: decides how much of the previous hidden state to keep
    </span>
   </li>
   <li>
    <span>
     Input gate: decides how much of the current input to consider
    </span>
   </li>
   <li>
    <span>
     Output gate: decides how much of the hidden state is exposed to the output
    </span>
   </li>
  </ul>
  <p>
   <span>
    All those gates are implemented with single layer feedforward networks. This type of RNN is able to manage better the long-term dependencies, at the expenses of having four times the parameters. But with sufficient training examples, the network is able to learn how to output the correct values and how to mix the different inputs.
   </span>
  </p>
  <p>
   <span>
    Of this cell exist many implementation, the most common is the basic LSTM that is shown in the picture. Many variations exists (with peephole connections, or other variations on the gates).
   </span>
  </p>
  <p>
   <span>
    MATH FORMULATION
   </span>
  </p>
  <p>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=it%3D%CF%83%28Wi%5Ccdot%7B%7D%5Bht-1%2Cxt%5D%2Bbi%29%5C+..."/>
  </p>
  <p>
   <span>
    How LSTM solve vanishing gradient problem:
   </span>
  </p>
  <p>
   <span>
    In the recurrency of the LSTM the activation function is the identity function (the + from c
   </span>
   <span>
    t-1
   </span>
   <span>
    to c
   </span>
   <span>
    t
   </span>
   <span>
    ) with a derivative of 1.0. So, the backpropagated gradient neither vanishes or explodes when passing through, but remains constant.
   </span>
  </p>
  <p>
   <span>
    The effective weight of the recurrency is equal to the forget gate activation. So, if the forget gate is on (activation close to 1.0), then the gradient does not vanish. Since the forget gate activation is never &gt;1.0, the gradient can't explode either.
   </span>
  </p>
  <h5 id="h.32ny97nh7agm">
   <span>
    GRU
   </span>
  </h5>
  <p>
   <span>
    Later studies
   </span>
   <sup>
    <a href="#ftnt29" id="ftnt_ref29">
     [29]
    </a>
   </sup>
   <span>
    have proposed a new type of cell/unit GRU that has only two gates: reset gate and update gate that adaptively control how much each hidden unit remembers or forgets while reading/generating a sequence. The hidden state and the state cell are merged together and therefore the output gate is no more required.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 557.00px; height: 287.00px;">
    <img alt="rnn (3).png" src="https://lh5.googleusercontent.com/0R2ILdqVuioopoMyzGhthdaELUESNy2-JBatSNcENB-3xKBr0o2kfUe9xAQHy8xK3XUqyE4lhK4dl1EEYh1PTXtxlZTSdJwosNY49D-67Bjv4PU-Vb-C1fzxzbgd64aydv-vH7fM" style="width: 557.00px; height: 287.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    The advantage of this type of element with respect to LSTM is that less parameters are used. Being more recent, less studies have used them, but from the performance point of view, they seem to be of the same order of LSTM [this
   </span>
   <sup>
    <a href="#ftnt30" id="ftnt_ref30">
     [30]
    </a>
   </sup>
   <span>
    and this
   </span>
   <sup>
    <a href="#ftnt31" id="ftnt_ref31">
     [31]
    </a>
   </sup>
   <span>
    ].
   </span>
  </p>
  <p>
   <span>
    MATH FORMULATION
   </span>
  </p>
  <h4 id="h.i4cpdnpdhslm">
   <span>
    Word embeddings
   </span>
  </h4>
  <p>
   <span>
    Now that the bases of recurrent networks have been described, let’s focus on a very important point: inputs. The choice of the inputs to the neural network is very important and can affect a lot its performances.
   </span>
  </p>
  <p>
   <span>
    Why we need to define the inputs? We already have them: sentences. Sentences are made up of words and we would like to use those words as inputs to the classifier network.
   </span>
  </p>
  <p>
   <span>
    But since all neural networks only work with numbers, there must be a layer at the beginning that, given the input words, transforms them in numerical form.
   </span>
  </p>
  <p>
   <span>
    The most simple and naive approach is to consider the “one-hot” vector of the words. This representation is an array with length corresponding to the length of the input dictionary and contains values that are all zeros except for the one whose index corresponds to the index of the word in the vocabulary. In other words, a dictionary is built and the representation of the i-th word in the dictionary is an array with a single non-zero value on the i-th value. This is straightforward to implement, but has some problems because highly depends on the input dictionary: the length of one-hot vectors is the same as the length of the dictionary. Any network following this representations will have a big problem: different words, with similar meanings, will be completely different so any parameter that can be learnt on one words will not be applicable to a similar word. This approach can’t work with words that are not contained in the training set.
   </span>
  </p>
  <p>
   <span>
    A better approach is to use a representation of words that considers semantics and syntactic informations. The hypothesis behind this method is the distributional semantics: words that appear in the same context (the context is there defined as the surrounding words) are considered similar, because somehow they can be exchanged the one for the other since they appear in similar contexts. Those representation of the words are called word embeddings and are made up of dense real vectors with a fixed dimension. This dimension is a lot smaller than the size of the input dictionary. The word embeddings are usually pre-trained on large corpuses of unlabeled data (for example wikipedia or the bigger CommonCrawl). From those vectors it’s possible also to compute the similarity of two words and visualizing the word distribution on reduced dimensionality (for example plotting on a 2d plane).
   </span>
  </p>
  <p>
   <span>
    Using word embeddings as inputs to the neural network has the advantages:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Reduced size of input arrays
    </span>
   </li>
   <li>
    <span>
     Semantic and syntactic similarities of words are considered
    </span>
   </li>
  </ul>
  <p>
   <span>
    The embeddings can be part of the model (in this case the weights of the embedding layer are trainable) or can be pre-computed on external bigger corpus. The first option is preferred when the size of the used corpus is big enough, and is thought to be comprehensive enough in terms of word coverage (no unexpected new words in prediction time). Instead when the corpus of the considered problem is not big enough to model the word distribution in terms of syntax and semantics, it’s better to use pre-trained word embeddings.
   </span>
  </p>
  <p>
   <span>
    Pre-trained word embeddings can also be fine-tuned. Fine-tuning has as main advantage to reduce the loss on the desired task, but can also have some disadvantages: if two words are “near” in the pre-trained embedding space, and only one of them is used in a neural network and fine-tuning is applied, the position in the embedding space can change and move far from the other word, not trained in the model. If the other word occurs in model testing (inference), the network will have some problems because the two words are not anymore similar.
   </span>
  </p>
  <h5 id="h.ghm3u8bbffns">
   <span>
    Out-of-vocabulary
   </span>
  </h5>
  <p>
   <span>
    Having word embeddings is a good starting point. But for words that are not present in the training corpus it would be fine to have some way to infer some embeddings (maybe from the similarity to known words
   </span>
   <sup>
    <a href="#ftnt32" id="ftnt_ref32">
     [32]
    </a>
   </sup>
   <span>
    or from the context the word belongs or as in fastText considering the character n-grams
   </span>
   <sup>
    <a href="#ftnt33" id="ftnt_ref33">
     [33]
    </a>
   </sup>
   <span>
    ). Considering the local context of a word could also be useful for words that may have different meanings depending on the surrounding context.
   </span>
  </p>
  <p>
   <span>
    This is unfeasible because slows down all the process (see spacy context vectors, that are completely another type of vectors, not related and with different dimension), and in general is not required since OOV will be very few and the model should work also with them. OOV will be mainly on entity values, the main part of the sentence should be covered by vocabulary.
   </span>
  </p>
  <h5 id="h.22j2kzqci3l">
   <span>
    Not only words
   </span>
  </h5>
  <p>
   <span>
    An approach based only on words may be good for scenarios where the users don’t have the time to write complex punctuations and other stuff. But when punctuation is provided, it would be a pity to throw it away. In text: punctuation, in voice: accents and other stuff; those are additional features that may be useful for doing a better classification.
   </span>
  </p>
  <h5 id="h.utduqnbomopx">
   <span>
    Truecasing
   </span>
  </h5>
  <p>
   <span>
    A useful feature of words can also be how uppercase/lowercase is used. Simpler approaches that discard this feature do a lowercase of all the words. But capital letters can be useful in tasks like Named Entity Extraction. On the other side, beginning of sentences have capital letters that should be lowercased. For this reason truecasing model have been proposed
   </span>
   <span>
    <a href="https://dl.acm.org/citation.cfm?id%3D1075096.1075116">
     https://dl.acm.org/citation.cfm?id=1075096.1075116
    </a>
   </span>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h5 id="h.i1ebf8cnkswp">
   <span>
    The algorithms
   </span>
  </h5>
  <p>
   <span>
    TODO
   </span>
  </p>
  <p>
   <span>
    There are different algorithms of producing word vectors: word2vec (CBOW or skipgram), glove.
   </span>
  </p>
  <p>
   <span>
    <a href="https://www.quora.com/How-is-GloVe-different-from-word2vec">
     https://www.quora.com/How-is-GloVe-different-from-word2vec
    </a>
   </span>
   <span>
    predictive vs count-based
   </span>
  </p>
  <p>
   <span>
    <a href="https://www.quora.com/How-does-word2vec-work-Can-someone-walk-through-a-specific-example">
     https://www.quora.com/How-does-word2vec-work-Can-someone-walk-through-a-specific-example
    </a>
   </span>
   <span>
    other explaination
   </span>
  </p>
  <p>
   <span>
    Some have also derived word embeddings by looking at the single characters (look at fastText).
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Out Of Word (with respect to pretrained word vectors) Mimicking Word Embeddings using Subword RNNs:
   </span>
   <span>
    <a href="http://aclweb.org/anthology/D/D17/D17-1010.pdf">
     http://aclweb.org/anthology/D/D17/D17-1010.pdf
    </a>
   </span>
   <span>
    (only one citation??) found on
   </span>
   <span>
    <a href="http://blog.aylien.com/highlights-emnlp-2017-exciting-datasets-return-clusters/">
     http://blog.aylien.com/highlights-emnlp-2017-exciting-datasets-return-clusters/
    </a>
   </span>
   <span>
   </span>
   <span>
   </span>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 417.14px; height: 467.50px;">
    <img alt="" src="https://lh3.googleusercontent.com/90HtZION8cUpWoIgOVzcAyiUNNZqNmZ9S-wyS7Vaakky9ogLrbJIY0dyz_j96aE14SlQ0NmCUNuE6alNT-mfVGfpM2gxsdj0ZXhuugLxhwcsrSv8YRpjHjPvHTXjD9uxDanZDrc0" style="width: 417.14px; height: 467.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <h4 id="h.xe8uo79i8lhz">
   <span>
    Intent classification
   </span>
  </h4>
  <p>
   <span>
    Let’s talk here about approaches for the first task of NLU, intent classification, that takes as input the sentence and provides on output a label that corresponds to the intent type of the sentence. It is a multi-class classification.
   </span>
  </p>
  <h5 id="h.4hmj3bjv7gq3">
   <span>
    Keyword based
   </span>
  </h5>
  <p>
   <span>
    The first approach that can be considered is keyword-based. In this approach, for each intent type we determine a set of keywords that, if present in the current input, give a score to the selected intent type. For example …
   </span>
  </p>
  <p>
   <span>
    EXAMPLE
   </span>
  </p>
  <p>
   <span>
    This approach is not good enough because it looks only at some words in the current input sentence.
   </span>
  </p>
  <p>
   <span>
    Also it is difficult to establish which are the keywords for each class in a way that the conflicts are minimized.
   </span>
  </p>
  <p>
   <span>
    A better idea would be to compute a sentence-level representation that summarizes all the words and the meaning of the sentence, and from this vector do a classification on the output labels (intent types). For this sentence vector a lot of different approaches can be applied.
   </span>
  </p>
  <h5 id="h.dazxotng1x74">
   <span>
    Average of word vectors
   </span>
  </h5>
  <p>
   <span>
    First of all, given the word vectors for each word contained in the sentence, an average can be done. This strategy however is not good because it does not consider the order and the relationships between the words. The order matters a lot in natural language.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 316.00px; height: 381.00px;">
    <img alt="rnn (4).png" src="https://lh5.googleusercontent.com/d23uIOBMF_uSPRxH6pfMp-jPIManjpqTRCNnUverdtYPgrWXG8dF72rl-D4uQGYoktXYAQKddFj--NM1i-0dQaSbfn6sY_F-uQ7j2fb8X-Fc55VrYXI-w4bgtSfHCsOyqLIn5J3A" style="width: 316.00px; height: 381.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <h5 id="h.6dcfrbpjww3p">
   <span>
    RNN approach
   </span>
  </h5>
  <p>
   <span>
    To consider the order, a RNN can be used to summarize the sentence and produce a sentence representation that can be used in another layer to classify on the intent types. The output of the RNN is taken only at the end of the sequence. This approach, applied with bidirectional RNN, achieves good results as can be seen in the evaluation section.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 341.00px; height: 461.00px;">
    <img alt="rnn (5).png" src="https://lh3.googleusercontent.com/4tSkMuhbLUAh53q779byGpesW0cEdLAhMiIFIEs8kTuWWP_zgRqWe0MQhZEWFiD5QSHVeO27Rj8dYQKAhkItvxcCEj48hGDJeeca50RhKviF_0V1Eg09YYtxjDA_r6HK0E15Kd9R" style="width: 341.00px; height: 461.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <h5 id="h.6q4caqfmx7jf">
   <span>
    RNN + attention + other stuff
   </span>
  </h5>
  <p>
   <span>
    An attention mechanism can be used to learn a distribution over time of words that tells how much relevant is this word for the task.
   </span>
  </p>
  <p>
   <span>
    Attention used for sentiment analysis: see there
   </span>
   <span>
    <a href="https://arxiv.org/pdf/1703.03130.pdf">
     https://arxiv.org/pdf/1703.03130.pdf
    </a>
   </span>
   <span>
   </span>
  </p>
  <p>
   <span>
    Universal Sentence Representations:
   </span>
   <span>
    <a href="http://aclweb.org/anthology/D/D17/D17-1071.pdf">
     http://aclweb.org/anthology/D/D17/D17-1071.pdf
    </a>
   </span>
   <span>
   </span>
  </p>
  <h4 id="h.e7wkj7v9f1ma">
   <span>
    Sequence to sequence models for slot tagging
   </span>
  </h4>
  <p>
   <span>
    The task of slot tagging / named entity recognition instead consists of generating an output sequence in which each element corresponds to a tag for the corresponding input word. The tag can be directly the entity type or the IOB format can be used. IOB can be useful when there is need to deal with multi-word slots.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    A lot of the architectures listed below have been created for the task of Neural Machine Translation. This task takes as input the sequence of words in the source language and outputs a sequence of words in another language. This is somehow similar to the task of sequence labeling, because it maps an input sequence to an output sequence, but has some differences that make it a lot different and require a different approach. The differences will be explained in details during the analysis.
   </span>
  </p>
  <p>
   <span>
    A common characteristic of them is the presence of two key elements: an encoder and a decoder.
   </span>
  </p>
  <p>
   <span>
    The encoder is responsible of collecting all the useful features on the input sequence. The decoder instead must generate the output sequence. The differences between the different models are on the way that the encoder provides input to the decoding stage.
   </span>
  </p>
  <h6 id="h.1zr6peoxh1xl">
   <span>
    Output dependencies
   </span>
  </h6>
  <p>
   <span>
    Output labels of the words are correlated with the neighbors. For example a “I-ent” is always preceded by a “B-ent” or another “I-ent”. Also there can be some patterns that highlight that after a certain entity it is more probable to find another one. This is called output dependency, that occurs at the decoding stage, just before the production of the outputs.
   </span>
  </p>
  <p>
   <span>
    Modeling output dependencies:
   </span>
  </p>
  <ul>
   <li>
    <span>
     local choice: using this approach, each decoding step produces an output that is then projected to the labels. The decision about which label to assign is done locally, performing a simple softmax operation followed by a sampling
    </span>
   </li>
   <li>
    <span>
     feed the previous output together with the current input to decoding timestep (Jordan Network??): in this approach, the output is fed back as input to the next timestep for the decoder network, and the network in training learns the output dependencies
    </span>
   </li>
   <li>
    <span>
     linear-chain CRF: an alternative to RNN for modeling the output dependencies is using a linear-chain CRF. This can substitute the decoder network, and use the encoder to produce its input features. CRF finds the paths with higher energy in a very similar way to RNN. In other words, it finds the output sequence that is mostly probable
    </span>
   </li>
  </ul>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 311.00px; height: 366.00px;">
    <img alt="rnn (6).png" src="https://lh5.googleusercontent.com/FlzKOjyPCM7CcZBkt2CI0zAwVu8_nS2P4vd3DGJSiccNRHsw9KmTAj13yx7KO1j4qvcyASg2oZfZsdSTfaxx7zbIb1vlJix1AsG4aEKHoDiwjR_EttAcJSqjFrllVlOIvyNartOo" style="width: 311.00px; height: 366.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <h5 id="h.ahu5xsb0fuvt">
   <span>
    Simple Encoder-Decoder
   </span>
  </h5>
  <p>
   <span>
    The most simple approach is the one where the encoder collects all the word vectors and using a RNN computes a final representation of the sentence (as in the intent classification task). This representation is passed to the decoder RNN that for each timestep produces an output word.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 402.00px; height: 349.00px;">
    <img alt="rnn (8).png" src="https://lh5.googleusercontent.com/jRpx2GsVDJtyIgPL859hsPfg_FAGyK70u9kAHf-1zOkFkgKgzsn_deX75Ep1yZ_ezqFZoTnIERuVCm-qAi-RlYdW1zeFuaE5aNvaaxvLPkfFprg6gMxMjHO5W_lGKBEPf_yvhZIq" style="width: 402.00px; height: 349.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    This model has some problems. First of all, the decoding part depends only on the sentence vector c, that must be able to keep all the informations on the input sequence, whose length can vary, in a fixed size. Then, the decoding steps may easily lose the relevant information from this vector, since the decoding can take a lot of decoding timesteps. But a much greater problem comes from the lack of constraint on the output sequence: in translation between different languages, this can be a good feature, but in our task of sequence labeling we want an output sequence with fixed length. The last observation we can make is that there is no alignment model between the input and output sequences. All the output depend on all the inputs, without having different encoded informations for the decoding of the output sequence.
   </span>
  </p>
  <h5 id="h.w9da5qrt3zos">
   <span>
    Encoder-decoder keeping sentence vector
   </span>
  </h5>
  <p>
   <span>
    This model comes from a study done in 2014
   </span>
   <sup>
    <a href="#ftnt34" id="ftnt_ref34">
     [34]
    </a>
   </sup>
   <span>
    on the task of neural machine translation. It is a enhanced version of the previously considered model because the sentence vector coming from the encoding stage is passed to all the decoding stages.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 401.00px; height: 350.00px;">
    <img alt="rnn (9).png" src="https://lh5.googleusercontent.com/pZ6U9kWLWB6n7-fDkWukQt9uR9rdfWSJr-9oMvor_uNEVVtxEZOs03vY0NU3pntggFzemBVyhZmqXFlDGyCdF-7UC63vqfNlvqD_Wc80D5Z7ZptvSffEiEYvYVA5AjJSlZEJhs-Q" style="width: 401.00px; height: 350.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    This approach helps the decoding stages that, receiving the sentence vector directly, can perform better in their task. However some problems are not solved: the output sequence length has no constraints and there is no alignment model.
   </span>
  </p>
  <h5 id="h.u4i2e8o9kcra">
   <span>
    Encoder-decoder with aligned inputs
   </span>
  </h5>
  <p>
   <span>
    This model was not born for the task of translation between languages, but has been proposed
   </span>
   <sup>
    <a href="#ftnt35" id="ftnt_ref35">
     [35]
    </a>
   </sup>
   <span>
    specifically for the sequence labeling problem (that can be applied to slot filling, POS tagging). In this model the encoder sends some information to the decoder for each input word instead of sending a single vector at the end.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 476.00px; height: 399.00px;">
    <img alt="rnn (10).png" src="https://lh4.googleusercontent.com/wZWau4rhltIDzvtE--fZx65Jjxp3m6b8A6Aj-JW68_u2CI8Dfvhn9zcX5hAye5ln0C1u70Jgx7QaOoOPIo-mmcceIZnFvQ-42xiUAggCvjkktrfZsD-2EpXJ8K2dMn6QzVEG0L3H" style="width: 476.00px; height: 399.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    This model fixes the output sequence length to the length of the input sequence. The alignment model is fixed: the decisions in decoding are done looking at current input word in the current left+right context.
   </span>
  </p>
  <h5 id="h.g5uc72zc01b8">
   <span>
    Encoder-decoder with attention
   </span>
  </h5>
  <p>
   <span>
    The idea of attention empowers recent studies on translation. The purpose is to decide which output of the encoder are more relevant for the current decoding step dynamically. On the previous model, always the aligned encoded input is used, but for language translation this can be a limitation.
   </span>
  </p>
  <p>
   <span>
    Using the attention, that provides a dynamic alignment model, it’s possible to:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Determine which are the encoded inputs that are more relevant
    </span>
   </li>
   <li>
    <span>
     Use them to provide a better translation
    </span>
   </li>
  </ul>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 351.50px; height: 352.24px;">
    <img alt="rnn (11).png" src="https://lh4.googleusercontent.com/bJjT-RS8B0wuc0-dWScWANiigB5p_D1All_tS9o4AESrcZyaCSna21LSzk-leuBrpzj8FdQths7bkQaBP383YHMAfvMy4ABHmHeNHZTe2RG7GsY36w4tbFGK_MCksiaXjk1lg1GZ" style="width: 351.50px; height: 352.24px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 402.00px; height: 446.00px;">
    <img alt="rnn (12).png" src="https://lh5.googleusercontent.com/oLrVgiGmnoDt5yLp_6PU6F_c0GR23T6hFz9eMyHE02txtOSvafvrxenjgaaeoAef8ySyBfLmhkErn9EK3MByiJ9nifVWoMPr5avJSqATAvN8g0jAqmBCnlEyhOlMK2-bPCxWFYyu" style="width: 402.00px; height: 446.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    With this network, used for translations, it is useful to show how the attention maps the words in the two languages. Using a matrix representation that says which input words were used to provide the corresponding output words.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 646.67px;">
    <img alt="Screen-Shot-2015-12-30-at-1.23.48-PM.png" src="https://lh4.googleusercontent.com/4unbZt4D8t_7wToG1hZX7rAWjSgW5s28WptmDgQXCRznBdibjM1oWCsn1PnYVNkhsryroQ2GzmmLA01H6wGmbPI86tdpRkUO9Aj2Xnv2gdmFqgc5csPzjEOYcO31DfbD2SVUx_PR" style="width: 602.00px; height: 646.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
   <sup>
    <a href="#ftnt36" id="ftnt_ref36">
     [36]
    </a>
   </sup>
  </p>
  <p>
   <span>
    The attention model is quite advanced, and since a lot of other parameters are there in the network, a lot more of training samples must be used.
   </span>
  </p>
  <p>
   <span>
    For the task of sequence tagging it seems to be too much. The output labels depend only on the current word context, that is already given by the bidirectional encoder RNN, so the expected values for the attention distribution is that it will keep the inputs and outputs aligned. It can be kept together with the aligned model just to provide more features
   </span>
  </p>
  <h4 id="h.2ijwzgcp6f7s">
   <span>
    Joint SLU
   </span>
  </h4>
  <p>
   <span>
    The two tasks can be combined together in one single network in different ways and with a wide variety of additional things that can be added (for example attention mechanism, output dependencies). The approaches found in literature try to use a common encoding stage and then differentiate on the decoding, one for each task. The network has to fork at some point because the shape of the outputs is different: one single label for the intent and many slot labels, considering a single input sentence.
   </span>
  </p>
  <p>
   <span>
    In [Liu et al.
   </span>
   <sup>
    <a href="#ftnt37" id="ftnt_ref37">
     [37]
    </a>
   </sup>
   <span>
    ] there are two different proposed architectures: one is based on the encoder-decoder adding the intent output, while the other collapses all in one single compact structure.
   </span>
  </p>
  <h5 id="h.b8fu65frdqly">
   <span>
    Encoder-decoder model with aligned inputs and attention
   </span>
  </h5>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 204.00px;">
    <img alt="rnn (16).png" src="https://lh3.googleusercontent.com/1MYU7QqZRUMWkROuJiquZP6tyZ13qqULM3D2nCsOQR6PActHzHsq7LLlGZRWHwT-fvspfStScOqsQg2NM_Fz0Vacqqb9pEEouwGzwFUbRyjBDPlFlVEn9Xwi8rHJE-EQye_dGfsj" style="width: 602.00px; height: 204.27px; margin-left: 0.00px; margin-top: -0.14px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    The intent classifier is added as a branch that takes the last state of the encoder and then projects to the intent space with a single layer feedforward.
   </span>
  </p>
  <h5 id="h.m8ph9y69kpg2">
   <span>
    Attention-based RNN model
   </span>
  </h5>
  <p>
   <span>
    The other network proposed in the paper is a bit different. Instead of having a separate RNN for encoder and decoder, it has a single bidirectional RNN, that in the forward direction also has the modeling of slot label dependencies. The intent classification is done on top of the bidirectional RNN output, doing a mean pooling on the states at each timestep or, if attention is enabled, by using a weighted average.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 458.00px; height: 492.00px;">
    <img alt="rnn (15).png" src="https://lh6.googleusercontent.com/YB8miahDVN3fKIuWwNHowwbCGuQaswaAdV8iAUIaDXW8MvTlQFSYEc7GNJEGgYmflnBnHsZ0hH4kJe7AYLcMKSa4sr2laDq3BnqccIazWugKDUATsSDtI3BbXMtF7_8O_tltz9UT" style="width: 458.00px; height: 492.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h3 id="h.1lih8pa124nk">
   <span>
    Beyond the single sentence
   </span>
  </h3>
  <p>
   <span>
    Considering context (previous interactions) and multi-turn interactions
   </span>
  </p>
  <p>
   <span>
    All the previously described networks only care about the current sentence, but human natural language understanding does not limit itself on this strict form. What happens when we meet some people that are already talking about something? It may take some time to understand the topic of the discussion, we need to collect some informations as the interaction goes on to get the point of the discussion. This happens because most of the sentences are not standalone, but belong to a context that goes beyond the single sentence. This feature can be seen as some kind of memory that actors in a conversation need to keep. The context is necessary to understand the dialogue at different levels:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Understand the role of some words in a sentence given some information contained in other sentences (e.g. “To the beach” is the name of a restaurant, because Mary asked John what was the name of it)
    </span>
   </li>
   <li>
    <span>
     Understand the meaning of the current sentence (e.g. in this sentence John told Mary that the restaurant was not as good as he expected)
    </span>
   </li>
   <li>
    <span>
     Understand the topic of the conversation (a synthesized attribute, e.g. John and Mary are talking about their last experience in a restaurant)
    </span>
   </li>
   <li>
    <span>
     Linking the meanings of all the things that were discussed, and being able to answer to some questions (test)
    </span>
   </li>
  </ul>
  <p>
   <span>
    All those levels of understanding are challenges that are felt by the NLP community and have not a general established solution. They fall under different names:
   </span>
  </p>
  <ul>
   <li>
    <span>
     “multi-turn interactions”: specific to virtual assistants, is a name for the first level of understanding: the goal is to identify the intent and entities in a dialogue where the user is not using a single sentence to ask for informations. It is the case where after the initial question of the user the
    </span>
    <span>
     agent asks back for some clarifications / parameters
    </span>
    <span>
     to refine the search. Example: “Send an email to Bob” “What would you like to write?” “Hi Bob, How are you?”. In this case the assistant should put together those sentences in a structure that says: send_email(to=”Bob”, body=”Hi Bob, How are You?”). Other common cases are
    </span>
    <span>
     user follow-up questions
    </span>
    <span>
     . Receiving some results from the agent, the user could ask for more details or to change some parameters. Example: “Find me a chinese restaurant” “Those are the results for chinese restaurants” “What about italian ones?”. In this case, the user refers to the previous query and is only changing the type of restaurant.
    </span>
   </li>
   <li>
    <span>
     “Dialogue state tracking”: mostly known because of the Dialogue State Tracking Challenge. This refers to the last level of understanding a dialogue: represent the dialogue state and updating it as the conversation keeps going on. MORE ON DIALOG STATE TRACKING
    </span>
   </li>
  </ul>
  <p>
   <span>
    Different solutions exist and depend on where we want the neural network to come in contact with the application logic (what to establish as manual rules and what is inferred by the neural-network approach). Literature shows case studies where there is a strict separation between the understanding module and the management of the dialog state, but also cases where everything is put together in an end-to-end fashion in a way more independent from domain rules.
   </span>
  </p>
  <p>
   <span>
    In the first type of systems, the NLU module contains the recurrent neural network stuff and produces in output intent+entities. Once they are extracted from the dialogue, another module “dialogue state tracker” keeps trace of the conversation and applying some handwritten rules decides the flow of the conversation and provides responses back to the user.
   </span>
  </p>
  <p>
   <span>
    Instead in the end-to-end architectures, all the components are trained by dialog examples. The positive points is that there is no need to handwritten rules for the dialogue state tracker. The rules are inferred by the dialog corpus and the system learns what is the most appropriate answer to provide. Some parts are still not part of the trainable model: the module accessing the data exposes some operations via some API. The trainable model learns when to issue API calls.
   </span>
  </p>
  <h4 id="h.wttb2qhy3rtz">
   <span>
    Multi-turn SLU
   </span>
  </h4>
  <p>
   <span>
    As said before, the goal of multi-turn SLU is simply to extract the intent and the entities when the user is not providing a single sentence with all the required parameters. In a second moment, could be after the agent asks back for some parameters, other sentences complete the initial one with more entities that are used to refine the search. It can be seen as an iterative filling of a fixed-structure form. For this reason, a naive approach could be simply to classify the intent on the first sentence and then collecting the parameters in a key-value fashion (the key is the name of the slot, that is asked by the agent, and the value is the answer taken “as is”).
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 326.00px; height: 136.00px;">
    <img alt="" src="https://lh4.googleusercontent.com/pdSLTBgzqcYwLkOBPsLY2t1jtzclpNaoeXCgcqmAd0Kt7sKL_InEE_qGYUBIaPL_W9o3Fw_2ULN-1wT-F8VCds2fgwq5IiMSFeUc8Y-JvHLadfvyklsobBqz_urhzYVTWXIs7fb2" style="width: 326.00px; height: 136.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    This approach is quite simple but has some problems: this is not natural language. Natural language dialogue has not this fixed structure, and the system must be able to receive a sentence that contains more than a single entity. Another issue is that the user may change his mind in the middle of the interaction and start a new intent. For those reasons the SLU task should be expanded to the multi-turn environment by a more complete approach.
   </span>
  </p>
  <h5 id="h.h5tesva7dlyr">
   <span>
    Contextual SLU [Chen et al. 2016
   </span>
   <sup>
    <a href="#ftnt38" id="ftnt_ref38">
     [38]
    </a>
   </sup>
   <span>
    ]
   </span>
  </h5>
  <p>
   <span>
    This is an approach that has been studied for the multi-turn SLU. The idea is to additionally incorporate contextual knowledge during slot tagging.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 324.00px;">
    <img alt="" src="https://lh3.googleusercontent.com/3ae-Y63logbUiiUI-dpTT5F-hNHhWR1mrt9UDxG0yGv3KMXf4sYqxSvu-vW99N6exGBRRT2jA3Q75x1rKs8r1KiVjAZzknJIFLr9u_f5y2DIcZlwZtTzuv4LLje9IJoCvcNzeMrU" style="width: 602.00px; height: 324.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    The main difference with the encoder-decoder structures that were previously explained is that also informations from the previous sentences are provided to the decoding stage. Those information are computed by doing a weighted sum of all the previous encoded sentences. To compute the weights, that represent the level of attention that has to be kept to the relative sentences, an inner product is performed to obtain a similarity score between the current sentence and the old ones. The hypothesis behind this scoring is that sentences that have similar encoded representation should be considered more than the ones that are different.
   </span>
  </p>
  <p>
   <span>
    The results of this approach have been evaluated on a proprietary dataset and therefore the results cannot be compared with other studies on the field. Furthermore in the paper the intent network is not described at all. However, this study is interesting in the perspective of how the previous sentences are considered together with the current one for the slot-tagging task.
   </span>
  </p>
  <p>
   <span>
    A problem of this architecture is that the agent sentences are not considered. But the agent sentences could contain some keywords that may help to identify the slot in the decoding stage. For example if a trip requires a source and a destination, the agent asked for the source and the user answered with that, it is very relevant for the task of slot filling to know that the agent asked for source and not for the destination.
   </span>
  </p>
  <h5 id="h.hywob0csablx">
   <span>
    Considering time and roles [Chen et al. 2017
   </span>
   <sup>
    <a href="#ftnt39" id="ftnt_ref39">
     [39]
    </a>
   </sup>
   <span>
    ]
   </span>
  </h5>
  <p>
   <span>
    Another study by the same group has been done on the value of time and roles in conversation. About the time, the idea is that most recent sentences count more, and an attention score is given with values that fade out as the time is more remote. Instead for the roles, two similar networks are used, one for each role.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 258.67px;">
    <img alt="" src="https://lh4.googleusercontent.com/EW59TRmjAztp0pSFyaGysYbl0iJx0jbQuSU8095V6vqWn4lmV9Uafbllus6LLRnlBMmpPTp7FW-t9__cswTsN-kYRk39KRY0wGzH1A9-cDPXQ4dcMh-BzI2mlRrIPvflL8cMssbi" style="width: 602.00px; height: 258.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    We can see from this picture that the history of sentences is encoded with different networks, one for the tourist and one for the agent. Two different mechanism of attention are used: one is the content-aware, that is computed in the same way as in the previous mentioned paper with a dot product in the embedding space; the second one is the time-aware, that gives higher importance to recent sentences.
   </span>
  </p>
  <p>
   <span>
    In this case the work has been done on the DSTC4 dataset that contains human-to-human dialogues. One of them is a tourist and the other one is a guide.
   </span>
  </p>
  <h5 id="h.ohlmco880bb0">
   <span>
    Other contextual SLU
   </span>
  </h5>
  <p>
   <span>
    The following two papers show other ways to include informations from previous sentences to perform some work on the current one.
   </span>
  </p>
  <ul>
   <li>
    <span>
     Contextual domain classification in SLU using RNN:
    </span>
    <span>
     <a href="https://www.clsp.jhu.edu/~puyang/papers/rnn_dom.pdf">
      https://www.clsp.jhu.edu/~puyang/papers/rnn_dom.pdf
     </a>
    </span>
    <span>
     the previous domain classification is fed together with every word input to the CNN classifier
    </span>
    <span>
    </span>
    <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 526.00px; height: 338.00px;">
     <img alt="" src="https://lh6.googleusercontent.com/eloQnhOcbiuZD4YhNqG-ZQCBbyYPpU4oTldCI_mV_cLSqBjlvPCaN7kEQ6trxUZug6fCEgUE-AWcerLK8tZKvb0ra-6ItoRCmmVHwmxG8UNn53UebG5tOpTG_pHxG74FE7lV3Nob" style="width: 526.00px; height: 338.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
    </span>
   </li>
   <li>
    <span>
     Easy contextual intent prediction and slot detection: using CRF and HMM
    </span>
    <span>
     <a href="https://pdfs.semanticscholar.org/cd2f/0c6aac6cdf98f96984ac37d106afc599d99d.pdf">
      https://pdfs.semanticscholar.org/cd2f/0c6aac6cdf98f96984ac37d106afc599d99d.pdf
     </a>
    </span>
    <span>
    </span>
    <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 519.00px; height: 418.00px;">
     <img alt="" src="https://lh3.googleusercontent.com/4yGWJAgkDoSB0Z9WGTBAQHq03HT6Zhk6-fj9HZ_kb_B2gpCbEgtc-PljtmEjTRRKcOCxVPMB-BnyCnncjG9lgjanV1yNCXq1g3oa8BJTD1atUXcrEP1VQTCgflTw3Dns4_8XlZ3R" style="width: 519.00px; height: 418.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
    </span>
    <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 527.00px; height: 458.00px;">
     <img alt="" src="https://lh5.googleusercontent.com/M16aOfNZkvZB3swYsEkvEdc-XQpxveBoV5gex0aJmKsjxfvtjzwqezLfw34Qg5mV9DC6H9TpW5zrKgwv8g3gHrBN8Cw0GqkHtCwcMOVKWqQvtzmVJMGPFgxmvu9s6DXpWve0Jcmq" style="width: 527.00px; height: 458.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
    </span>
   </li>
  </ul>
  <h4 id="h.13sbtqu88z5j">
   <span>
    Dialogue tracking systems
   </span>
  </h4>
  <p>
   <span>
    Explain something about DSTC.
   </span>
  </p>
  <p>
   <span>
    The role of memory in interaction
   </span>
  </p>
  <p>
   <span>
    Get machines to reason, remember things, turn text and facts into operational knowledge.
   </span>
  </p>
  <p>
   <span>
    Memory-augmented networks:
   </span>
  </p>
  <p>
   <span>
    Recurrent neural networks that are augmented by memory that is a separate module but can also be trained.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 539.00px; height: 71.00px;">
    <img alt="" src="https://lh6.googleusercontent.com/2h2uKHN6Fa-2Bd3rLpruXzHvz1-DZ-aFoAR4lU0EFgzPrCJlIc5UN208NHksf1FZs1ORJSiQMjEgXgEmXCE5A-J1lu5Ddl0ajBwLxQTJ6csBzuLHtnSZpJcxS4_0_Wkoxe7TQ3Sx" style="width: 539.00px; height: 71.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    Store each turn in memory
   </span>
   <sup>
    <a href="#ftnt40" id="ftnt_ref40">
     [40]
    </a>
   </sup>
   <span>
    . Each turn is composed of a triple: subject+verb+object
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    The main goal for those systems is to keep track of a dialogue in terms of what is being said. The architecture makes use of some memory, that is a kind of “soft” hash table. It stores key-values pairs. The output of a lookup is a weighted sum of the values, where the weights are attention weights derived by the training procedure.
   </span>
  </p>
  <h4 id="h.5h19s2dnfiyp">
   <span>
    End to End Goal-oriented dialog
   </span>
  </h4>
  <p>
   <span>
    As mentioned before, this approach wants to overcome the need of domain-specific handcrafting in favour of a more generic system that can fastly be used for new domains. This approach has its origins in another type of setting: general chit-chat dialogs. From corpuses of discussions (forum threads / movie conversations) the system is trained in a end-to-end fashion to predict the next sentence. The porting from this scenario to goal-oriented ones has some issues, that generate from the fact that the conversation is intrinsically different in the purpose: the system is interrogated by the user and has to provide some information back. Beyond keeping the conversation smooth, the agent has to ask questions to the user to have a better formulation of the request, query the Knowledge Bases, interpreting the results and provide them to the user in the correct shape in order to complete a transaction.
   </span>
  </p>
  <p>
   <span>
    In this type of system the prediction are both the agent responses and the API calls.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Using this approach, not only the understanding part is turned from handcrafted rules to an
   </span>
   <span>
   </span>
   <span>
    optimization problem, but also the replying part. The API to call are determined without using fixed rules, but with a model that is determined by the training corpus. The same applies to the response generation: there aren’t fixed rules deciding what to say in responses.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Learning end-to-end goal-oriented dialog
   </span>
  </p>
  <p>
   <span>
    Paper
   </span>
   <span>
    <a href="https://arxiv.org/pdf/1605.07683.pdf">
     https://arxiv.org/pdf/1605.07683.pdf
    </a>
   </span>
   <span>
   </span>
  </p>
  <p>
   <span>
    Implementation
   </span>
   <span>
    <a href="https://github.com/vyraun/chatbot-MemN2N-tensorflow">
     https://github.com/vyraun/chatbot-MemN2N-tensorflow
    </a>
   </span>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    This system needs a fixed set of API to navigate the KB. Instead at Stanford they experimented with a data exploration that can be explored as a set of key-value pairs. In this way the system, trained in an end2end fashion will be able to explore the KB in a more dynamic way, in a content-aware way with structured data
   </span>
   <sup>
    <a href="#ftnt41" id="ftnt_ref41">
     [41]
    </a>
   </sup>
   <span>
    .
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Those kind of system make use of the memory, as the dialogue tracking systems, but with some differences. While the DST are only tracking the state of the dialogue to provide an answer at the end, that corresponds to a value in the memory, those systems are linked to a KB and can issue some API calls or simply give back the next sentence.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Reinforcement learning techniques to avoid lack of train data
   </span>
  </p>
  <p>
   <span>
    TODO
   </span>
  </p>
  <p>
   <span>
    Quick reinforcement learning
   </span>
   <span>
    <a href="https://medium.com/rasa-blog/a-new-approach-to-conversational-software-2e64a5d05f2a">
     https://medium.com/rasa-blog/a-new-approach-to-conversational-software-2e64a5d05f2a
    </a>
   </span>
   <span>
    done with interactive learning mode, providing feedback on each step to quickly reinforce the model for calling actions.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h3 id="h.iqkeiq1d8ogq">
   <span>
    NLU as a service
   </span>
  </h3>
  <p>
   <span>
    As of today, there are a lot of platforms that provide this as a service. Big companies have decided to invest on it, to be the ones that hold the technology. They made those topics extremely easy for the developers, that through a web interface can create dialogue flows and annotate manually the data. There is no need to know about the technology that is inside. All you need is to understand the jargon used by the platform and configure your black box.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Which ones:
   </span>
  </p>
  <p>
   <span>
    TALK about wit.ai, dialogflow, LUIS, IBM, …, kitt.ai
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Features provided:
   </span>
  </p>
  <p>
   <span>
    On the features provided by those platforms, usually only the natural language understanding is provided, with an approach that is not trained in a end-to-end way. The provided features turn the user sentences in structured data. Some platforms offer a way to force the user to provide some parameters (compulsory entities for some intents), managing the questions that the bot should ask when they are missing and call the fulfillment endpoint (the service owned by the developers of the bot, receiving the structured data) only when those parameters have been provided. The programmers are left to the role of handling the dialogue state, retrieving the informations and providing a response.
   </span>
  </p>
  <p>
   <span>
    But this does not scale well when the dialogue can have lots of different states and the user is left to change the topic of conversation in every moment. In those situations the task of handcrafting rules to track the state of the conversation can become quite difficult. When something in the behaviour of the bot is not correct, the investigation of the problem is quite difficult.
   </span>
  </p>
  <p>
   <span>
    Instead if the dialogue-state-tracking and the dialogue itself is trained end-to-end, changing the behaviour is as easy as adding a new example to the training corpus.
   </span>
  </p>
  <p>
   <span>
    This kind of end-to-end dialogue is supported on the shelf by RASA, an open source tool that has two main components: the NLU and the core. The first one turns sentences into intents and entities, while the second one manages the state of the conversation and determines responses and API calls without handcrafted rules. The developers of a bot using this libraries have to define the intents, entities, the actions (that correspond to some methods) and templates (for the responses) and provide examples for training both the NLU and the Core.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Measurement of the performances:
   </span>
  </p>
  <p>
   <span>
    There have been some efforts to compare the performances of all those systems by SNIPS:
   </span>
   <span>
    <a href="https://snips.ai/content/sdk-benchmark-visualisation/">
     https://snips.ai/content/sdk-benchmark-visualisation/
    </a>
   </span>
   <span>
   </span>
   <span>
    <a href="https://medium.com/snips-ai/benchmarking-natural-language-understanding-systems-google-facebook-microsoft-and-snips-2b8ddcf9fb19">
     https://medium.com/snips-ai/benchmarking-natural-language-understanding-systems-google-facebook-microsoft-and-snips-2b8ddcf9fb19
    </a>
   </span>
   <span>
   </span>
   <span>
    <a href="https://www.slideshare.net/KonstantinSavenkov/nlu-intent-detection-benchmark-by-intento-august-2017">
     https://www.slideshare.net/KonstantinSavenkov/nlu-intent-detection-benchmark-by-intento-august-2017
    </a>
   </span>
   <span>
    that tested a lot of those platforms to compare the results on intent and slots. Emphasis also on the detection of out-of-domain samples (things that the agent was not trained for), learning curve, language support, response time. Dataset used:
   </span>
   <span>
    <a href="https://github.com/snipsco/nlu-benchmark">
     https://github.com/snipsco/nlu-benchmark
    </a>
   </span>
   <span>
   </span>
  </p>
  <p>
   <span>
    EXTEND THIS SECTION
   </span>
  </p>
  <p>
   <span>
    How to bootstrap a new NLU to a new domain with limited data? Transfer Learning, pretrained embeddings,
   </span>
  </p>
  <p>
   <span>
    <a href="https://www.researchgate.net/profile/Avraham_Shinnar/publication/321664993_Bootstrapping_Chatbots_for_Novel_Domains/links/5a29ff24a6fdccfbbf81994a/Bootstrapping-Chatbots-for-Novel-Domains.pdf">
     https://www.researchgate.net/profile/Avraham_Shinnar/publication/321664993_Bootstrapping_Chatbots_for_Novel_Domains/links/5a29ff24a6fdccfbbf81994a/Bootstrapping-Chatbots-for-Novel-Domains.pdf
    </a>
   </span>
   <span>
    explores intent+slots from swagger documentation
   </span>
  </p>
  <hr style="page-break-before:always;display:none;"/>
  <p>
   <span>
   </span>
  </p>
  <h2 id="h.ltq6wupkvj0h">
   <span>
    Personalization
   </span>
  </h2>
  <p>
   <span>
    This section analyzes the state of the art of personalization, starting on how to profile users and determine their features, and considering the existing approaches for recommender systems and their problems.
   </span>
  </p>
  <h3 id="h.2del0db8mgct">
   <span>
    User features and Personality analysis
   </span>
  </h3>
  <p>
   <span>
    A personalization or recommender system in order to provide some outputs needs in input some information about the user. Those information can be provided explicitly by the user, for example by asking him to fill up a questionnaire, or can be collected implicitly, analyzing his behaviour and doing some forecasts.
   </span>
  </p>
  <p>
   <span>
    While having those features provided directly by the user is straightforward to be done, the implicit discovery is a bit more challenging. However users may not want to tell their personal details to the system to receive targeted recommendation. Is something that is not seen as good, because personal details are sensible data and no one wants to share them. Also when receiving explicit self-judgement from users, a big problem is how to trust them. For those reasons the implicit discovery of some features may help in different ways: looking at some available data and behaviour of the user, personal details can be estimated and users can be clustered together considering some dimensions that may reflect sensible data and their personality.
   </span>
  </p>
  <p>
   <span>
    With the spread of social networks a lot of data is ready to be analyzed to build big models that are able to predict personal informations. The most common nowadays, Facebook, stores informations of any kind about users: from age, occupation, and other personal informations to others relative to interests. This is the ideal setting for training models that predict some user features based on others, contained themselves on the social network or coming from other sources. Specially on the fields of social sciences and personality analysis
   </span>
   <sup>
    <a href="#ftnt42" id="ftnt_ref42">
     [42]
    </a>
   </sup>
   <span>
    .
   </span>
  </p>
  <p>
   <span>
    For example, one of the biggest datasets to perform personality analysis is the myPersonality dataset
   </span>
   <sup>
    <a href="#ftnt43" id="ftnt_ref43">
     [43]
    </a>
   </sup>
   <span>
    . Born as an application to take psychometrics tests and allowing users to give access to their personal data on Facebook, the Cambridge researchers built managed to build a model that, thanks to 6 milions volunteers, is able to predict some informations of users simply by looking at what are the Likes or by looking at some text.
   </span>
  </p>
  <p>
   <span>
    The personality is usually analyzed in five dimensions, in a model that has been widely shared, discussed and applied both in academic environments and in empirical ones. The “Big-Five Factors”, formulated by [McCrae and Costa]
   </span>
   <sup>
    <a href="#ftnt44" id="ftnt_ref44">
     [44]
    </a>
   </sup>
   <span>
    and their development traced by Goldberg
   </span>
   <sup>
    <a href="#ftnt45" id="ftnt_ref45">
     [45]
    </a>
   </sup>
   <span>
    , are the following:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Openness to experience
    </span>
   </li>
   <li>
    <span>
     Conscientiousness
    </span>
   </li>
   <li>
    <span>
     Extraversion
    </span>
   </li>
   <li>
    <span>
     Agreeableness
    </span>
   </li>
   <li>
    <span>
     Neuroticism
    </span>
   </li>
  </ul>
  <p>
   <span>
    Having good values for those traits is not an easy task, because self-judgement and also human judgement can be easily polarized. To obtain values that are somewhat objective, different criteria can be used. [YouYou et Al.]
   </span>
   <sup>
    <a href="#ftnt46" id="ftnt_ref46">
     [46]
    </a>
   </sup>
   <span>
    in their study used three different criteria.
   </span>
  </p>
  <p>
   <span>
    The first one, called “self-other agreement” is based on how much an external judger agrees with self-rating. The second one is the “interjudge agreement”, that evaluates the similarity of the ratings given by two external judger. The last one is the “external validity”, that measures the prediction on life outcomes: based on observation of facts that can be verified, a comparison is done between the personality score provided by the different actors. This last criterion is not easy to use and not always applicable.
   </span>
  </p>
  <p>
   <span>
    In this paper, their objective was to measure the accuracy of personality judgement done by computers against those made by humans. The dataset they used was myPersonality, based on user likes and attached personality tests. The results they produced established that computer-based judgements are more accurate, because they can take in account a very big quantity of data using them in statistical modeling, However human judgement can capture more subtle cues that may be ignored by automated systems.
   </span>
  </p>
  <p>
   <span>
    Another study
   </span>
   <sup>
    <a href="#ftnt47" id="ftnt_ref47">
     [47]
    </a>
   </sup>
   <span>
    , also exploiting the same dataset, makes use of the Facebook profiles in order to predict some private user information, such as ethnicity, gender and age that may be unpublished. Given other available digital traces, such as user likes, the system is able to predict accurately those informations. For recommender systems needing those features, it is no more necessary to explicitly ask them to the user, but simply knowing what a person likes those values can be inferred.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    All those works on unwilling profiling, especially considering details that users may want to hide (such as sexual and political orientation), implies a decrease in trust of online services by people that have a bit of knowledge about it.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Other works, explore ways of profiling the users given some text they produced. Different things can be discovered, from the sentiments the user is feeling to their personality
   </span>
   <sup>
    <a href="#ftnt48" id="ftnt_ref48">
     [48]
    </a>
   </sup>
   <span>
    . The requirement is having more than few words and know the environment the text belongs to, in order to remove the environmental bias. This approach applies better on social networks that are more text-based, like Twitter. Since usually the tweets are openly available, analysis on text can be easily done.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    <a href="https://web.stanford.edu/class/cs124/lec/emo2016.pdf">
     https://web.stanford.edu/class/cs124/lec/emo2016.pdf
    </a>
   </span>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h3 id="h.kfp5i756qgiq">
   <span>
    Recommendation Approaches
   </span>
  </h3>
  <p>
   <span>
    Given some representation of the users, different approaches can be used to recommend items to the users.
   </span>
  </p>
  <p>
   <span>
    <a href="https://www.quora.com/What-is-the-difference-between-content-based-filtering-and-collaborative-filtering">
     https://www.quora.com/What-is-the-difference-between-content-based-filtering-and-collaborative-filtering
    </a>
   </span>
   <span>
   </span>
  </p>
  <h4 id="h.gaetitna0e6a">
   <span>
    Content-based filtering
   </span>
  </h4>
  <p>
   <span>
    TODO
   </span>
  </p>
  <p>
   <span>
    Based on the content. Creating a model of the user’s preferences. History of interactions. Goal: match the user representation with the items representations
   </span>
  </p>
  <p>
   <span>
    See also
   </span>
   <span>
    <a href="https://link.springer.com/chapter/10.1007/978-0-387-85820-3_3">
     https://link.springer.com/chapter/10.1007/978-0-387-85820-3_3
    </a>
   </span>
   <span>
   </span>
  </p>
  <h4 id="h.npg08ioyklcj">
   <span>
    Collaborative filtering
   </span>
  </h4>
  <p>
   <span>
    What user will like based on similarities of other users. Doesn’t require “understanding” the items. Can be done using user similarity and/or items similarity.
   </span>
  </p>
  <p>
   <span>
    Assumption: people who agreed in the past will agree in the future
   </span>
  </p>
  <h4 id="h.ceqcivfparu">
   <span>
    Hybrid
   </span>
  </h4>
  <h3 id="h.gjp0hmp20g33">
   <span>
    Cold start
   </span>
  </h3>
  <p>
   <span>
    Another possible approach, without using the facebook data, would be to use a bootstrap in the
   </span>
   <span>
    cold start
   </span>
   <span>
    phase: a few questions to build a first model of the user. Then more informations can be
   </span>
   <span>
    extracted as the conversation flows.
   </span>
  </p>
  <p>
   <span>
    An explicit preference elicitation at the beginning, not too long, that then becomes implicit.
   </span>
  </p>
  <p>
   <span>
    Other systems apply this procedure (e.g. netflix)
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Facing the cold start problem in recommender systems”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: find a good solution for the user-side cold start
    </span>
   </li>
   <li>
    <span>
     Other approaches to the problem:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Content-based: requires ratings by user. Cold start problem
    </span>
   </li>
   <li>
    <span>
     Collaborative-filtering: requires other users ratings
    </span>
   </li>
   <li>
    <span>
     Explicit interview to new user about items (adapting new questions to the answers provided)
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Implementation: three phases (Collaborative-filtering)
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Classification of the new user in a specific group (based on demographic data): using C4.5 algorithm (decision tree) and Naive Bayes
    </span>
   </li>
   <li>
    <span>
     Find “neighbours” of the new user inside the group: weighted average of demographic data
    </span>
   </li>
   <li>
    <span>
     Calculation of outcome: prediction techniques
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Dealing with the new user cold-start problem in recommender systems: A comparative review”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: compare existing algorithms (collaborative filtering) on the cold-start problem. Types of systems:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Uses additional data sources (e.g. demographic data): a limitation of these systems is that sometimes data is not available (because user did not associate social profile)
    </span>
   </li>
   <li>
    <span>
     Selects a group of analogous users (without additional data sources): construct a decision tree where nodes are questions. NHSM also takes into account the global preference of user behaviors, using three factors of similarity: Proximity (how much two ratings are near), Significance (how much distant from the median) and Singularity (how the two ratings are different from others). Limitations: how to choose the optimal number of groups and splitting criteria. Must have some rating from new user (bootstrap)
    </span>
   </li>
   <li>
    <span>
     Hybrid methods:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     FARAMS: using multiple approaches (fuzzy sets and association rules).
    </span>
   </li>
   <li>
    <span>
     HU-FCF: combines analysis on demographic data (fuzzy similarity matrix) and on rating data (hard similarity matrix)
    </span>
   </li>
   <li>
    <span>
     Limitations: irrelevant users are still included in the computation of similarities
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Results: NHSM is the best one, both for accuracy and for computational time
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Learning multiple-question decision trees for cold-start recommendation”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: find a solution to the cold-start recommendation problem that maximizes accuracy and minimizes user efforts. Problem of classic bootstrap is that user usually does not know items in the first interactions. The idea is to build a tree with multiple questions at each node.
    </span>
   </li>
   <li>
    <span>
     Results:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Accuracy is better than single-question based system (because for each node/page more informations are extracted) and also better than linear-combination of multiple trees
    </span>
   </li>
   <li>
    <span>
     Time increment to answer more questions per screen is sublinear
    </span>
   </li>
   <li>
    <span>
     Time difference between rating scale and binary answer doesn’t have strong dependency on the number of questions
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Matrix Factorization Techniques for Recommender Systems”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Collaborative filtering Approaches:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Neighborhood methods:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     User-oriented: find similar users to the target user, and from them find the items that they like
    </span>
   </li>
   <li>
    <span>
     Product-oriented: find items similar to the ones liked by the target user
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Latent factor models: characterize items and users on some factors (user/item vector) inferred from rating patterns (explicit or implicit feedback). Predicted rating is the dot product between the two vectors
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Learning algorithms for extracting the factor vectors:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Stochastic gradient descent: easier and faster
    </span>
   </li>
   <li>
    <span>
     Alternating least squares: can be parallelized, better on sparse data
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Temporal dynamics: the model should be dynamic because some terms vary over time (item biases, user biases and user preferences)
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <h3 id="h.27kmyxe8tkxx">
   <span>
    Proactive experience
   </span>
  </h3>
  <p>
   <span>
    <a href="http://www.kdd.org/kdd2016/papers/files/adf0295-sunA.pdf">
     http://www.kdd.org/kdd2016/papers/files/adf0295-sunA.pdf
    </a>
   </span>
   <span>
    get things done before you ask. “utilizes collaborative capabilities among users, and learns, for each user, a personalized dynamic system that effectively models the sequential correlation among contextual signals and intent”
   </span>
  </p>
  <hr style="page-break-before:always;display:none;"/>
  <p>
   <span>
   </span>
  </p>
  <h1 id="h.btd4x8l3aptx">
   <span>
    Approach
   </span>
  </h1>
  <p>
   <span>
    This chapter describes the design and the approach that has been chosen in order to create a domain specific bot. The domain is the urban mobility. More in specific, the bot should provide a natural language interface towards the bike sharing service.
   </span>
  </p>
  <p>
   <span>
    Since there is no agreement with the bike sharing providers, the informations are only available in read mode, and only from public sources. For this reason the main things a user can do with the bot is to ask for informations on the station and availability of bikes. No account linking is possible and users can’t unlock bikes using the bot.
   </span>
  </p>
  <p>
   <span>
    The idea is to provide useful informations to the users in a spatial-context-aware setting. To provide this, the bot should analyze the area relative to the informations and provide some suggestions to the user.
   </span>
  </p>
  <p>
   <span>
    The following sections will be structured in this way: first giving an idea of the informations and use cases this bot aims to address, then talking about the platform independence of the core of the bot, then analyzing the Natural Language Understanding in both single turn interactions and in multi turn interactions, then going on more high level functionalities like the personalization.
   </span>
  </p>
  <h2 id="h.bi6zv7oh1003">
   <span>
    Scenarios
   </span>
  </h2>
  <p>
   <span>
    Cover here some common scenarios the bot has been designed for.
   </span>
  </p>
  <p>
   <span>
    TODOs
   </span>
  </p>
  <ul>
   <li>
    <span>
     this stuff needs to be explained in less messy way, without details on the modules
    </span>
   </li>
   <li>
    <span>
     A high level model of the system has to be explained
    </span>
   </li>
  </ul>
  <h3 id="h.5b37239td2lm">
   <span>
    Search for bike stations informations
   </span>
  </h3>
  <p>
   <span>
    The main scenario is the one where a user asks for informations about bikes and available parking slots. This can be expressed by a search of one station given some parameters or by a search that includes more stations.
   </span>
  </p>
  <p>
   <span>
    For the first case, the queries can be to find an available bike or a free parking slot given the current user position or another location. Instead for the second case, the user may want to ask for direction between two points: this involves finding a bike near the source and finding a place where to leave it near the destination.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    The utterance is processed by the NLU module. The query understanding engine activates the contextual knowledge engine to get data that is related to the question, and in parallel records the event into the episodic knowledge. The contextual knowledge is composed of informations belonging to bike sharing, meteo, placesAPI. The results from the different sources are combined by the appropriate module. At this point the recommender is interrogated, in order to provide some personalized suggestions, if they have some metrics (telling that the suggestion is appropriate and relevant). The structured information of the result that is going to be provided needs some elaboration to provide a natural-language feeling: this is the role of the sentence generation engine that, based on templates, puts the response in human-like format.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h3 id="h.ffagw94370j3">
   <span>
    Small talk, refining the user model
   </span>
  </h3>
  <p>
   <span>
    Expressing availability to talk (familiarization):
   </span>
  </p>
  <p>
   <span>
    The user may be sending some messages that don’t have some needs inside. Also the first interaction, in which the user sends some “/start” or “hi“ messages can activate this use case. The NLU processes them as usual, and the query understanding engine detects the situation. Instead of using contextual knowledge, it interacts with the personalization module to find some questions for the user: what he expects from the bot, or asking some features (interests/…).
   </span>
  </p>
  <h3 id="h.7n4l306qi0se">
   <span>
    Proactive messages
   </span>
  </h3>
  <p>
   <span>
    Sending messages without being asked:
   </span>
  </p>
  <p>
   <span>
    Warning: this breaks the principle that bots should only interact when explicitly asked. For this reason this feature should be kept under full control of the user (opt-in proposition vs one-time test + asking if this is liked), that in any moment can decide to turn it off. The reminder/suggestion, based on explicit request to do that or because a pattern has been detected, is generated and sent. The feedback must always be measured. The activation may be defined by the user (e.g. “tomorrow at 5 p.m. give me informations about bikes to go home”) or when the expected time, derived from the detected pattern, is coming (e.g: user always asks for a bike between 8:10 and 8:30 to go to university, now it is 8:00, could be the right time to send him the suggestion).
   </span>
  </p>
  <p>
   <span>
    Another useful activation is user asks for station X→ give results now on X and activate watcher on station X → estimate user-arrival-time to X → if station X is becoming useless (no/few bikes/slots depending on user need) send message to warn user
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Location can be inside the utterance as entity or can be the user position. In this case it must be up-to-date (define a timeout of validity and a way to ask user: “are you still there?”). If the dependency is not fulfilled, a way of skip
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <a id="t.2c97474fc09cd2758e0b91a19c5cfdca72f43631">
  </a>
  <a id="t.1">
  </a>
  <table>
   <tbody>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        intent
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        dependencies
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        steps
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        decision
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Search bike/free slot
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Location / updated position
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Find station informations, check meteo. Collect event. Activate watcher on station. Interrogate recommender
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Station informations + meteo warning* + recommendation*
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Plan trip
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Location A+B
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Same as above, but twice because both station must be watched / both events need to be collected
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        As above
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        If i will go home in 30 minutes, will I get wet?
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        OMG
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
       </span>
      </p>
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h2 id="h.bwv3zlnhzl7g">
   <span>
    High level model
   </span>
  </h2>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 189.33px;">
    <img alt="" src="https://lh4.googleusercontent.com/ml7B5Ffk3OYJ2m5_fhYgm8yDTs-z45p-K-yDYsrfprhIMPKf7RVphmhVYFZhJCnIhhRfLcyxnu_Yz2zbWJ4ehwHdJRANJ7ams6FfHXmwgioqrbOn5Jfr-LA3zEmsuLiDiyJsDfNW" style="width: 602.00px; height: 189.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    The main scenarios that have been described require the presence and cooperation of different components. First of all, one to interface with the different messaging platforms. This is responsible to make messages arrive to the “brain” of the bot and to deliver responses back. Something about this component will be told in the
   </span>
   <span>
    <a href="#h.t6vk6kn3atxd">
     next paragraph
    </a>
   </span>
   <span>
    and in the
   </span>
   <span>
    <a href="#h.8zj60ofw3oog">
     implementation chapter
    </a>
   </span>
   <span>
    . Then another component will do the NLU, providing intents and resolved entities. Passing through the dialogue manager, that contains the rules for managing the conversation, the flow will reach the personalization module that will collect episodic knowledge and together with the contextual knowledge will provide recommendations. The responses will be generated by combining the results and some templates will be filled.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h2 id="h.t6vk6kn3atxd">
   <span>
    Omnichannel Support
   </span>
  </h2>
  <p>
   <span>
    Chatbots live on messaging platforms. Nowadays there are so many different messaging platforms available and every one of them has different features. There are even some of them that don’t allow bot users. For example, the most widely used chat platform WhatsApp strictly forbids non-human accounts, punishing with permanent banning.
   </span>
  </p>
  <p>
   <span>
    Starting with one of the first platform to support bots with official API, Telegram Messenger is surely the easiest platform to create bots on, allowing them since June 2015. Bot accounts can easily be created in few seconds with the only principle that the username should end with “bot” in order to be recognizable. User can interact with bots using text, voice, buttons of different kind, images (any kind of file can be sent) and including them also in chat groups. On mobile devices users can also send their location. This platform is the most friendly for bots, but is not widely used by non-geeks.
   </span>
  </p>
  <p>
   <span>
    A very commonly used platform is Facebook Messenger, that opened to bots in April 2016. This platform is widely spread because is part of Facebook, and is available for every devices, also from web browsers. On Facebook Messengers the features are text/voice exchange, buttons of different kind, images and even attaching the user location. Facebook Bots have their Facebook Page and a few configuration steps need to be done to be able to set up a new bot. Other chat platforms that allow bots are Skype, Slack, Kik and many others.
   </span>
  </p>
  <p>
   <span>
    Other interesting communication channels are arising with expandible virtual assistants, that focus more on the use of the voice. Expandible in the sense that developers can develop some abilities for some domains and users are allowed to select them and add to the behaviour of the virtual assistant. We are talking about Alexa Skills and Cortana Skills. For these platforms the interaction with the user is quite different because instead of communicating directly with the user, the virtual assistant manages the conversation.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    There are so many different platforms that it would be quite restricted to focus only on a single one. Furthermore the APIs change a lot between them, and even between different versions of the same platform.
   </span>
  </p>
  <p>
   <span>
    Since all the details of the API are only an implementation detail, that will be fastly covered in the implementation chapter, here we will give only the motivation that lead the design of the system to be split into two parts: one will manage the channels and communication from and towards them, while the other will handle the important conversation stages (comprehension with NLU, dialogue state management, information retrieval, personalization). The message exchange between the two parts is done using a neutral representation, not dependent on any of the destination platforms.
   </span>
  </p>
  <p>
   <span>
    This is what is intended with the term Omnichannel Support: the core of the chatbot can be put in communication with any messaging platforms, given that the message-proxy component (the part of the two that depends on the specific platform) is correctly configured. This component can make use of one of the many available solutions to manage different channels: Microsoft Bot Framework, Recast.AI Bot Connector, or start from scratch the implementation of the different endpoints. As will be seen in the implementation chapter, having this component has many advantages.
   </span>
  </p>
  <h2 id="h.loyigzz2moom">
   <span>
    Main steps (remove me)
   </span>
  </h2>
  <p>
   <span>
    This section will disappear. Move the contents between this chapter (approach) and the chapter of implementation
   </span>
  </p>
  <ul>
   <li>
    <span>
     bike sharing (easy part)
    </span>
   </li>
   <li>
    <span>
     Context of the city: events, places. Possible source of data:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Foursquare
    </span>
    <span>
     <a href="https://developer.foursquare.com/docs/venues/explore">
      https://developer.foursquare.com/docs/venues/explore
     </a>
    </span>
    <span>
     can easily ask for suggestion on venues with specific category/property (120.000 requests per day, bigger coverage)
    </span>
   </li>
   <li>
    <span>
     Google places API
    </span>
    <span>
     <a href="https://developers.google.com/places/web-service/search">
      https://developers.google.com/places/web-service/search
     </a>
    </span>
    <span>
     provides similar features (1.000 requests per day)
    </span>
   </li>
   <li>
    <span>
     <a href="https://www.quora.com/What-are-the-pros-and-cons-of-each-Places-API">
      https://www.quora.com/What-are-the-pros-and-cons-of-each-Places-API
     </a>
    </span>
    <span>
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Weather forecasts: if bad weather is expected, tell user
    </span>
    <span>
     <a href="https://www.wunderground.com/weather/api/">
      https://www.wunderground.com/weather/api/
     </a>
    </span>
    <span>
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <ul>
   <li>
    <span>
     External profile data interface (consider technical difficulties) for more personalization and less explicit interview. If present, should be optional (not every user are on facebook, and also if they are, they may not want to share their profile)
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h3 id="h.m202m0o42x2b">
   <span>
    Recommendation phases
   </span>
  </h3>
  <ol start="1">
   <li>
    <span>
     SSH (social sciences and humanities):
    </span>
   </li>
  </ol>
  <ol start="1">
   <li>
    <span>
     Input: bootstrap/social data, episodic KB
    </span>
   </li>
   <li>
    <span>
     Output: user features (can be described and are kind of explicit dimensions)
    </span>
   </li>
  </ol>
  <ol start="2">
   <li>
    <span>
     Recommender system:
    </span>
   </li>
  </ol>
  <ol start="1">
   <li>
    <span>
     Input: user features + stimulus (what type of recommendation)
    </span>
   </li>
   <li>
    <span>
     Output: recommendation
    </span>
   </li>
   <li>
    <span>
     Internally using user profile vector that contains model-generated dimensions (can match or not the user features)
    </span>
   </li>
  </ol>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Finding recurrent actions:
   </span>
  </p>
  <p>
   <span>
    The EpisodicKB of actions describes events like: “on day DD/MM/YYYY time HH:MM user XXX searched for a bike at place PPP”. The place is not a simple position but is an external key to a Place row. Those places contain info that come from the contextual knowledge derived from some PlacesAPI (externally stored), or are stations or other specific positions with a role (home/work/school) (internally stored)
   </span>
  </p>
  <p>
   <span>
    This table is monitored to find patterns that are then stored in the RecurrentAction table
   </span>
  </p>
  <h2 id="h.tdcu8bfg7czy">
   <span>
    NLU
   </span>
  </h2>
  <p>
   <span>
    The subcomponents:
   </span>
  </p>
  <ol start="1">
   <li>
    <span>
     Intent + slot filling (RNN)
    </span>
   </li>
   <li>
    <span>
     Entity resolver
    </span>
   </li>
  </ol>
  <ol start="1">
   <li>
    <span>
     from words to structured objects, e.g. place name → lat,lng,full_name,...
    </span>
   </li>
   <li>
    <span>
     Disambiguation when multiple results
    </span>
   </li>
   <li>
    <span>
     Contextualization to current city
    </span>
   </li>
   <li>
    <span>
     Variables (e.g. home, work, school)
    </span>
   </li>
  </ol>
  <ol start="3">
   <li>
    <span>
     Put the Dialogue manager here?
    </span>
   </li>
  </ol>
  <h3 id="h.ujlxf9t9kbms">
   <span>
    Intents and slots
   </span>
  </h3>
  <p>
   <span>
    The intents have to cover the desired scenarios, to provide categories to user sentences
   </span>
  </p>
  <p>
   <span>
    Intent types:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Search a bike
    </span>
   </li>
   <li>
    <span>
     Search a free slot
    </span>
   </li>
   <li>
    <span>
     Plan a trip (1+2)
    </span>
   </li>
   <li>
    <span>
     Communicate location
    </span>
   </li>
   <li>
    <span>
     Get informations about a station
    </span>
   </li>
   <li>
    <span>
     Find closest station to a certain point
    </span>
   </li>
   <li>
    <span>
     Search other information (restaurant, cafe, …)
    </span>
   </li>
   <li>
    <span>
     Basic interactions (greet/thank)
    </span>
   </li>
   <li>
    <span>
     Feedback (on recommendation/place/system)
    </span>
   </li>
   <li>
    <span>
     Informations on the bot (who are you? What can you do?)
    </span>
   </li>
   <li>
    <span>
     Setting preferences:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     User favorite places (role → place)
    </span>
   </li>
   <li>
    <span>
     Enable/disable unsolicited messages
    </span>
   </li>
   <li>
    <span>
     Customization
    </span>
   </li>
  </ul>
  <p>
   <span>
    Entity types:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Location (with roles → slot types LOCATION_FROM, LOCATION_TO, LOCATION)
    </span>
   </li>
   <li>
    <span>
     Time
    </span>
   </li>
  </ul>
  <p>
   <span>
    On the entities some work needs to be done by the entity resolver, in order to recognize some values (such as home/work) and to disambiguate in a way to provide the best result in the current location context
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h3 id="h.bc96kppcy8e3">
   <span>
    Where the NN leaves the space to handwritten rules
   </span>
  </h3>
  <p>
   <span>
    Before describing the NLU module, it is important to remember the approach that has been chosen: it is NLU empowered, not generative. For this reason the dialogue management is still done using rules and the responses are provided by filling some templates.
   </span>
  </p>
  <p>
   <span>
    Without having an existing corpus to train the dialogues in an end-to-end fashion, this is the approach that best fits the situation. For example the fact that “to search for a bike you should provide a search criterion or the last known position should be known and recent (2hr or ask for confirmation)” is not manageable from a data-only focused approach. Inside the domain it is better to provide static rules that are handwritten in some configuration files. Although there are several studies on end-to-end goal-oriented dialog, for the implementation of this prototype a rule-based logic has been chosen. The neural network are only applied in the NLU module that is responsible to extract intent and entities from the current dialog, eventually using the multi-turn environment only to better identify slots.
   </span>
  </p>
  <h3 id="h.574j5code3vd">
   <span>
    Single-turn NLU
   </span>
  </h3>
  <p>
   <span>
    The solution that has been chosen to perform the joint task of intent classification and slot filling is the encoder-decoder model proposed by Liu et Al.
   </span>
   <sup>
    <a href="#ftnt49" id="ftnt_ref49">
     [49]
    </a>
   </sup>
   <span>
    because of its “state of the art” condition. The focus has been on how to provide better inputs to the system in terms of word embeddings and on output dependency modeling.
   </span>
  </p>
  <h4 id="h.wwhr5qye57xb">
   <span>
    Word embeddings
   </span>
  </h4>
  <p>
   <span>
    The paper presenting the used model is not describing the way the words are fed into the network. The implementation provided by the authors
   </span>
   <sup>
    <a href="#ftnt50" id="ftnt_ref50">
     [50]
    </a>
   </sup>
   <span>
    however shows that a word embedding layer is part of the model and the values are randomly initialized. Since the dataset on which the evaluation has been done is not very big, in the approach chosen fixed word embeddings, pretrained on bigger corpuses, have been used as inputs and this gave a little boost on performances [TODO ADD LINK TO MEASURES] as can be seen in the evaluation section.
   </span>
  </p>
  <ul>
   <li>
    <span>
     Instead of trainable from scratch, pretrained on bigger corpus
    </span>
   </li>
   <li>
    <span>
     Improvement of results with pre-trained on ATIS dataset
    </span>
   </li>
  </ul>
  <h4 id="h.gorlkbt8j0xz">
   <span>
    Output dependencies
   </span>
  </h4>
  <p>
   <span>
    As mentioned in the state of the art section, modeling the output dependencies is very important in sequence labeling tasks. In the description of the architecture in the paper it is not described how the output sequence labels are fed into the decoder. Different methods can be used: using one-hot encoding, that has the problem of scalability when the output dictionary is big enough, and word embeddings, to have a size independent from the number of existing slot labels. This time the word embeddings, being the labels part of an independent dictionary with respect to the input words, have been included in the trainable parameters of the computational graph.
   </span>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 391.00px; height: 298.00px;">
    <img alt="" src="https://lh3.googleusercontent.com/aHgGVSdG8K2pSRRbMiqERWiZzgx8iW66krO69uA3hqoXSEC7lAZWxL16Lkj2IvZ-qjZFBD-B3XmEkxMTNs_pvh7wdsr38K-cY5Zo9p8Zd-YuMnwaZ0q722BuFmOz2rzNF5SU0svz" style="width: 391.00px; height: 298.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <h3 id="h.4xv0kvc2etve">
   <span>
    Statefulness of architecture for multi-turn NLU
   </span>
  </h3>
  <p>
   <span>
    For solving the problem of multi-turn NLU, after analyzing all the ideas about keeping the context, the decision has been to change the single-turn architecture to make it stateful. Making it stateful means that the RNN cells involved will have the initial hidden state equal to the last hidden state. The hope is that initializing the RNN cells will make the work on the current sentence depend also on previous sentences, both for the intent classification and for the slot tagging. The current sentence may not contain indicators of the intent that is contained in previous user sentence. And specifically for the slot filling, we decided to consider also the agent sentence in order to provide the decoding stage. Since the model is a single one, the agent turn words are used in both the intent classification and in the slot filling networks.
   </span>
  </p>
  <p>
   <span>
    Additional work on the dataset:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Identifying the sessions
    </span>
   </li>
   <li>
    <span>
     Aligning (initializing with the previous state of the same session)
    </span>
   </li>
  </ul>
  <p>
   <span>
    Architecture:
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 388.00px;">
    <img alt="" src="https://lh3.googleusercontent.com/hPOJ7glI3EQi0OVAOQYS6u0WgDA_CgeWEG4uUHanTlqJQ7NKYuFMEmihEI61dts-1q3Kw_A3zrRAvWzm9pfgAcKb-Qv7ZcViaCUmznmY10oTTZEcKyyP29eZhIdGYecTFX6hBJDQ" style="width: 602.00px; height: 391.23px; margin-left: 0.00px; margin-top: -1.62px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    TODO explain why saving to memory and restoring stops the backpropagation. The training has to be done without save/restore. Only problem: backward RNN in train is connected, in test the arcs across turns are disconnected.
   </span>
  </p>
  <p>
   <span>
    TODO define better how to use masks to train slots only over user turns.
   </span>
  </p>
  <p>
   <span>
    TODO remove intent decoding RNN, explaining that the performances on single turn were better without it.
   </span>
  </p>
  <p>
   <span>
    The architecture chosen is basically the same of the encoder-decoder with aligned inputs for joint SLU. The part of the network for the intent, instead of taking simply the last state of the encoder, takes all the values of the encoder and summarizes them using a RNN. [Liu et al
   </span>
   <sup>
    <a href="#ftnt51" id="ftnt_ref51">
     [51]
    </a>
   </sup>
   <span>
    ] only used the last encoder state for the first proposed architecture, and in the second architecture used a mean pooling over the encoded states, additionally using an attention mechanism. An RNN was chosen because it can learn how to put together all the encoded states, and also because it is able to keep memory, that is what we want in a multi-turn environment.
   </span>
  </p>
  <p>
   <span>
    For the memory between turns, the hidden states of the RNN cells are saved for each user, and are restored back in the cells when a new sentence is received from the same user. To also consider the agent turn, the network receives as inputs not only the current user turn words, but also the agent turn words. Those won’t produce in output slot labels, because is not our interest, but will affect the RNN cells in the forward direction.
   </span>
  </p>
  <p>
   <span>
    Looking at the complete structure, it can be noticed that the memory of the states only initializes the forward cells. This is wanted, because in this way the new turn is affected by the old one, and the backward links are not connected through different turns because the decisions that have been taken in the past cannot be modified by next turns.
   </span>
  </p>
  <p>
   <span>
    A critical point where this model should prove the goodness of the chosen cell, is when two completely unrelated sentence follow each others (e.g. two different intents). In this case the LSTMs/GRUs should learn to forget their past state in order to provide the new values. In inference time we can never know whether a certain sentence is a follow-up or is a new intent without any logical linking with the previous one, so the only good option is to train the network to learn when this happens. For this motivation, instead of training on single independent sessions, the batch size has been reduced to 1 and all the sessions have been concatenated. In this way the model will be able to work better both on sentences that belong to the same session (for example answering back to a missing slot) and on other independent questions (for example an independent intent).
   </span>
  </p>
  <p>
   <span>
    Batch_size=1
   </span>
  </p>
  <ul>
   <li>
    <span>
     The order of samples is chosen in the program flow
    </span>
   </li>
   <li>
    <span>
     Can avoid padding:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     No mess with masks
    </span>
   </li>
   <li>
    <span>
     In stateful RNN padding is likely to deteriorate performances
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Performance decreases (only at training/evaluation time, not in usage because sentences to the bot are likely not to arrive all together)
    </span>
   </li>
  </ul>
  <h2 id="h.wkd8mmsq8hx8">
   <span>
    Managing the dialogue and providing informations
   </span>
  </h2>
  <h3 id="h.ycl7s8xb1vf8">
   <span>
    Dialogue State Management
   </span>
  </h3>
  <p>
   <span>
    TODO tell something about:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Intents that require some entities and rules
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <h3 id="h.h6o8pzps7nue">
   <span>
    Getting the informations
   </span>
  </h3>
  <p>
   <span>
    About bikes. Lack of open data. Every city and company has different ways to interact and get informations. Some libraries (pybikes) or some APIs
   </span>
   <span>
    <a href="https://api.citybik.es/v2/">
     https://api.citybik.es/v2/
    </a>
   </span>
   <span>
    .
   </span>
  </p>
  <p>
   <span>
    How to find a path between two points: fastest feasible on bike vs other approaches that consider beauty, smells of places (
   </span>
   <span>
    <a href="http://goodcitylife.org/happymaps/">
     http://goodcitylife.org/happymaps/
    </a>
   </span>
   <span>
    )
   </span>
  </p>
  <h2 id="h.p32l2grv4p6t">
   <span>
    Personalization
   </span>
  </h2>
  <p>
   <span>
    This section explains how the personalization techniques can be applied to the interactions with a chatbot, providing personalized content recommendations in a tailored communication fashion. This approach is composed of two main parts: content recommendation, that is usually analyzed in the RecSys community, and in this specific case the contents that are provided as recommendations are places around the user that he could be interested in. The second part instead is relative to the interaction itself: the goal is to change the behaviour of the bot relatively to the mood the user is in, in a dynamic way.
   </span>
  </p>
  <h3 id="h.kic1lrz03qac">
   <span>
    Content recommendation
   </span>
  </h3>
  <p>
   <span>
    For giving content recommendations, the objective of the recommender is to provide interesting places for the user around his trip. The area of search is therefore established by the results of the informations: different strategies could be used to determine this surface, but the fastest one, that helps also querying providers of places informations (e.g. Foursquare, Facebook Places, Google Places), is to use a circle determined by its center (the mean point between the source and the destination) and its radius (chosen to cover the source and destination, with some margin).
   </span>
  </p>
  <p>
   <span>
    Determined the valid surface, the places in it are candidates for the recommendation. To restrict the places list and provide one as recommendation, the strategy to be applied is to determine a category that the user could be interested in.
   </span>
  </p>
  <p>
   <span>
    To find this category, a bit of user modeling needs to be done.
   </span>
  </p>
  <h4 id="h.9j2yq7sjjkvs">
   <span>
    User modeling
   </span>
  </h4>
  <p>
   <span>
    What variables to collect. How to build the user model (features)
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    First of all, it is important to understand which variables need to be collected to build the user model. Some manifest variables like gender, age, profession, religion, political orientation are used for many profiling techniques, but for the task of giving recommendations on places the utility of these features can be not so much. Moreover, from a bot that is giving bike sharing informations it is strange to be asked for those values, and can only make the user stop using the bot because it’s asking strange questions. For these reasons, those information are not even asked to the user.
   </span>
  </p>
  <p>
   <span>
    As explicit data collection, some questions can be asked to the user in a more appropriate way, relatively to bike sharing. For example asking why the user makes use of bike sharing, to go to school, work or simply to run errands around the city. These informations can be collected at the beginning as bootstrap questions, or also after some dialogue with the user has been done. In this way the user can soon use the service without being blocked by bootstrap questions, but after some minutes of inactivity he can be sent some questions in order to improve the service.
   </span>
  </p>
  <p>
   <span>
    Other explicit informations can be collected when the recommendation is in act, as feedback to the suggestions: feedback can be positive or negative, collected through specific buttons or intercepting the user clicking on the informations of the place. The feedback can help refining the user interest and also understanding his attitude towards recommendations (as will be seen in the “tailored communication”).
   </span>
  </p>
  <p>
   <span>
    As implicit user modeling, observing the patterns (temporal and spatial) of the queries performed can help find users similarity and improve the collaborative filtering approach (see the following on recommendation paths). The patterns in the spatial dimension correspond to frequent places, while in the temporal dimension can be interpreted as habits.
   </span>
  </p>
  <p>
   <span>
    Other optional sources of data may come from social networks. On social networks people usually show their interests towards different kind of things: check-ins, places, music, films, and many others. This can be very helpful to user modeling, both providing user features both providing item (places in this case) features. The inputs in this case are the interests of the user, that can be provided by doing a login on the desired social network. This login is proposed to the user as optional and if denied must not preclude any functionalities of the bot. Simply the model for this user won’t be accurate and more generic recommendation will be provided.
   </span>
  </p>
  <p>
   <span>
    Given the social network footprints, a prediction on the Big5 personality traits can be done using some online available APIs: the big5 will be user features, together with “concentration” on different areas. Mention where those model come from.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Once these inputs are collected, different recommendation paths can be used to suggest the categories of places.
   </span>
  </p>
  <h4 id="h.seno7d4pqxiv">
   <span>
    Recommendation paths
   </span>
  </h4>
  <p>
   <span>
    Hybrid content-based and collaborative.
   </span>
  </p>
  <p>
   <span>
    TODO explain here the different flows:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Exploiting social login
    </span>
   </li>
   <li>
    <span>
     Exploiting feedback
    </span>
   </li>
   <li>
    <span>
     Training the mapping between user features and places categories
    </span>
   </li>
   <li>
    <span>
     Exploiting neighbors (collaborative)
    </span>
   </li>
  </ul>
  <p>
   <span>
    Selection of category given a distribution: analyze how a concept of “temperature” can model more randomness versus a fixed output (sampling)
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <ul>
   <li>
    <span>
     Suggestion engine:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Trip suggestion and reminders (must not be invasive):
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Previously used stations
    </span>
   </li>
   <li>
    <span>
     Previous schemes/timetables
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Provide informations on the city context (not strictly related to domain)
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     User relevant places on the way
    </span>
   </li>
   <li>
    <span>
     Time relevant (suggested place should be open)
    </span>
   </li>
  </ul>
  <h3 id="h.h3t6ly4gdrse">
   <span>
    Tailored communication
   </span>
  </h3>
  <p>
   <span>
    Operative modes, …
   </span>
  </p>
  <p>
   <span>
    From feedback (which ones) to understanding attitude towards recommendations
   </span>
  </p>
  <h2 id="h.78zc2qf1izjb">
   <span>
    Data modeling
   </span>
  </h2>
  <p>
   <span>
    Put there some data modelling
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Request: {utterance: {userID, time, content}, NLP: {intent, entities:[type,value]}, context, info: {stations:[], places:[], meteo:[]}, recommendations: [type,value,confidence], decision: {}}
   </span>
  </p>
  <p>
   <span>
    Is filled along the pipeline. Each stage is adding an element to it
   </span>
  </p>
  <p>
   <span>
    NLP
   </span>
  </p>
  <p>
   <span>
    From utterance:
   </span>
   <span>
    utteranceID
   </span>
   <span>
    , userID, time, content
   </span>
  </p>
  <p>
   <span>
    To: {intent,entities[type,value]}
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Entity resolver
   </span>
  </p>
  <p>
   <span>
    From user model:
   </span>
  </p>
  <ul>
   <li>
    <span>
     User city
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     User position
    </span>
   </li>
   <li>
    <span>
     User places: home, work/school
    </span>
   </li>
  </ul>
  <p>
   <span>
    The location entities become references to places in the DB (using places API):
   </span>
  </p>
  <ul>
   <li>
    <span>
     From name to place (the name in the utterance is used as keyword to do a search in the city proximity)
    </span>
   </li>
   <li>
    <span>
     From position to place (when user sends position, find a relevant place in the proximity)
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Core
   </span>
  </p>
  <p>
   <span>
    Internal state:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Previous questions / topic (field of dialogue): to understand messages without explicit intent and to resume interaction (stack of interactions: “i want a bike” PUSH “where are you?” -&gt; “i am at XXX” -&gt; “ok got it” POP “you can find 3 bikes at YYY”)
    </span>
   </li>
   <li>
    <span>
     Special topic of bootstrap: activated at the beginning
    </span>
   </li>
  </ul>
  <p>
   <span>
    Core steps:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Intent
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     If the intent is not specified (or generic intent “approve”/”disapprove”), look at the entity provided and if a previous state is saved. Based on these, derive the intent
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Preparation of parameters for functions (take from entities) and check requirements
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     If requirements are ok, proceed with steps
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     If previous state, pop it
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     If some requirements are missing, save current state (push) and ask for requirement
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Bike sharing:
   </span>
  </p>
  <p>
   <span>
    Station:
   </span>
   <span>
    stationID
   </span>
   <span>
    , name, description
   </span>
   <span>
    , (lat,long), free_bikes, free_slots
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Personalization:
   </span>
  </p>
  <p>
   <span>
    User:
   </span>
   <span>
    userID
   </span>
   <span>
    , Name, Surname, sex, age
   </span>
  </p>
  <p>
   <span>
    Episodic KB:
   </span>
   <span>
    userID, location, time
   </span>
   <span>
    , action (was at/took bike/left bike)
   </span>
  </p>
  <p>
   <span>
    Explicit rating: collecting user feedback after recommendation / bootstrap / unsolicited rating → needs to be investigated
   </span>
  </p>
  <p>
   <span>
    User features (built by the model): favorite/recurrent places, sportiveness, art interest
   </span>
  </p>
  <ul>
   <li>
    <span>
     RecurrentAction:
    </span>
    <span>
     ID
    </span>
    <span>
     , userID, type, frequency (days in week pattern), time of day, userPlaceID (external key to UserPlace)
    </span>
   </li>
   <li>
    <span>
     UserPlaces:
    </span>
    <span>
     userID, placeID
    </span>
    <span>
     , role (home/work/school/other)
    </span>
   </li>
  </ul>
  <p>
   <span>
    The stimulus link from the result provider to the recommender contains: trip information / direct question to the recommending system
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    User_feature:
   </span>
   <span>
    userID, featureID
   </span>
   <span>
    , value ← output of user modeler, input to recommender
   </span>
  </p>
  <p>
   <span>
    Place:
   </span>
   <span>
    placeID
   </span>
   <span>
    , name, description, (lat,long), category, subcategory
   </span>
  </p>
  <p>
   <span>
    Place_feature:
   </span>
   <span>
    placeID, featureID
   </span>
   <span>
    , value
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <hr style="page-break-before:always;display:none;"/>
  <p>
   <span>
   </span>
  </p>
  <h1 id="h.p8l11mdful87">
   <span>
    Implementation
   </span>
  </h1>
  <p>
   <span>
    This section focuses on the bot prototype that has been developed to put in practise some of the studies that have been done. The implementation can be divided in three main areas:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Omnichannel implementation, with platform-dependent code completely decoupled from the core of the bot
    </span>
   </li>
   <li>
    <span>
     Natural Language Understanding, with the implementation of recurrent neural network for joint intent detection and slot filling in a multiturn environment
    </span>
   </li>
   <li>
    <span>
     Personalization and interaction with the FB graph API
    </span>
   </li>
  </ul>
  <h2 id="h.8zj60ofw3oog">
   <span>
    Interaction with chat platforms
   </span>
  </h2>
  <p>
   <span>
    Decoupling botkit from core
   </span>
  </p>
  <p>
   <span>
    Which ones
   </span>
  </p>
  <p>
   <span>
    How
   </span>
  </p>
  <p>
   <span>
    Advantages of public webserver detached:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Free https and nameserver
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Required for some messaging platforms in webhook mode
    </span>
   </li>
   <li>
    <span>
     Required to do facebook login
    </span>
   </li>
   <li>
    <span>
     Better in terms of security, no plaintext interactions
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Low-power consuming on webserver, more computing power on the brain
    </span>
   </li>
   <li>
    <span>
     Implement link rewriting with fixed nameserver
    </span>
   </li>
  </ul>
  <h2 id="h.4sq87epw7a2r">
   <span>
    Data sources
   </span>
  </h2>
  <p>
   <span>
    Getting the bike sharing informations
   </span>
  </p>
  <p>
   <span>
    Getting places informations
   </span>
  </p>
  <p>
   <span>
    Other data sources
   </span>
  </p>
  <p>
   <span>
    Meteo
   </span>
  </p>
  <h2 id="h.b7qyoqg579vm">
   <span>
    Wit.ai prototype
   </span>
  </h2>
  <p>
   <span>
    A concept of the bot has been developed using wit.ai for doing intent detection and entity extraction. This can be used as a reference. The training data can be downloaded from the platform, useful for our model training.
   </span>
  </p>
  <p>
   <span>
    Objectives:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Experimenting
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Collect sentences and interactions for single turn
    </span>
   </li>
   <li>
    <span>
     Quick prototype to understand the user needs
    </span>
   </li>
  </ul>
  <h2 id="h.rdt81o3mb6tf">
   <span>
    Neural Networks
   </span>
  </h2>
  <h3 id="h.5gr30bgy3moi">
   <span>
    Framework
   </span>
  </h3>
  <p>
   <span>
    Keras vs Tensorflow
   </span>
  </p>
  <p>
   <span>
    In this section there is a comparison between the two most popular neural network libraries.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <a id="t.522a20e01ac7b3f467e4103ac499479fb019deb7">
  </a>
  <a id="t.2">
  </a>
  <table>
   <tbody>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Keras
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Tensorflow
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        High level
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Low level access, but also high level API
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        easy
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        More difficult
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Less cells/layers available
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Lot of already defined classes, contrib package
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Less customizable
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Highly customizable and flexible
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Rapid prototyping
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Advanced operations
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Documentation good for what’s implemented
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Documentation gap between trivial and advanced
       </span>
      </p>
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Keras is more like LEGO, build from blocks. On tensorflow can customize operations.
   </span>
  </p>
  <h4 id="h.30pe1175q3g4">
   <span>
    Keras Recurrent API
   </span>
  </h4>
  <p>
   <span>
    Keras Recurrent API is described extensively there (
   </span>
   <span>
    <a href="https://keras.io/layers/recurrent/">
     https://keras.io/layers/recurrent/
    </a>
   </span>
   <span>
    ). Here a quick overview is performed.
   </span>
  </p>
  <p>
   <span>
    To define a recurrent
   </span>
   <span>
    layer
   </span>
   <span>
    in Keras the class keras.layer.RNN takes as argument a RNN cell. Some parameters can be used to define how the layer will behave.
   </span>
  </p>
  <p>
   <span>
    A
   </span>
   <span>
    RNN cell
   </span>
   <span>
    is a class that has a call method that takes as input the actual input and the current state and produces the output and the next state. Different implemented
   </span>
   <span>
    cells
   </span>
   <span>
    exist.
   </span>
  </p>
  <ul>
   <li>
    <span>
     SimpleRNNCell: a fully connected RNN
    </span>
   </li>
   <li>
    <span>
     GRUCell: Gated Recurrent Unit - Cho et al. 2014
    </span>
   </li>
   <li>
    <span>
     LSTMCell: Long-Short Term Memory layer - Hochreiter 1997
    </span>
   </li>
  </ul>
  <p>
   <span>
    Layers can be easily stacked and the creation of deep recurrent network is very easy. Networks that are a simple stack of different layers (Recurrent, Dropout, Dense) won’t find any problem in implementation. But when the networks are not so linear in the flow or when some advanced layers are required, there might not be an already implemented solution. For example there is no native implementation for seq2seq models and layers, no Attention mechanism and is not possible to have a ready to use help for doing output dependency modeling (feeding back the outputs in a decoding layer). They can be implemented with the existing API, but they are not part of the library itself (some implementations can be found online as
   </span>
   <span>
    <a href="https://github.com/farizrahman4u/recurrentshop">
     https://github.com/farizrahman4u/recurrentshop
    </a>
   </span>
   <span>
    , but the number of people working on it is order of magnitude smaller than the tensorflow community
   </span>
   <span>
    ).
   </span>
  </p>
  <h4 id="h.3rp3zhi54s79">
   <span>
    Tensorflow Recurrent API
   </span>
  </h4>
  <p>
   <span>
    Also tensorflow has classes for defining RNN layers and RNN cells. But there is more:
   </span>
  </p>
  <ul>
   <li>
    <span>
     More cells: instead of a single implementation of LSTM and GRU, a wide variety of cells can be chosen
    </span>
   </li>
   <li>
    <span>
     More wrappers: wrappers can be used to encapsulate cells. DropoutWrapper, AttentionWrapper and a lot more can be added to the cells
    </span>
   </li>
   <li>
    <span>
     Seq2Seq native support: classes for defining encoders and decoders, with some helpers for modeling output dependencies can make a lot easier to build complex models that are not a linear chain of layers but contain links that move backwards (as in output dependencies, where the output label is fed back to the decoder RNN). For example to build a decoder that considers output dependencies, in tensorflow 3 components are needed: a RNN cell (e.g. LSTMCell), a CustomHelper (that is responsible to provide values to the cell and take the outputs. All this by simply declaring three custom functions) and a BasicDecoder that combines the cell with the helper and builds a layer. By dividing the roles of the cell and of the helper, a complex decoder can be built without a big effort.
    </span>
   </li>
  </ul>
  <p>
   <span>
    More details about the available classes can be found in the official documentation
   </span>
   <span>
    <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq">
     https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq
    </a>
   </span>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h4 id="h.8mdxf2pa39ns">
   <span>
    Performances
   </span>
  </h4>
  <p>
   <span>
    Since Keras can use as backed both Tensorflow and Theano, but Theano has many disadvantages (single GPU, non intuitive API, ceased development, ...), Keras can be seen as the official higher level API for tensorflow
   </span>
   <sup>
    <a href="#ftnt52" id="ftnt_ref52">
     [52]
    </a>
   </sup>
   <span>
    . For this reason, using tensorflow directly or inside Keras should make no big differences in terms of performance. There is also the possibility to use Keras API inside tensorflow, via the tensorflow.contrib.keras package.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h4 id="h.a08x42jk1qdf">
   <span>
    Decision
   </span>
  </h4>
  <p>
   <span>
    The objectives of the decision are:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Ability to build the proposed network without too much headache. Especially for the decoding stage, some advanced wiring is needed and would like not to start from scratch (error-prone) but have some support from the library
    </span>
   </li>
   <li>
    <span>
     Code readability/maintainability: possibly using quite high level operations, no reinventing the wheel (in a buggy version)
    </span>
   </li>
   <li>
    <span>
     Quite good performances
    </span>
   </li>
  </ul>
  <p>
   <span>
    Considering those objectives, the choice has been to use the native tensorflow APIs for building the solution, using the seq2seq package for faster implementation
   </span>
  </p>
  <h3 id="h.bqazqpo58hkq">
   <span>
    Implementation details
   </span>
  </h3>
  <p>
   <span>
    Computational graph definition using tensors. Difference between definition time of the model with placeholders, and usage with real data.
   </span>
  </p>
  <ul>
   <li>
    <span>
     Word tensors
    </span>
   </li>
   <li>
    <span>
     Seq2seq package
    </span>
   </li>
  </ul>
  <p>
   <span>
    Saving the models and serialization issues with py_func
   </span>
  </p>
  <h3 id="h.894qbnpqhcw5">
   <span>
    Word embeddings
   </span>
  </h3>
  <p>
   <span>
    A library for common NLP tasks, SpaCy, has been used for tokenization and for pretrained word embeddings on the english language. The corpus used comes from CommonCrawl, and the word embeddings have a size of 300. Different models come with the library, and they change how much space the word embeddings take. The biggest one, used in the implementation, contains 1.1m of words in the dictionary. Smaller models apply a reduction on the number of vectors and on the dictionary size; the mapping between those two dimensions is not always one-to-one: different words can be mapped on the same vector for further reduction in space.
   </span>
  </p>
  <h5 id="h.jwmi7ejc7ele">
   <span>
    Integrating SpaCy word vectors in the computational graph
   </span>
  </h5>
  <p>
   <span>
    To integrate the embeddings values provided by this library in the computational graph written in tensorflow, the `py_func` method has been used. In this way a function declared in definition time will be called in runtime: tensorflow manages this by making the py_func wrapper work externally on tensors and internally with numpy arrays.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h4 id="h.rhub0ifx6or2">
   <span>
    Italian word embeddings
   </span>
  </h4>
  <p>
   <span>
    For the italian language, SpaCy has an incomplete model that does not contain word vectors. For this reason, a side work has been done, taking the wikipedia corpus and computing GloVe vectors on it. Some preprocessing has been done to extract the text only of the wikipedia dumps (available at
   </span>
   <span>
    <a href="https://dumps.wikimedia.org/itwiki/">
     https://dumps.wikimedia.org/itwiki/
    </a>
   </span>
   <span>
    ) with a tool written at Pisa University (
   </span>
   <span>
    <a href="http://medialab.di.unipi.it/wiki/Wikipedia_Extractor">
     http://medialab.di.unipi.it/wiki/Wikipedia_Extractor
    </a>
   </span>
   <span>
    ), and eventually changing the whitespace separation of words in order to align it to the tokenization done by SpaCy. In this way the dictionary used in the training of the word embeddings is the same that is used by the computational graph (tokenization: how the input sentences are separated in single words)
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Look at commoncrawl preprocessed corpus
   </span>
   <span>
    <a href="http://data.statmt.org/ngrams/deduped/">
     http://data.statmt.org/ngrams/deduped/
    </a>
   </span>
   <span>
    76GB
   </span>
  </p>
  <p>
   <span>
    <a href="http://slides.com/smerity/common-crawl-for-nlp%23/">
     http://slides.com/smerity/common-crawl-for-nlp#/
    </a>
   </span>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h3 id="h.scnyotsag2iy">
   <span>
    Datasets
   </span>
  </h3>
  <p>
   <span>
    For single turn:
   </span>
  </p>
  <ul>
   <li>
    <span>
     ATIS: Air Travel Information System
    </span>
    <sup>
     <a href="#ftnt53" id="ftnt_ref53">
      [53]
     </a>
    </sup>
   </li>
   <li>
    <span>
     NLU_benchmark
    </span>
   </li>
   <li>
    <span>
     Botcycle it/en single turn collection
    </span>
   </li>
  </ul>
  <p>
   <span>
    Multi-turn:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Kvret dataset TODO
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Problems: intent per session; slots are quite messy, overlapping, both in driver and car turns
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Botcycle multiturn TODO
    </span>
   </li>
  </ul>
  <p>
   <span>
    See also validation
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h2 id="h.txecykd802tw">
   <span>
    Personalization
   </span>
  </h2>
  <p>
   <span>
    Magicsauce
   </span>
  </p>
  <p>
   <span>
    Bootstrap
   </span>
  </p>
  <p>
   <span>
    Facebook login (why, how)
   </span>
  </p>
  <p>
   <span>
    content
   </span>
  </p>
  <p>
   <span>
    Foursquare (why, how)
   </span>
  </p>
  <p>
   <span>
    From the user to the places
   </span>
  </p>
  <p>
   <span>
    Interaction
   </span>
  </p>
  <p>
   <span>
    Operative modes
   </span>
  </p>
  <h3 id="h.tm5ee3a05f8l">
   <span>
    Accessing user data on facebook
   </span>
  </h3>
  <p>
   <span>
    The data that is available directly from the messenger API is the following: first_name,last_name,profile_pic,locale,timezone,gender,is_payment_enabled
   </span>
  </p>
  <p>
   <span>
    In order to be able to do personalization, there exists a way to link the messenger id (page-scoped) to other ids (also facebook id indirectly):
   </span>
   <span>
    <a href="https://developers.facebook.com/docs/messenger-platform/account-linking">
     https://developers.facebook.com/docs/messenger-platform/account-linking
    </a>
   </span>
  </p>
  <p>
   <span>
    Once the facebook id is retrieved, it is possible to use the facebook graph API.
   </span>
  </p>
  <p>
   <span>
    To the external account the messenger platform sends a request to an url specified by the developer, adding account_linking_token and redirect_uri (page to redirect user after login). The external site, if login successful, redirects to redirect_uri with authentication_code custom (maybe put there the id in order to allow the bot to do the join)
   </span>
  </p>
  <p>
   <span>
    From the other side, the oauth flow:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Redirect user to facebook oauth with client_id (the app id) and redirect_uri (where facebook will send the user)
    </span>
   </li>
   <li>
    <span>
     After that user gives permissions, fb will redirect to redirect_uri with other parameters
    </span>
   </li>
  </ul>
  <p>
   <span>
    The procedure seems quite complicated and requires a web server component that interacts on one side with messenger platform and on the other side handles the facebook login.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    TODO update. Talk also about not saving personal details of users, only looking at likes and tagged places
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <hr style="page-break-before:always;display:none;"/>
  <p>
   <span>
   </span>
  </p>
  <h1 id="h.9ldkmut5kq6f">
   <span>
    Validation
   </span>
  </h1>
  <p>
   <span>
    “How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation”
   </span>
  </p>
  <p>
   <span>
    This paper analyzes metrics for
   </span>
   <span>
    unsupervised
   </span>
   <span>
    dialogue systems -&gt; not task focused. Correlation between automatic metrics and human ratings
   </span>
  </p>
  <h2 id="h.wr006axt8lw8">
   <span>
    NLU evaluation
   </span>
  </h2>
  <h3 id="h.mmb42f81ek3g">
   <span>
    Datasets
   </span>
  </h3>
  <ul>
   <li>
    <span>
     Single turn:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     ATIS
    </span>
   </li>
   <li>
    <span>
     Nlu-benchmark
    </span>
   </li>
   <li>
    <span>
     Botcycle (no baseline available)
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Multi-turn (problem of comparison with results by other):
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Kvret:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Problems: intent per session; slots are quite messy, overlapping, both in driver and car turns
    </span>
   </li>
  </ul>
  <h3 id="h.scboixtw2zz2">
   <span>
    Measures
   </span>
  </h3>
  <p>
   <span>
    Comparison with state of the art and other benchmarks
   </span>
  </p>
  <p>
   <span>
    For intents:
   </span>
  </p>
  <ul>
   <li>
    <span>
     F1 measure
    </span>
   </li>
  </ul>
  <p>
   <span>
    For entities:
   </span>
  </p>
  <ul>
   <li>
    <span>
     F1 based on single words
    </span>
   </li>
   <li>
    <span>
     F1 based on entities
    </span>
   </li>
   <li>
    <span>
     Error rate
    </span>
   </li>
  </ul>
  <h4 id="h.9m2ky1sgb9v1">
   <span>
    Word embeddings
   </span>
  </h4>
  <p>
   <span>
    Keeping the same network, show here boost on different datasets given by:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Pretrained word vectors and effect of spacy model size with respect to embedding layer
    </span>
   </li>
   <li>
    <span>
     Effect of better tokenization in italian language and comparison with
    </span>
    <span>
     <a href="http://hlt.isti.cnr.it/wordembeddings/">
      http://hlt.isti.cnr.it/wordembeddings/
     </a>
    </span>
    <span>
    </span>
   </li>
  </ul>
  <h4 id="h.vybi8bw7eizb">
   <span>
    Adding intent decoder RNN
   </span>
  </h4>
  <p>
   <span>
    Degradation of performances
   </span>
  </p>
  <h2 id="h.9u2lsbvmk08z">
   <span>
    System as a whole
   </span>
  </h2>
  <p>
   <span>
    Ground-truth based (comparing with a predetermined output. Using some already available datasets, but domain problems)
   </span>
  </p>
  <ul>
   <li>
    <span>
     Continuous sequences of correct actions from the beginning of the dialog
    </span>
   </li>
   <li>
    <span>
     Dialogue success rate
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Objective measures:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Duration of conversation
    </span>
   </li>
   <li>
    <span>
     Length of sentences
    </span>
   </li>
   <li>
    <span>
     Uptime of the bot
    </span>
   </li>
   <li>
    <span>
     Errors in time unit
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Detecting from chat:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Sentiment detection: not always applicable (“cold sentences”)
    </span>
   </li>
   <li>
    <span>
     User feedback: ok/thanks
    </span>
   </li>
   <li>
    <span>
     Thumbs up/down at the end of conversation → must be shown to user
    </span>
   </li>
   <li>
    <span>
     User repeats question → means the system didn’t catch it, but dangerous, maybe user wanted again updated data (can distinguish on time passed: short time could show that system didn’t catch)
    </span>
   </li>
   <li>
    <span>
     Not recognised intent→ could be
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Out of domain
    </span>
   </li>
   <li>
    <span>
     Into the domain but unforeseen
    </span>
   </li>
   <li>
    <span>
     Number of error messages
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     System turn duration (to generate a response)
    </span>
   </li>
   <li>
    <span>
     Task completion time (could compare against existing app)
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Survey-based:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Overall evaluation:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Usefulness
    </span>
   </li>
   <li>
    <span>
     Usability
    </span>
   </li>
   <li>
    <span>
     Relevance of results
    </span>
   </li>
   <li>
    <span>
     Missing features
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Per-interaction feedback (C. Chakrabarti, G.F. Luger / Expert Systems with Applications 42 (2015) 6878–6897):
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Grice’s maxims:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Quality: informations are good
    </span>
   </li>
   <li>
    <span>
     Quantity: appropriate quantity of informations
    </span>
   </li>
   <li>
    <span>
     Relation: relevant to context and to the topic of conversation
    </span>
   </li>
   <li>
    <span>
     Manner: direct and straightforward
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Solving problems
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     The bot asked the required information
    </span>
   </li>
   <li>
    <span>
     The bot kept conversation on-topic (coherence)
    </span>
   </li>
   <li>
    <span>
     The bot solved issue
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Evaluation of personalization: when giving a personalized response, check if user prefers the basic one or the customized one
    </span>
    <hr style="page-break-before:always;display:none;"/>
   </li>
  </ul>
  <h1 id="h.quszjp5rl9mp">
   <span>
    Conclusion
   </span>
  </h1>
  <p>
   <span>
   </span>
  </p>
  <div>
   <p>
    <span>
    </span>
   </p>
  </div>
  <hr/>
  <div>
   <p>
    <a href="#ftnt_ref1" id="ftnt1">
     [1]
    </a>
    <span>
     Turing, A. (1950).
    </span>
    <span>
     Computing Machinery and Intelligence
    </span>
    <span>
     . Mind
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref2" id="ftnt2">
     [2]
    </a>
    <span>
    </span>
    <span>
     <a href="https://www.theverge.com/2017/10/30/16552006/robot-rights-citizenship-saudi-arabia-sophia">
      https://www.theverge.com/2017/10/30/16552006/robot-rights-citizenship-saudi-arabia-sophia
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref3" id="ftnt3">
     [3]
    </a>
    <span>
     Walsh, T. (2016).
    </span>
    <span>
     Turing’s red flag
    </span>
    <span>
     . Communications of the ACM
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref4" id="ftnt4">
     [4]
    </a>
    <span>
     (1865). Locomotive Act. The Statutes of the United Kingdom of Great Britain and Ireland
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref5" id="ftnt5">
     [5]
    </a>
    <span>
     Zlotowski, J., Yogeeswaran, K., &amp; Bartneck, C. (2017).
    </span>
    <span>
     Can we control it? Autonomous robots threaten human identity, uniqueness, safety, and resources
    </span>
    <span>
     . International Journal of Human-Computer Studies
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref6" id="ftnt6">
     [6]
    </a>
    <span>
     Zlotowski, J., Yogeeswaran, K., &amp; Bartneck, C. (2017).
    </span>
    <span>
     Can we control it? Autonomous robots threaten human identity, uniqueness, safety, and resources
    </span>
    <span>
     . International Journal of Human-Computer Studies
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref7" id="ftnt7">
     [7]
    </a>
    <span>
     Pearl, J. (2017).
    </span>
    <span>
     Theoretical Impediments to Machine Learning
    </span>
    <span>
     . cse-lab.ethz.ch
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref8" id="ftnt8">
     [8]
    </a>
    <span>
     Gottfredson, L. S. (1997).
    </span>
    <span>
     Mainstream science on intelligence: An editorial with 52 signatories, history, and bibliography.
    </span>
    <span>
     Intelligence, 24, 13–23.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref9" id="ftnt9">
     [9]
    </a>
    <span>
     Lake, Brenden M and Ullman, Tomer D and Tenenbaum, Joshua B and Gershman, Samuel J. (2017). Building machines that learn and think like people. Behavioral and Brain Sciences
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref10" id="ftnt10">
     [10]
    </a>
    <span>
     Pearl, J. (2017). Theoretical Impediments to Machine Learning. cse-lab.ethz.ch
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref11" id="ftnt11">
     [11]
    </a>
    <span>
     Müller, V., Bostrom, N. (2016).
    </span>
    <span>
     Future progress in artificial intelligence: A survey of expert opinion
    </span>
    <span>
     . Fundamental Issues of Artificial Intelligence (Synthese Library; Berlin: Springer).
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref12" id="ftnt12">
     [12]
    </a>
    <span>
     Human Brain Project,
    </span>
    <span>
     <a href="https://www.humanbrainproject.eu/en/">
      https://www.humanbrainproject.eu/en/
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref13" id="ftnt13">
     [13]
    </a>
    <span>
    </span>
    <span>
     <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID%3D0134732">
      https://www.nsf.gov/awardsearch/showAward?AWD_ID=0134732
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref14" id="ftnt14">
     [14]
    </a>
    <span>
     Hawkins, J. and Ahmad, S. (2016).
    </span>
    <span>
     Why Neurons Have Thousands of Synapses, A Theory of Sequence Memory in Neocortex
    </span>
    <span>
     . Frontiers in neural circuits
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref15" id="ftnt15">
     [15]
    </a>
    <span>
     “A Theory of How Columns in the Neocortex Enable Learning the Structure of the World” J Hawkins, S Ahmad, Y Cui - Frontiers in Neural Circuits, 2017
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref16" id="ftnt16">
     [16]
    </a>
    <span>
    </span>
    <span>
     <a href="https://www.theverge.com/2017/11/10/16617092/sophia-the-robot-citizen-ai-hanson-robotics-ben-goertzel">
      https://www.theverge.com/2017/11/10/16617092/sophia-the-robot-citizen-ai-hanson-robotics-ben-goertzel
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref17" id="ftnt17">
     [17]
    </a>
    <span>
     Shieber, S. (1992). Lessons from a Restricted Turing Test.
    </span>
    <span>
     Technical Report TR-19-92, Harvard University
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref18" id="ftnt18">
     [18]
    </a>
    <span>
     Mauldin, M. L. (1994). Chatterbots, TinyMUDs and the Turing Test: Entering the Loebner Prize Competition.
    </span>
    <span>
     Proc. AAAI-94
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref19" id="ftnt19">
     [19]
    </a>
    <span>
     Ghose, S., Barua, J. (2013). Toward the Implementation of a Topic Specific Dialogue-Based Natural Language Chatbot as an Undergraduate Advisor.
    </span>
    <span>
     Proceeds of the International Conference on Informatics, Electronics, and Vision
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref20" id="ftnt20">
     [20]
    </a>
    <span>
     Ritter, A., Cherry, C., Dolan, W. (2011). Data-driven response generation in social media.
    </span>
    <span>
     EMNLP
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref21" id="ftnt21">
     [21]
    </a>
    <span>
     Li, J., Galley, M., Brockett, C., Gao, J., Bill, D. (2016). A persona-based neural conversation model.
    </span>
    <span>
     ACL, pages 994–1003
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref22" id="ftnt22">
     [22]
    </a>
    <span>
     Li, J., Galley, M., Brockett, C., Gao, J., and Dolan, B. (2015). A diversity-promoting objective function for neural conversation models.
    </span>
    <span>
     CoRR abs/1510.03055.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref23" id="ftnt23">
     [23]
    </a>
    <span>
     Kannan, A., Kurach, K., Ravi, S., Kaufmann, T., Tomkins, A., Miklos, B., Corrado, G., Lukacs, L., Ganea, M., et al. (2016). Smart reply: Automated response suggestion for email.
    </span>
    <span>
     ACM SIGKDD
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref24" id="ftnt24">
     [24]
    </a>
    <span>
     Guo, D., Tur, G., Yih, W., Zweig, G. (2014).
    </span>
    <span>
     Joint semantic utterance classification and slot filling with recursive neural networks
    </span>
    <span>
     . Proceedings of the IEEE SLT Workshop
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref25" id="ftnt25">
     [25]
    </a>
    <span>
     Liu, B. and Lane, I. (2016).
    </span>
    <span>
     Attention-based recurrent neural network models for joint intent detection and slot filling
    </span>
    <span>
     . Proceedings of The 17th Annual Meeting of the International Speech Communication Association
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref26" id="ftnt26">
     [26]
    </a>
    <span>
     Bengio, Y., Simard, P., Frasconi, P. (1994).
    </span>
    <span>
     Learning long-term dependencies with gradient descent is difficult
    </span>
    <span>
     . IEEE Transactions on Neural Networks and Learning Systems
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref27" id="ftnt27">
     [27]
    </a>
    <span>
     Hochreiter, S. (1998).
    </span>
    <span>
     The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions
    </span>
    <span>
     . International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref28" id="ftnt28">
     [28]
    </a>
    <span>
     Hochreiter, S., Schmidhuber, J. (1997).
    </span>
    <span>
     Long short-term memory
    </span>
    <span>
     . Neural computation - MIT Press
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref29" id="ftnt29">
     [29]
    </a>
    <span>
     Cho, K. et al. (2014).
    </span>
    <span>
     Learning phrase representations using RNN encoder-decoder for statistical machine translation
    </span>
    <span>
     . Proc. Conference on Empirical Methods in Natural Language Processing 1724–1734
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref30" id="ftnt30">
     [30]
    </a>
    <span>
     Jozefowicz, R., Zaremba, W., Sutskever, I. (2015).
    </span>
    <span>
     An empirical exploration of recurrent network architectures
    </span>
    <span>
     . Proceedings of the 32nd International Conference on Machine Learning, pages 2342–2350
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref31" id="ftnt31">
     [31]
    </a>
    <span>
     Chung, J., Gulcehre, C., Cho, K., Bengio, Y. (2014).
    </span>
    <span>
     Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling
    </span>
    <span>
     . NIPS Deep Learning Workshop
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref32" id="ftnt32">
     [32]
    </a>
    <span>
     Pinter,Y., Guthrie, R., Eisenstein, J. (2017). Mimicking word embeddings using subword rnns.
    </span>
    <span>
     Proceedings of EMNLP
    </span>
    <span>
     .
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref33" id="ftnt33">
     [33]
    </a>
    <span>
     Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas. (2016). Enriching Word Vectors with Subword Information.
    </span>
    <span>
     arXiv preprint arXiv:1607.04606
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref34" id="ftnt34">
     [34]
    </a>
    <span>
     Cho, K. et al. (2014).
    </span>
    <span>
     Learning phrase representations using RNN encoder-decoder for statistical machine translation
    </span>
    <span>
     . Proc. Conference on Empirical Methods in Natural Language Processing 1724–1734
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref35" id="ftnt35">
     [35]
    </a>
    <span>
     Liu, B. and Lane, I. (2016).
    </span>
    <span>
     Attention-based recurrent neural network models for joint intent detection and slot filling
    </span>
    <span>
     . Proceedings of The 17th Annual Meeting of the International Speech Communication Association
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref36" id="ftnt36">
     [36]
    </a>
    <span>
     Bahdanau, D., Cho, K. &amp; Bengio, Y. (2015).
    </span>
    <span>
     Neural machine translation by jointly learning to align and translate
    </span>
    <span>
     . Proc. International Conference on Learning Representations
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref37" id="ftnt37">
     [37]
    </a>
    <span>
     Liu, B. and Lane, I. (2016).
    </span>
    <span>
     Attention-based recurrent neural network models for joint intent detection and slot filling
    </span>
    <span>
     . Proceedings of The 17th Annual Meeting of the International Speech Communication Association
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref38" id="ftnt38">
     [38]
    </a>
    <span>
     Chen, Y., Hakkani-Tur, D., Tur, G., Gao, J., Li, D. (2016).
    </span>
    <span>
     End-to-end memory networks with knowledge carryover for multi-turn spoken language understanding
    </span>
    <span>
     . Proceedings of Interspeech
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref39" id="ftnt39">
     [39]
    </a>
    <span>
     Chen, P., Chi, T., Su, S., Chen, Y. (2017).
    </span>
    <span>
     Dynamic Time-Aware Attention to Speaker Roles and Contexts for Spoken Language Understanding
    </span>
    <span>
     . Proceedings of 2017 IEEE Workshop on Automatic Speech Recognition and Understanding
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref40" id="ftnt40">
     [40]
    </a>
    <span>
     Sukhbaatar, S., Szlam, A., Weston, J., Fergus, R. (2015).
    </span>
    <span>
     End-to-end memory networks
    </span>
    <span>
     . Proceedings of NIPS
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref41" id="ftnt41">
     [41]
    </a>
    <span>
     Eric, M. and Manning, C. (2017).
    </span>
    <span>
     Key-value retrieval networks for task-oriented dialogue
    </span>
    <span>
     . SIGDIAL 2017: Session on Natural Language Generation for Dialog Systems
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref42" id="ftnt42">
     [42]
    </a>
    <span>
     Kosinski, M., Matz, S. C., &amp; Gosling, S. D. (2015). Facebook as a research tool for the social sciences. American Psychologist, 70(6), 543–556. doi:10.1037/a0039210
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref43" id="ftnt43">
     [43]
    </a>
    <span>
     https://www.psychometrics.cam.ac.uk/productsservices/mypersonality
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref44" id="ftnt44">
     [44]
    </a>
    <span>
     Costa, P.T. Jr. &amp; McCrae, R.R. (1992). Revised NEO Personality Inventory (NEO-PI-R) and NEO Five-Factor Inventory (NEO-FFI) manual.
    </span>
    <span>
     Odessa, FL: Psychological Assessment Resources
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref45" id="ftnt45">
     [45]
    </a>
    <span>
     Goldberg, L. R. (1993). The structure of phenotypic personality traits.
    </span>
    <span>
     American Psychologist. 48: 26–34. doi:10.1037/0003-066x.48.1.26
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref46" id="ftnt46">
     [46]
    </a>
    <span>
     Youyou, W., Kosinski, M., Stillwell, D. (2015). Computer-based personality judgments are more accurate than those made by humans.
    </span>
    <span>
     PNAS pp. 1–5
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref47" id="ftnt47">
     [47]
    </a>
    <span>
     Kosinski, M., Stillwell, D., Graepel, Y. (2013). Private traits and attributes are predictable from digital records of human behavior.
    </span>
    <span>
     Proceedings of the National Academy of Sciences (PNAS).
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref48" id="ftnt48">
     [48]
    </a>
    <span>
     F. Mairesse, M.A. Walker, M.R. Mehl, and R.K. Moore. (2007). Using linguistic cues for the automatic recognition of personality in conversation and text.
    </span>
    <span>
     Journal of Artificial Intelligence Research, 30(1):457–500
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref49" id="ftnt49">
     [49]
    </a>
    <span>
     Liu, B. and Lane, I. (2016). Attention-based recurrent neural network models for joint intent detection and slot filling.
    </span>
    <span>
     Proceedings of The 17th Annual Meeting of the International Speech Communication Association
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref50" id="ftnt50">
     [50]
    </a>
    <span>
    </span>
    <span>
     <a href="https://github.com/HadoopIt/rnn-nlu">
      https://github.com/HadoopIt/rnn-nlu
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref51" id="ftnt51">
     [51]
    </a>
    <span>
     Liu, B. and Lane, I. (2016).
    </span>
    <span>
     Attention-based recurrent neural network models for joint intent detection and slot filling
    </span>
    <span>
     . Proceedings of The 17th Annual Meeting of the International Speech Communication Association
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref52" id="ftnt52">
     [52]
    </a>
    <span>
    </span>
    <span>
     <a href="http://www.fast.ai/2017/01/03/keras/">
      http://www.fast.ai/2017/01/03/keras/
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref53" id="ftnt53">
     [53]
    </a>
    <span>
     Hemphill, C., Godfrey, J., Doddington, G. (1990).
    </span>
    <span>
     The ATIS spoken language systems pilot corpus. DARPA Speech and Natural Language Workshop
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#cmnt_ref1" id="cmnt1">
     [a]
    </a>
    <span>
     forse ne parli dopo, c'e' un progetto EU molto grande https://www.humanbrainproject.eu/en/
    </span>
   </p>
   <p>
    <span>
    </span>
   </p>
   <p>
    <span>
     e ce n'e' anche uno americano analogo
    </span>
   </p>
   <p>
    <span>
     https://www.nsf.gov/awardsearch/showAward?AWD_ID=0134732
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#cmnt_ref2" id="cmnt2">
     [b]
    </a>
    <span>
     invece di due dimensioni separate, incrociare in matrice
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#cmnt_ref3" id="cmnt3">
     [c]
    </a>
    <span>
     See https://web.stanford.edu/class/cs124/lec/chatbot17.pdf for detailed analysis
    </span>
   </p>
   <p>
    <span>
     For frame-based and another introduction see https://web.stanford.edu/class/cs224s/lectures/224s.17.lec10.pdf
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#cmnt_ref4" id="cmnt4">
     [d]
    </a>
    <span>
     prima di tutto mettere la mappatura, poi introdurre le metodologie, sfide
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#cmnt_ref5" id="cmnt5">
     [e]
    </a>
    <span>
     non convince il termine
    </span>
   </p>
  </div>
 </body>
</html>