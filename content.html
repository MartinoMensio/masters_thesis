<html>
 <head>
  <meta content="text/html; charset=utf-8" http-equiv="content-type"/>
 </head>
 <body>
  <div>
   <p>
    <span>
    </span>
   </p>
  </div>
  <p id="h.sdrab4c7fiee">
   <span>
    Deep Semantic Learning for Conversational Agents
   </span>
  </p>
  <p>
   <span>
    Abstract
   </span>
  </p>
  <h2 id="h.jxjvu43t16o9">
   <span>
    Context
   </span>
   <sup>
    <a href="#cmnt1" id="cmnt_ref1">
     [a]
    </a>
   </sup>
  </h2>
  <p>
   <span>
    Conversational agents are now, more than ever, the answer to establish a seamless interaction between the end users and any service out there. The spreading of these agents has highlighted an important need: going beyond the simple (often pre-computed) answer and provide personalized answers according to users' profiles.
   </span>
  </p>
  <p>
   <span>
    This work contains therefore two major themes - Natural Language Understanding (NLU) and personalization - that follow each other’s in a traversal argumentation from the literature to the selected approaches.
   </span>
  </p>
  <p>
   <span>
    The
   </span>
   <span>
    Natural Language Interface
   </span>
   <span>
    enables a communication with the machine in the language that is really used by humans in their everyday interactions with other people. Traditional mobile applications force the human to imitate the computer to exchange information, performing precise questions in the form of commands. The goal of Conversational Interfaces instead, is to reverse this imitation process, in order to bring the interaction closer to the users. Being close to the user means also to use his native language, and this is why bots should be available in local languages too.
   </span>
  </p>
  <p>
   <span>
    Once this interaction channel is ready, with the mutual understanding of the involved parties, a process of
   </span>
   <span>
    personalization
   </span>
   <span>
    can be applied in order to provide user-centric contents and interactions.
   </span>
  </p>
  <h2 id="h.9jlmvfyx0hgy">
   <span>
    Goals
   </span>
  </h2>
  <p>
   <span>
    The goals of this work are of two different natures:
   </span>
   <span>
    i)
   </span>
   <span>
    analyze the State of the Art in order to identify the approaches that better suit building a Conversational Agent
   </span>
   <span>
    ii)
   </span>
   <span>
    build a bot prototype that uses the selected approaches.
   </span>
  </p>
  <p>
   <span>
    The first goal includes the two main themes: Natural Language Understanding and personalization.
   </span>
   <span>
    For the first one, the focus stands on Recurrent Neural Network (RNN) approaches that can perform a sentence classification (intent detection task) and extraction of parameters (slot filling task). The literature also shows how the input words can be used, by exploiting features (word embeddings
   </span>
   <sup>
    <a href="#ftnt1" id="ftnt_ref1">
     [1]
    </a>
   </sup>
   <span>
    ) computed on large datasets of unlabeled text, to capture syntactic and semantic similarities on big dictionaries. These similarities allow to be more robust towards words that have not been considered in training sets. For the personalization theme instead, the Content-based and Collaborative filtering are explained, together with available methods to extract user features. These user features can be of anagraphic type or can be more undeclared traits such as Big Five Personality Traits
   </span>
   <sup>
    <a href="#ftnt2" id="ftnt_ref2">
     [2]
    </a>
   </sup>
   <span>
    that can be computed by analyzing external data coming from social networks and interests.
   </span>
  </p>
  <p>
   <span>
    The second goal, the running prototype, is focused on the only theme of Natural Language Understanding.
   </span>
   <span>
    As use case t
   </span>
   <span>
    he domain of urban mobility is chosen, having as specific objective the design and implementation of a Conversational Agent to
   </span>
   <span>
    retrieve real-time information about the bike sharing system. The bot should offer capabilities to find bikes given the user location, or plan trips between different points of a given city, using the data from the bike sharing providers. To provide a natural interaction with the user, the bot should be able to process requests that are not in the strict form of commands, but in a more conversational way. To reduce the distance between the system and the user, the bot should support both English and Italian languages.
   </span>
  </p>
  <h2 id="h.bl1jg0yqtzn6">
   <span>
    Personal contributions and results obtained
   </span>
  </h2>
  <p>
   <span>
    Moving on the fulfillment of these goals, the approach is structured as follows. Firstly, an analysis is done of the target scenarios where the bot for bike sharing is meant to be used. After giving a high-level view of the components required to interact with chat platforms (such as Facebook Messenger, Telegram, Skype) and the required services (bike sharing providers, directions provider), the description focuses on Natural Language Understanding. To target two types of interaction (single-turn where each sentence is processed independently and multi-turn where an interaction context exists) the selected approach is described: for the single-turn interactions the reference State of The Art joint approach
   </span>
   <sup>
    <a href="#ftnt3" id="ftnt_ref3">
     [3]
    </a>
   </sup>
   <span>
    is chosen, while for the multi-turn ones a slight modification is proposed as novelty. For the personalization theme instead, two main needs are described: providing content recommendation, such as interesting places along the path of the cyclist, and tailoring the communication mean to the needs of the user. The personalization theme actually ends here with a sketch that can be used as guideline for implementation (is neither implemented nor part of the running prototype).
   </span>
  </p>
  <p>
   <span>
    Moving to the implementation, a description is done of the details that made it possible to bring the bot prototype to life. The considered parts are the interaction with the chat platforms, the frameworks used for the NLU, and a dedicated section to word embeddings that shows how they have been computed on the Wikipedia Italian corpus
   </span>
   <sup>
    <a href="#ftnt4" id="ftnt_ref4">
     [4]
    </a>
   </sup>
   <span>
    to support this secondary language.
   </span>
  </p>
  <p>
   <span>
    Finally, for the validation (both of the selected NLU approach and of the prototype) a description is done of the evaluation framework that has been used. For the evaluation of NLU, the datasets (standardized ones and collected ones for the specific bike sharing bot in both English and Italian) are explained and the measures are defined. At the end the results are presented, underlying the importance of features such as word embeddings or the structure of the network itself. For the overall evaluation of the prototype instead, it emerges how difficult is to obtain human-like understanding performances.
   </span>
  </p>
  <hr style="page-break-before:always;display:none;"/>
  <h1 id="h.a7xlb6rokf5x">
   <span>
    Introduction
   </span>
  </h1>
  <p>
   <span>
    This chapter provides a general introduction to the work done in this thesis project. Starting with a brief introduction to the setting where this work has been conducted, an introduction to the topic of Conversational Agents (also known as Chatbots or Bots) is given. An overview of the field of Artificial Intelligence is done, by considering both its current achievements and some general purpose guidelines that should lead the research and development of such systems.
   </span>
  </p>
  <p>
   <span>
    The chapter ends with an outline of the next chapters.
   </span>
  </p>
  <h2 id="h.fyuwi9wr3wod">
   <span>
    Istituto Superiore Mario Boella
   </span>
  </h2>
  <p>
   <span>
    This work has been conducted in a joint collaboration between Politecnico di Torino
   </span>
   <sup>
    <a href="#ftnt5" id="ftnt_ref5">
     [5]
    </a>
   </sup>
   <span>
    and Istituto Superiore Mario Boella
   </span>
   <sup>
    <a href="#ftnt6" id="ftnt_ref6">
     [6]
    </a>
   </sup>
   <span>
    . This institute, founded in 200 as a research and innovation center by Compagnia di San Paolo and Politecnico di Torino, is organized in different research areas that focus on some core sectors of ICT, like cloud computing, electromagnetics, optical communications, ICT innovation, mobile solutions, wireless systems and sensors, and navigation technologies.
   </span>
  </p>
  <p>
   <span>
    The research area where the current work has been done is Innovation Development, that provides practical support to public and private decision makers in the use of information and communication technologies as a strategic tool for the promotion of sustainable development. The topics covered by the Data Science group varies from language processing, recommender systems, data semantics, ...
   </span>
  </p>
  <p>
   <span>
    This work, being on the topics of Natural Language and Personalization, has been supported by the Data Science Group through its studies and development.
   </span>
  </p>
  <h2 id="h.7zcrbk1mcnss">
   <span>
    Autonomous Systems in the society
   </span>
  </h2>
  <p>
   <span>
    This work about Conversational Agents belongs to the wider topic of Autonomous Systems. If we think about independent systems that live together with humans, a lot of science fiction stories may come to our minds. From this literature we can extract some major themes that may reflect how the society can see the development of autonomous systems.
   </span>
  </p>
  <p>
   <span>
    One of them is surely the problem of dominance and control. There is a fight (real or implicit) between the humans and the machines. The dominance can be hold by both sides, and what emerges is the difference between the goals of the robots and the goals of the humans.
   </span>
  </p>
  <p>
   <span>
    Another very important topic that emerges is the self-awareness and what distinguishes humans from machines. This is where
   </span>
   <span>
    sentient
   </span>
   <span>
    AI seek understanding of the world together with a purpose of existence, and always arises ethical questions, such as effects on our behavior and interaction, how to keep control over the singularity, what are the rights of both sides.
   </span>
  </p>
  <p>
   <span>
    All those stories, being part of science fiction, are mostly far from reality. But since technology is advancing faster and faster, an analysis should be done on the consequences it can have on the society, and how the existence of some principles could turn those advances into empowering tools for the humans and not something to be afraid of.
   </span>
  </p>
  <p>
   <span>
    These concepts have to be analyzed and some principles must be known and followed when designing autonomous systems. Keeping in mind that the systems analyzed in this work are far from generic artificial intelligence, because they are designed to solve very narrow problems, those principles have to be analyzed at the beginning.
   </span>
  </p>
  <p>
   <span>
    For this reason, after giving an introduction to Artificial Intelligence that empowers those autonomous agents, looking at its roots and evolution (subsection [REF:aiEvolution]), the discussion will go on some guidelines (subsection [REF:aiGuidelines]) which have been derived from common sense and scientific literature
   </span>
   <sup>
    <a href="#ftnt7" id="ftnt_ref7">
     [7]
    </a>
   </sup>
   <sup>
    <a href="#ftnt8" id="ftnt_ref8">
     [8]
    </a>
   </sup>
   <sup>
    <a href="#ftnt9" id="ftnt_ref9">
     [9]
    </a>
   </sup>
   <sup>
    <a href="#ftnt10" id="ftnt_ref10">
     [10]
    </a>
   </sup>
   <span>
    .
   </span>
  </p>
  <h3 id="h.n4yc6583sez">
   <span>
    Artificial Intelligence evolution
   </span>
  </h3>
  <p>
   <span>
    [LABEL:aiEvolution]
   </span>
  </p>
  <p>
   <span>
    Here it is provided a bit of background on Artificial Intelligence, starting from the historical roots that lead to development of systems that imitate the human. Successively, an overview is given about what can be done by AI today in different fields. Then a brief exploration of the Machine Learning techniques open the discourse of general Artificial Intelligence.
   </span>
  </p>
  <h4 id="h.ana2zm6bt6x">
   <span>
    Artificial Intelligence as Human imitation
   </span>
  </h4>
  <p>
   <span>
    The development of artificial intelligence has its roots back in the 50s. The idea is to build an artificial brain, inspired by the human
   </span>
   <span>
    brain
   </span>
   <sup>
    <a href="#cmnt2" id="cmnt_ref2">
     [b]
    </a>
   </sup>
   <span>
    . Empowered by the studies carried in those years about neurons and synapses, imitating those elements and their connection became an active topic of research. The first machines trying to achieve this task didn’t use computers, but were completely controlled by analog circuitry. The first neural network machine was built by Marvin Minsky in 1951
   </span>
   <sup>
    <a href="#ftnt11" id="ftnt_ref11">
     [11]
    </a>
   </sup>
   <span>
    , based on the idea of the artificial neuron
   </span>
   <sup>
    <a href="#ftnt12" id="ftnt_ref12">
     [12]
    </a>
   </sup>
   <sup>
    <a href="#ftnt13" id="ftnt_ref13">
     [13]
    </a>
   </sup>
   <span>
    .
   </span>
  </p>
  <p>
   <span>
    With the goal of exploring how the human brain works, the neuroscience community has done a lot of progresses with the advance of years, and the studies are still open trying to find the biological basis of information processing (take a look at the “human brain project” in [REF:generalAI]).
   </span>
  </p>
  <p>
   <span>
    While the first experiment were carried out in 50s, on the philosophical side Alan Turing published a paper
   </span>
   <sup>
    <a href="#ftnt14" id="ftnt_ref14">
     [14]
    </a>
   </sup>
   <span>
    in which he sustains that a “thinking machine” machine could be built. The criterion defined to distinguish the thinking process is based on human judgement: if the machine is not distinguishable from another human being in a conversation over a teleprinter, we can say that the machine is able to think. This setting is the so called “Turing Test”. In this test two players (a human and a machine) are tested against a human interrogator, that tries to understand their nature. A machine that passes this test is said to be intelligent.
   </span>
  </p>
  <p>
   <span>
    As we can see, all is based on imitation of humans: at detailed level, the artificial neural networks try to mimic the behaviour of the human brain. If the intelligence of a human is located in its brain, a good reproduction of it can potentially have the same intelligence of the original one. And the root of the abilities of the brain stands in its structure. At more higher level of abstraction, a system can be seen as intelligent if it can emulate a human characteristic (such as holding a conversation over a teleprinter) well enough to convince the interrogator.
   </span>
  </p>
  <p>
   <span>
    This can be see
   </span>
   <span>
    n as a first definition of artificial intelligence:
   </span>
   <span>
    the simulation of human intelligence by machines
   </span>
   <span>
    . This imitation also is reflected in the shapes of robots, because a more human-like appearance is a faster way to immediately feel more human. Having a face that can emulate some expressions is an active field of work in the robotics. As we know, the interaction is not bound to what is said in communication, but has its strength in multimodality, combining visual and audio channels.
   </span>
  </p>
  <p>
   <span>
    But is really the human intelligence the best an autonomous system can achieve? In the world of today, with the enormous quantity of data available, and with the increasing processing power of computers, an artificial system can be seen as smart because combines and extracts the information available very fast and in a way that is useful to the humans using it.
   </span>
  </p>
  <p>
   <span>
    Having a good distinction between what is human intelligence and what artificial intelligence could be, can help building a roadmap about how the society wants and will like to be empowered by technology.
   </span>
  </p>
  <p>
   <span>
    The concept used to express this collaboration between human and machine is
   </span>
   <span>
    heteromation
   </span>
   <sup>
    <a href="#ftnt15" id="ftnt_ref15">
     [15]
    </a>
   </sup>
   <span>
    : the labour is divided between humans and machines by using the automation for all the repetitive tasks while the humans are used for critical and decisional tasks.
   </span>
  </p>
  <h4 id="h.jbtt7f1i39bb">
   <span>
    What AI can do today
   </span>
  </h4>
  <p>
   <span>
    As of today, Artificial Intelligence has reached some good results in very narrow tasks. The key point that made this possible is how the intelligent behaviour: from historical systems that aim to act by following huge sets of rules (deductive reasoning), to Machine Learning techniques that allow to learn things simply from examples (inductive reasoning). Over the Machine Learning techniques, a major role is being played by Neural Networks that try to imitate the physical and chemical structure of human brain. The fields where Machine Learning has achieved recent outstanding results are many. Here we try to provide some examples of them.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    With respect to image processing, Machine Learning can be able to detect, segment and recognize objects and regions. For these tasks Deep Convolutional Networks are used
   </span>
   <sup>
    <a href="#ftnt16" id="ftnt_ref16">
     [16]
    </a>
   </sup>
   <span>
    . Their objective is to abstract from pixel-level features to more complex features such as lines, borders and shapes. In this way they can successfully categorize images based on the contents: identify objects, recognize written text (OCR), detect faces, recognize people, detect facial expressions
   </span>
   <sup>
    <a href="#ftnt17" id="ftnt_ref17">
     [17]
    </a>
   </sup>
   <span>
    , find new planets
   </span>
   <sup>
    <a href="#ftnt18" id="ftnt_ref18">
     [18]
    </a>
   </sup>
   <span>
    .
   </span>
  </p>
  <p>
   <span>
    Instead on the field of language a lot of different tasks are addressed: automated translation
   </span>
   <sup>
    <a href="#ftnt19" id="ftnt_ref19">
     [19]
    </a>
   </sup>
   <span>
    , sentiment analysis, entity recognition, Natural Language Understanding (where this work focuses, see the State of the Art chapter [REF:soa]). For this set of tasks, more than Convolutional Networks, the best results are provided by Recurrent Neural Networks that are able to process order-sensitive sequences. These approaches usually make use of distributed representation of words
   </span>
   <sup>
    <a href="#ftnt20" id="ftnt_ref20">
     [20]
    </a>
   </sup>
   <span>
    , known as Word Embeddings, that following the Distributional Hypothesis: “linguistic items with similar distributions have similar meanings”
   </span>
   <sup>
    <a href="#ftnt21" id="ftnt_ref21">
     [21]
    </a>
   </sup>
   <span>
    (see [REF:soaWordEmbeddings]). Those approaches, labeled as Natural Language Processing, can be used for different goals: analysis of text from online sources or social media, provide services through Conversational Interfaces, entertain the user with chit-chat dialogues.
   </span>
  </p>
  <p>
   <span>
    Machine Learning also applies well to the field of games. In 2016 AlphaGo
   </span>
   <sup>
    <a href="#ftnt22" id="ftnt_ref22">
     [22]
    </a>
   </sup>
   <span>
    managed to beat the world champion of the Go game. The interesting part of this story is that the AI trained for the match by not only observing a lot of previous matches but also by playing against itself. Other studies focus on other games such as Texas hold’em
   </span>
   <sup>
    <a href="#ftnt23" id="ftnt_ref23">
     [23]
    </a>
   </sup>
   <span>
    , or on video games
   </span>
   <sup>
    <a href="#ftnt24" id="ftnt_ref24">
     [24]
    </a>
   </sup>
   <span>
    . Using Reinforcement Learning techniques, those systems are able to learn the consequences of performing actions and adaptively learn with time to optimize their goals.
   </span>
  </p>
  <p>
   <span>
    Those different fields of application have also been combined together: for example generating textual descriptions of images
   </span>
   <sup>
    <a href="#ftnt25" id="ftnt_ref25">
     [25]
    </a>
   </sup>
   <span>
    , or providing multimodal interaction (voice, text, visual) with smart assistants (Alexa, Siri, Cortana).
   </span>
  </p>
  <h4 id="h.vq46acbre51d">
   <span>
    The evolution of machine learning
   </span>
  </h4>
  <p>
   <span>
    All those results have been achieved thanks to the evolution of the Machine Learning techniques.
   </span>
  </p>
  <p>
   <span>
    A first division of them can be done by considering if the desired output is provided or not in the training set: the distinction is between supervised techniques (that have input-output pairs) and unsupervised ones (that have only inputs, and apply in tasks like clustering).
   </span>
  </p>
  <p>
   <span>
    The principle at the basis of supervised ML techniques is to show examples to a system that learns how to obtain the desired output. Different approaches exist: decision trees, association rules, neural networks. But the trend nowadays is towards Artificial Neural Networks, because of their power in being able to model non-linear relationships and for this reason are very general and applicable in different ways. They have tunable parameters that are learnt by using the Backpropagation algorithm
   </span>
   <sup>
    <a href="#ftnt26" id="ftnt_ref26">
     [26]
    </a>
   </sup>
   <span>
    . From the errors on the predictions with respect to the truth values, the sources of errors are found by evaluating the backpropagation of the gradients, and the tunable parameters are modified in order to reduce this error by using some Gradient Descent techniques
   </span>
   <sup>
    <a href="#ftnt27" id="ftnt_ref27">
     [27]
    </a>
   </sup>
   <span>
    .
   </span>
  </p>
  <p>
   <span>
    Given this mechanism to automatically learn from examples, there has been an evolution considering the structure itself of the network (adding more and more layers, of different kinds), on the training algorithms and on the kind of input that are fed.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Neural Networks come in the field of Artificial Intelligence as systems that can model some non-linear functions and substitute handcrafted rules in classification tasks. The first generation of this approach was based on a strong definition of input features, that were defined manually and inputs are annotated with a lot of them. This manual specification of input features requires a lot of effort in their design and in data annotation, and for this reason is not easily applicable to new problems. For some problems the features themselves can become quite complex and difficult to generate.
   </span>
  </p>
  <p>
   <span>
    Given those reasons, and also considering the advances of hardware that allow more computation power at reduced costs, the trend of Machine Learning has gone towards Deep Learning that uses several layers of computation in the networks. This increase in number of layers and in complexity of the computational graph allows to reduce the work done on input features, allowing rawer data to be fed into the networks. In Deep Learning techniques, features to more high level layers are extracted by the lower levels of the network. This reduces the work of feature engineering but on the other hand requires more training samples in order to understand how to extract the relevant features.
   </span>
  </p>
  <p>
   <span>
    This shift from simple networks with complex and elaborated features towards complex networks with simpler inputs can be observed also in the fields of Natural Language Understanding, that will be covered in the NLU section [REF:soaNLU].
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Having deep learning techniques however does not change the applied approach: it is always based on the observation of inputs and output pairs and learning a statistical model that will be able to predict something with sufficiently similar inputs. Random examples are presented to the computational graph that learns how to imitate them.
   </span>
  </p>
  <p>
   <span>
    Another approach that is relevant to mention is the Reinforcement Learning, that instead of statically seeing examples is more active also in the training epochs. Being able to act and observing the consequences on some rewarding functions it is able to learn what’s good and what’s not towards a certain goal. This is the field where computers recently have defeated human opponents in popular games, such as AlphaGo
   </span>
   <sup>
    <a href="#ftnt28" id="ftnt_ref28">
     [28]
    </a>
   </sup>
   <span>
    . In this example the reinforcement was applied also to matches between two computers. The strength of this dynamic training is to be able to act and observe the consequences of actions. In this way the machine is able to explore different choices and learn which one is better with respect to the rewards that have been established.
   </span>
  </p>
  <p>
   <span>
    A special kind of Reinforcement technique is the Adversarial ML, that by employing two components (the classifier and the adversary) with different goals builds a more robust classifier. This special training technique tries to overcome vulnerabilities of current Machine Learning systems, that may be exploited to generate wrong predictions if the inputs are preprocessed in some specific ways
   </span>
   <sup>
    <a href="#ftnt29" id="ftnt_ref29">
     [29]
    </a>
   </sup>
   <span>
    . This phenomenon of “AI hallucinations” is difficult to solve even with Obfuscated Gradients, and can make predict to the classifier any wrong output with only little but mirate changes on the inputs.
   </span>
   <sup>
    <a href="#ftnt30" id="ftnt_ref30">
     [30]
    </a>
   </sup>
   <span>
    The goal of Adversarial ML is to reduce the possible effects of adversarial examples by employing the adversary component abilities to reinforce the classifier via the trial-and error procedure.
   </span>
  </p>
  <p>
   <span>
    Even with
   </span>
   <span>
    Reinforcement Learning, the systems involved are unable to understand the ontological level of things: a system of this type is only able to work on a specific problem thanks to the optimization function that it learned to reduce. However what is unique to human intelligence is the ability to perform retrospective reasoning, in other words to truly understand the association between causes and consequences and being able to provide answer to associational questions.
   </span>
  </p>
  <p>
   <span>
    As Pearl
   </span>
   <sup>
    <a href="#ftnt31" id="ftnt_ref31">
     [31]
    </a>
   </sup>
   <span>
    analyzed, there are three levels of reasoning (seeing, doing, reasoning), that have strong dependencies between them and currently the Machine Learning approaches only reach the second level.
   </span>
  </p>
  <p>
   <span>
    An overview of building machines that are able to reach the third level (reasoning like humans) is provided in the next paragraph, as is a key characteristics of Artificial General Intelligence.
   </span>
  </p>
  <h4 id="h.k174ca6eq0n6">
   <span>
    Towards general AI
   </span>
  </h4>
  <p>
   <span>
    [LABEL:generalAI]
   </span>
  </p>
  <p>
   <span>
    The term Artificial General Intelligence, with its meaning of being able to perform tasks like humans, “
   </span>
   <span>
    involves the ability to reason, plan, solve problems, think abstractly, comprehend complex ideas, learn quickly and learn from experience. It is not merely book learning, a narrow academic skill, or test-taking smarts. Rather, it reflects a broader and deeper capability for comprehending our surroundings—’catching on,’ ‘making sense’ of things, or ‘figuring out’ what to do
   </span>
   <span>
    ”.
   </span>
   <sup>
    <a href="#ftnt32" id="ftnt_ref32">
     [32]
    </a>
   </sup>
  </p>
  <p>
   <span>
    Looking at the current situation of Artificial Intelligence, the gap with respect to General AI can be felt: the examples shown previously belong to a narrow field, with strong definitions of inputs and outputs, while this definition requires a very dynamic behaviour, learning what to do in contexts that were not analyzed.
   </span>
  </p>
  <p>
   <span>
    The main capabilities that are missing, as Lake et al.
   </span>
   <sup>
    <a href="#ftnt33" id="ftnt_ref33">
     [33]
    </a>
   </sup>
   <span>
    point out, are:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Building causal models that can support real understanding of causes and consequences
    </span>
   </li>
   <li>
    <span>
     Understanding of physics and psychology principles, in order to enrich the knowledge that is acquired through “senses” with general principles that rule the world
    </span>
   </li>
   <li>
    <span>
     Learning to learn in new environments, generalizing knowledge. → learn with small amounts of training data
    </span>
   </li>
  </ul>
  <p>
   <span>
    The first point is the same that has been analyzed by Pearl
   </span>
   <sup>
    <a href="#ftnt34" id="ftnt_ref34">
     [34]
    </a>
   </sup>
   <span>
    , and underlines the fact that mostly AI is applied to solving pattern recognition problems. Instead the human brain can build structured casual models thanks to the properties of the neocortex
   </span>
   <sup>
    <a href="#ftnt35" id="ftnt_ref35">
     [35]
    </a>
   </sup>
   <span>
    . This difference makes possible to recognize things also if they appear from different perspectives: for example in object recognition, rotated images can be misclassified simply because at training time the system was not trained from that point of view. Instead, having a richer model that simulates the 3d shape of objects helps to predict correctly also in those situations.
   </span>
  </p>
  <p>
   <span>
    The second point extends this characteristics: a prior knowledge of some general rules, in the example of rotated objects those rules are geometrical, can help understand better and faster the observation done on the domain. For example, learning to track an object knowing that it is solid and coherent, can help focusing on more important features, such as the trajectory, stability
   </span>
   <sup>
    <a href="#ftnt36" id="ftnt_ref36">
     [36]
    </a>
   </sup>
   <span>
    . Or for the psychological principles, observing a video gamer knowing that is trying to seek rewards while avoiding punishment can help focusing on his tactics and more advanced features.
   </span>
  </p>
  <p>
   <span>
    For the third point, between human learning and machine learning we know that the first one is more efficient and faster to learn from few examples. The ability of learning to learn comes from the causal models that give form to richly structured knowledge. This knowledge is not simply a set of patterns, but is a set of concepts that belong to a higher level of abstraction.
   </span>
  </p>
  <p>
   <span>
    From concepts, the human brain is able not only to do the pattern recognition task, but also to generate new examples and explain what are the major discriminator of the output classes. And those concepts are transferred along different problems thanks to the human memory.
   </span>
  </p>
  <p>
   <span>
    Machine Learning instead suffers that for every different problem it is trained from scratch. Because the learning network is designed for the current narrow process, the learning it has acquired is difficult to transfer to new applications. Humans, when trained on a completely new task, can exploit their previous experience on other problems and the richly structured knowledge they have been acquiring since their birth. This is also caused by the structure of the connections, that between artificial neurons are fixed. The brain instead, forming new synapses can quickly learn new things without affecting previous learnings.
   </span>
   <sup>
    <a href="#ftnt37" id="ftnt_ref37">
     [37]
    </a>
   </sup>
  </p>
  <p>
   <span>
    Following the article by Pearl et al.
   </span>
   <sup>
    <a href="#ftnt38" id="ftnt_ref38">
     [38]
    </a>
   </sup>
   <span>
    , the way to achieve better AI is to include physical and psychological fundamentals and generate models that capture the causality of the world. With those models as basis, new concepts could be learnt fast (thanks to Transfer Learning) and the training process could be really more efficient and require less examples.
   </span>
  </p>
  <p>
   <span>
    A very important feature of future machine learning, in the opinion of the authors, is to use
   </span>
   <span>
    compositionality
   </span>
   <span>
    (to put together the learned models from different problems) to build a single causal model that is able to learn new things and generalize faster.
   </span>
  </p>
  <p>
   <span>
    About machine reasoning, consider the MACNets approach
   </span>
   <sup>
    <a href="#ftnt39" id="ftnt_ref39">
     [39]
    </a>
   </sup>
   <span>
    that with the proposition of a new recurrent cell, Memory Attention Composition (MAC), tries to model both control and attention propagation. The problem where this approach is applied is Question-Answering on synthesized images
   </span>
   <sup>
    <a href="#ftnt40" id="ftnt_ref40">
     [40]
    </a>
   </sup>
   <span>
    , and the focus is on language compositionality and visual reasoning based on simple objects. The approach is able to answer questions such as “How many objects are either small cylinders or metal things?”.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    The scientific communities are quite divided in opinion about if and when AGI will be reached but from some surveys, completed by participants of conferences (Philosophy and Theory of AI 2011, AGI 12) and members of associations like EETN and “The 100 Top authors in artificial intelligence by citation in all years”, the opinion seems to be that it will be reached in this century
   </span>
   <sup>
    <a href="#ftnt41" id="ftnt_ref41">
     [41]
    </a>
   </sup>
   <span>
    .
   </span>
  </p>
  <p>
   <span>
    The opinions can be so positive about the reachability of this objective thanks to all the research that has been done in order to understand how the human brain works. Different projects exist today, both European
   </span>
   <sup>
    <a href="#ftnt42" id="ftnt_ref42">
     [42]
    </a>
   </sup>
   <span>
    and American
   </span>
   <sup>
    <a href="#ftnt43" id="ftnt_ref43">
     [43]
    </a>
   </sup>
   <span>
    that aim to explore this topic in neuroscience, computing and medicine. Models are built that can simulate brains and researchers can use them to do experiments in different fields: robotics, medicine, cognition.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    However nowadays the term AI is being used commercially as a fuzzy word, and is unfortunately generating a lot of examples that use the expression simply because they employ some statistical models but are far from the advanced processes that would be necessary to achieve AGI. The goal is simply to exploit the trending topic and generate some hype to promote products that are mostly based on rules or highly handcrafted features. It is the example of some Humanoid robots, that are remarkable good examples of progresses in human appearance and movements, but from the intelligent point of view are only well featured programs, far from the general intelligence.
   </span>
  </p>
  <p>
   <span>
    As humanoid examples of robots, we can start with Sophia, developed by Hanson Robotics, which has been granted citizenship by Saudi Arabia recently.
   </span>
   <sup>
    <a href="#ftnt44" id="ftnt_ref44">
     [44]
    </a>
   </sup>
   <span>
    This is an examples that is linked to many interesting topics. For some, this action “
   </span>
   <span>
    set a bad precedent for how we might treat robots in future
   </span>
   <span>
    ”
   </span>
   <sup>
    <a href="#ftnt45" id="ftnt_ref45">
     [45]
    </a>
   </sup>
   <span>
    and this will be covered in the next subsection [REF:aiGuidelines] about some guidelines. Moreover, it is not clear how much of this robot is real Artificial Intelligence and how much is just the emblematic representation of AI hype.
   </span>
  </p>
  <p>
   <span>
    Another example is Nadine
   </span>
   <sup>
    <a href="#ftnt46" id="ftnt_ref46">
     [46]
    </a>
   </sup>
   <span>
    , built at the Nanyang Technological University, that is able to recognize people and resume conversations based on previous chats.
   </span>
  </p>
  <p>
   <span>
    Those kind of robots are a combination of a wide number of AI methods: face tracking, emotion recognition, and robotic movements generated by deep neural networks
   </span>
   <sup>
    <a href="#ftnt47" id="ftnt_ref47">
     [47]
    </a>
   </sup>
   <span>
    . But anyway all those examples are far from general AI, because they work only in the specific field they have been designed to. The advances have been done on expanding the things that can be done, but purists of AI state that in this way the human intelligence cannot be reached
   </span>
   <sup>
    <a href="#ftnt48" id="ftnt_ref48">
     [48]
    </a>
   </sup>
   <span>
    . Human intelligence is more dynamic and general because it learns how to do new things in unbounded fields.
   </span>
  </p>
  <p>
   <span>
    Also considering the actual reinforcement learning, that learns from itself how to do tasks, is misleading because the fields in which this is applied are always defined a priori and the learning always occurs following a predefined rule. For example on the task of playing a specific game, a rewarding function is defined and the machine learns how to play better by challenging itself.
   </span>
  </p>
  <p>
   <span>
    Only by effectively performing machine reasoning, by enabling compositional and physical based thinking, AGI can be reached.
   </span>
  </p>
  <h3 id="h.wq3gbzxd2i57">
   <span>
    AI Guidelines
   </span>
  </h3>
  <p>
   <span>
    [LABEL:aiGuidelines]
   </span>
  </p>
  <p>
   <span>
    Also with the noticed distance from AGI, we can think about some general guidelines that should be followed when designing autonomous systems that are interacting with humans. Those guidelines can be seen as part of the AI ethics discussion
   </span>
   <sup>
    <a href="#ftnt49" id="ftnt_ref49">
     [49]
    </a>
   </sup>
   <sup>
    <a href="#ftnt50" id="ftnt_ref50">
     [50]
    </a>
   </sup>
   <span>
    .
   </span>
  </p>
  <p>
   <span>
    First of all, there are threats linked to the human-machine fight: physical threats. To avoid them, we can think of some simple principles, like the basic Three Laws of Robotics
   </span>
   <sup>
    <a href="#ftnt51" id="ftnt_ref51">
     [51]
    </a>
   </sup>
   <span>
    . Those rules, firstly expressed in a short story
   </span>
   <sup>
    <a href="#ftnt52" id="ftnt_ref52">
     [52]
    </a>
   </sup>
   <span>
    by Asimov, quoted from  the “Handbook of Robotics, 56th Edition, 2058 A.D.”, are:
   </span>
  </p>
  <ol start="1">
   <li>
    <span>
     A robot may not injure a human being or, through inaction, allow a human being to come to harm;
    </span>
   </li>
   <li>
    <span>
     A robot must obey the orders given it by human beings except where such orders would conflict with the First Law;
    </span>
   </li>
   <li>
    <span>
     A robot must protect its own existence as long as such protection does not conflict with the First or Second Laws.
    </span>
   </li>
  </ol>
  <p>
   <span>
    By observing those three rules, the main physical threats can be avoided. However, due to their historical birth, they may be a bit outdated and not consider all the possible consequences that may arise. This happens because the fields where autonomous systems can be applied vary dynamically with time, and the potential consequences change with them.
   </span>
  </p>
  <p>
   <span>
    More recently AI and robotics researchers, experts and other endorsers have produced a set of principles - the Asilomar AI Principles
   </span>
   <sup>
    <a href="#ftnt53" id="ftnt_ref53">
     [53]
    </a>
   </sup>
   <span>
    - that analyze issues that may arise in research (how to establish the goals, funding and cooperation of AI research towards beneficial intelligence), or in the fields of ethics and values (safety, human values, privacy, control), or also longer-term issues (like superintelligence for the benefit of humanity).
   </span>
  </p>
  <p>
   <span>
    About the ethics of Artificial Intelligence, there is a lot of discussion on the topic. James H. Moor on the relation between robots and ethics defines “
   </span>
   <span>
    Four Kinds of Ethical Robots
   </span>
   <span>
    ”
   </span>
   <sup>
    <a href="#ftnt54" id="ftnt_ref54">
     [54]
    </a>
   </sup>
   <span>
    :
   </span>
  </p>
  <ol start="1">
   <li>
    <span>
     Ethical impact agents: those systems, intentionally or not, can have an ethical impact by the actions they perform;
    </span>
   </li>
   <li>
    <span>
     Implicit ethical agents: are designed to avoid unethical outcomes, the strategy is to prevent them by limitations of the system capabilities;
    </span>
   </li>
   <li>
    <span>
     Explicit ethical agents: have algorithms to act ethically, they can identify and process ethical information and act accordingly;
    </span>
   </li>
   <li>
    <span>
     Full ethical agents: are as ethical as humans, thanks to features such
    </span>
    <span>
     free wil
    </span>
    <span>
     ,
    </span>
    <span>
     consciousness
    </span>
    <span>
     and
    </span>
    <span>
     intentionality
    </span>
    <span>
     .
    </span>
   </li>
  </ol>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Previously it has been mentioned that keeping a good distinction between the human intelligence and the intelligence provided by artificial systems, and understanding the advantages of both, could help focusing on the desired scope of the technology. Following this vision, we can say that artificial intelligence can provide an improvement to the society of today by being a set of useful tools. This is the idea expressed with the expression “utilitarian ethics for AI”
   </span>
   <sup>
    <a href="#ftnt55" id="ftnt_ref55">
     [55]
    </a>
   </sup>
   <span>
    .
   </span>
  </p>
  <p>
   <span>
    Letting the machines doing the stuff they can do better, such as automatic and repetitive tasks as has been done in factories by substituting human employees with robots in tasks like assembling of products, is needed in a word where fast production is a crucial element. Fields where this substitution is happening is with call centers, where the work is most repetitive and can be automated. While the automation in previous years has been focused on mechanical fields, now it is shifting to conversational fields. While machines do this works, the humans can focus on works they like more and on decisional processes.
   </span>
  </p>
  <p>
   <span>
    All where there is a distinction there is a point of contact, an interface, where the two sides meet and interact. Specifically for conversational agents, here are presented some boundaries that should be present on different dimensions that are examined in the following subsections:
   </span>
   <span>
    distinguishability
   </span>
   <span>
    ,
   </span>
   <span>
    autonomy
   </span>
   <span>
    and
   </span>
   <span>
    personality
   </span>
   <span>
    .
   </span>
  </p>
  <h4 id="h.2f4sbzb19wzh">
   <span>
    Distinguishability
   </span>
  </h4>
  <p>
   <span>
    One of the reasons that lead to the development of Conversational Agents is to emulate the human. The progress of those system is evaluated with measures of how seamless the interaction is and how likely the system can be confused with an human (see the Turing Test
   </span>
   <sup>
    <a href="#ftnt56" id="ftnt_ref56">
     [56]
    </a>
   </sup>
   <span>
    , or the Loebner Prize
   </span>
   <sup>
    <a href="#ftnt57" id="ftnt_ref57">
     [57]
    </a>
   </sup>
   <span>
    ). However, going in the opposite direction, we can put as a first general rule the distinguishability of autonomous systems. Following the “Turing’s red flag law”
   </span>
   <span>
    analyzed
   </span>
   <span>
    by Walsh
   </span>
   <sup>
    <a href="#ftnt58" id="ftnt_ref58">
     [58]
    </a>
   </sup>
   <span>
    , an autonomous system should be designed in a way to make clear that is not a human, and identify itself at the start of any interaction with other agents. In this article, the author is taking the expression “red flag law” from the Red Flag Act contained in the Locomotive Act
   </span>
   <sup>
    <a href="#ftnt59" id="ftnt_ref59">
     [59]
    </a>
   </sup>
   <span>
    , that stated that a self-propelled vehicle had to be led by a pedestrian waving a red flag or carrying a lantern to warn bystanders of the vehicle's approach. The term is modified by adding the artificial intelligence topic by referencing the author of the so largely known test.
   </span>
  </p>
  <p>
   <span>
    Walsh stands this principle in two parts. In the first one, the design itself of the system should be done by keeping in mind that the product is unlikely to be mistaken for human. This applies to the case of self-driving vehicles, that should be recognizable so that other actors on the road can have a more precise knowledge of the surrounding environment. This is crucial because the behaviour of human drivers and autonomous ones can be very different: both sides can make errors but of different kinds, a human can be distracted or fall asleep while a bot can do mistakes in situations it has not been designed to work.
   </span>
  </p>
  <p>
   <span>
    The second part is about stating the nature of the agent at the beginning of every conversation. This has to be done to be distinguished and put the interlocutor in the right setting and mood. Knowing the source of words is very important.
   </span>
  </p>
  <p>
   <span>
    The article also reports some examples for this specific part of the law. First of all with virtual assistants, that nowadays are so popular. Walsh observes that this rule is not always respected: if you ask Siri
   </span>
   <sup>
    <a href="#ftnt60" id="ftnt_ref60">
     [60]
    </a>
   </sup>
   <span>
    if she is human or not, the answer is not so clear. The playwright did that in order to keep a funny and unpredictable character of the assistant, but pretending to be human is a dangerous precedent. Now it is clear that they are AI, but with technological progress the difference could become unnoticed. Another example is with online games: bots can have some advantages and disadvantages, but user should know what kind of player they are playing with. Also when reading computer-generated text it should be explicit that the writer is not human: depending on the domain, this can impact the emotions of the reader.
   </span>
  </p>
  <p>
   <span>
    This criterion of distinguishability is not itself limiting the expressive power that the autonomous systems can have. It’s simply asking to make clear what species belongs the interlocutor.
   </span>
  </p>
  <h4 id="h.vt0e19a42ak8">
   <span>
    Autonomy
   </span>
  </h4>
  <p>
   <span>
    Another dimension in which there should exist a fixed barrier is the decisional one. On this dimension there should be a limit on what can be decided autonomously by the artificial agents in a way to establish on the one side the control of humans and their safety, and on the other one to allow some smart actions that improve the experience for humans. The goal of all the systems should be of this nature: being useful to the human user. Then the problems falls on how to decide what is good in an environment composed of different people with different goals. This problem is intrinsic in the society, also when no autonomous systems exist. Adding those presences, their function of autonomy must be aligned with the values and rules of the society.
   </span>
  </p>
  <p>
   <span>
    If their autonomy is none, there is no risk for the humans but there may be no advantages at all. With some degrees of freedom, regulamented in the right way (for example always letting human overrides and never hurting anyone), a good improvement can be done.
   </span>
  </p>
  <p>
   <span>
    A good regulamentation by the governments, maybe in a internationalized context, can help defining the boundaries in this dimension. This guideline is very similar to the first two Laws of Robotics
   </span>
   <sup>
    <a href="#ftnt61" id="ftnt_ref61">
     [61]
    </a>
   </sup>
   <span>
    .
   </span>
  </p>
  <p>
   <span>
    As an example, let’s take again the self-driving car. In the majority of the world this technology is not allowed to drive independently. Advanced cruise controls with lane assist can operate only if a driver with a licence is sitting on the driving place, ready to intervene in danger situations. Those kind of rules are necessary because those technology may not be ready and in general can find unexpected situations where they may fail.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    However in some cases there can be conflicts between what the user asks and the principle the system was designed with. This should be the only situation in which the system must disregard a command. If the system decides things on his own, also without conflicting with commands, this can be seen as a threat to human superiority on machines. The autonomy can be seen as menacing the safety and wellbeing of humans or also material resources
   </span>
   <sup>
    <a href="#ftnt62" id="ftnt_ref62">
     [62]
    </a>
   </sup>
   <span>
    . It’s a hierarchy problem. The humans should stay in control even if the AI becomes smarter. For this reason it would be better if autonomous systems in decisional processes are used only as advisors to empower humans to take smarter decision, not directly deciding on their own.
   </span>
  </p>
  <h4 id="h.9fg2s2vum9m4">
   <span>
    Personality and Emotions
   </span>
  </h4>
  <p>
   <span>
    AI can be seen as a threat to humans not only as direct menace, but also in more symbolic and subtle ways. Without establishing a strong discrimination between human and machine, the concept of identity and distinctiveness are threatened
   </span>
   <sup>
    <a href="#ftnt63" id="ftnt_ref63">
     [63]
    </a>
   </sup>
   <span>
    . Both kinds of threat lead to negative attitudes towards robots and robotics research. But beyond a possible negative attitude towards robotics generated from the recognition of those threats, we want to shortly describe the positive and negative effects on the individuals and society.
   </span>
  </p>
  <p>
   <span>
    We can take as examples the app Replika
   </span>
   <sup>
    <a href="#ftnt64" id="ftnt_ref64">
     [64]
    </a>
   </sup>
   <span>
    that resembles the Black Mirror episode “Be Right Back”: the goal of the app is to be “your best friend”, by learning from the conversation how to interact with the specific user and make him happy (rewarding function is the user feedback expressed via text or thumbs buttons).
   </span>
  </p>
  <p>
   <span>
    On the positive effect, having an artificial intelligence with some personality makes interactions more interesting and seamless for users. This makes the user feel more comfortable and can make the system to use emotions to express the vicinity to the user, using the Affective Computing concept
   </span>
   <sup>
    <a href="#ftnt65" id="ftnt_ref65">
     [65]
    </a>
   </sup>
   <span>
    .
   </span>
  </p>
  <p>
   <span>
    However, this emotional attachment to machines can cause also negative effects, both on the individual and on society. On the individual those system can cause addiction
   </span>
   <sup>
    <a href="#ftnt66" id="ftnt_ref66">
     [66]
    </a>
   </sup>
   <span>
    and isolation. The addiction is caused by the apparent relief given by interacting with something that agrees with things we say and its availability to listen to us. The isolation is the consequence of this addiction, and could be disruptive especially for the youngest ones that already under the effect of mobile addiction have a high decrease on empathic abilities
   </span>
   <sup>
    <a href="#ftnt67" id="ftnt_ref67">
     [67]
    </a>
   </sup>
   <span>
    and can sometimes fall in a very problematic isolation situation like
   </span>
   <span>
    hikikomori
   </span>
   <sup>
    <a href="#ftnt68" id="ftnt_ref68">
     [68]
    </a>
   </sup>
   <span>
    . And these changes on the individual and his personality can also affect society, weakening the human relations.
   </span>
  </p>
  <p>
   <span>
    For these reasons, a boundary needs to be defined in order to avoid emotional attachment and other inappropriate feelings towards machines. From the societal point of view, people need to keep their life in real world with real people, and use only the autonomous systems not as targets of happiness but as means to achieve wellness in the human only environment of feelings and emotions.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    These guidelines should put the design of autonomous systems oriented towards the wellbeing of humans, helping with repetitive tasks and providing powerful services and enhancing the connection between humans. All this should interface in a easy natural way for the humans, using natural language both in spoken and in written forms.
   </span>
  </p>
  <h2 id="h.ji6a57fyg5ls">
   <span>
    Remainder
   </span>
  </h2>
  <p>
   <span>
    The next chapter [REF:soa] gives a deep analysis on the topic of Conversational Agents. Starting from a classification [REF:soaClassification] of agents based on the contents of the dialogue, a description is given of the available approaches that best fit different contents. The focusing on the Goal-oriented agents the available Machine Learning approaches are analyzed [REF:soaNLU] focusing on the specific type of Neural Networks (Recurrent ones) that achieve the State of the Art condition for the specific goals: sentence classification (intent recognition) and extraction of parameters (slot filling). The chapter ends with an analysis of the personalization techniques [REF:soaPersonalization], describing how the user features are used both in content-based and collaborative filtering and what are the available options to avoid the cold start problem.
   </span>
  </p>
  <p>
   <span>
    The chapter of the Approach [REF:approach] instead focuses on the specific Chatbot use case: the bot for bike sharing information. The scenarios are delineed [REF:approachScenarios] and the high level model of the system is described [REF:approachModel] by also giving the motivation for having two different components that interact [REF:approachOmnichannel]. Then moving to the Natural Language Understanding [REF:approachNLU], a description of the intent and entities chosen [REF:approachTypes] proceeds the analysis of the selected approach is given for single-turn interaction
   </span>
   <sup>
    <a href="#ftnt69" id="ftnt_ref69">
     [69]
    </a>
   </sup>
   <span>
    in [REF:approachSingleTurn], and the proposed modifications for multi-turn interactions in [REF:approachMultiTurn]. On the topic of personalization [REF:approachPersonalization] two main needs emerge: provide content recommendation under the form of places near the path of the cyclist [REF:approachRec], and performing a tailored communication with the user in means of different operative modes, personal preferences and different linguistic style. In conclusion of this chapter, a description of the information retrieval techniques needed as dependencies to the presented approach are given [REF:approachIR].
   </span>
  </p>
  <p>
   <span>
    The fourth chapter [REF:implementation] focuses on the implementation of the Bot prototype. This includes the interaction mechanism with chat platforms [REF:implementationInteraction] and the Neural Network computational graph [REF:implementationNLU]. This last section describes the exploitation of an online NLU provider in the initial stage of the prototype [REF:implementationWit], followed by the choice of the Neural Network framework [REF:implementationNN] and the implementation details [REF:implementationNNDetails]. Then the two last subsection end the chapter: one for the word embeddings [REF:implementationWV] computed for the Italian language and retrieved for the English one, and the other for the collection of the single and multi-turn datasets [REF:implementationDatasets].
   </span>
  </p>
  <p>
   <span>
    The fifth chapter [REF:validation] contains the evaluation of the system, both for the Natural Language Understanding performance [REF:validationNLU] and for the overall system [REF:validationWhole]. For the NLU are presented the datasets used [REF:validationDatasets] both for single-turn and multi-turn interactions, then a description of the measures [REF:validationMeasures] and the obtained results [REF:validationResults], comparing the effects of different choices.
   </span>
  </p>
  <p>
   <span>
    The last chapter [REF:conclusion] concludes this work by underlying the reached objectives, both on personal side as acquired competences and as objective results, and prospecting possible future works.
   </span>
  </p>
  <hr style="page-break-before:always;display:none;"/>
  <p>
   <span>
   </span>
  </p>
  <h1 id="h.142k2sh6q0q7">
   <span>
    State of the Art
   </span>
  </h1>
  <p>
   <span>
    [LABEL:soa]
   </span>
  </p>
  <p>
   <span>
    In this section are presented the related works on the topic of Conversational Agents. Having done some considerations about how the interaction between autonomous agents and humans was born and which social implication can rise, the discussion will focus on a classification of chatbots [REF:soaClassification]. Then the focus will go deep into technologies that can help building bots that can interact with the user through Natural Language [REF:soaNLU]. The last part will cover the topic of personalization [REF:soaPersonalization].
   </span>
  </p>
  <h2 id="h.13m1srey0vz1">
   <span>
    Chatbots and their classification
   </span>
  </h2>
  <p>
   <span>
    [LABEL:soaClassification]
   </span>
  </p>
  <p>
   <span>
    Starting from the formulation of the Turing test, the problem of human-machine interaction has been analyzed and different solution have been found. The terms that are commonly used for those kind of systems may vary (e.g
   </span>
   <span>
    chatbots
   </span>
   <span>
    ,
   </span>
   <span>
    conversational agents
   </span>
   <span>
    ,
   </span>
   <span>
    virtual assistants
   </span>
   <span>
    ), but the substance does not change: one interlocutor is not of human nature
   </span>
   <span>
    .
   </span>
  </p>
  <p>
   <span>
    After the mobile-first wave that lead to development of thousands of applications for devices that people pass the day with (e.g. smartphones, tablets, wearable devices and smart watches), is the transition to intelligent interaction that put the emphasis on natural and seamless interactions with automated systems. The interaction mean shifts from using well-designed and sometimes complicated interfaces made of buttons and paged procedures to textual or vocal dialogue. Asking questions naturally has many advantages with respect to traditional app interaction. The main one is that the user does not need to know how the specific app works, everyone knows how to communicate and in this case the system is coming towards the user to make the interaction more natural.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    The evolution of these systems started long time ago, with first systems that were built to emulate a natural conversation, and has lead to today’s virtual assistants that live on our smartphones and are ready to complete tasks for us.
   </span>
  </p>
  <p>
   <span>
    The approaches that have been used have evolved through time to fit different needs and to overcome challenges that arise while developing such systems. The choices of the approach to use depend a lot on the purpose of the bot. Bots can be designed to entertain the user in a conversation or can be designed to provide information on a specific field.
   </span>
  </p>
  <p>
   <span>
    By firstly looking at the common applications of chatbots [REF:soaApplications] and existing classification in literature [REF:soaClassificationsExisting], a classification will be done by considering the contents [REF:soaClassificationContents] and the approaches [REF:soaClassificationApproaches]. At the end the main challenges [REF:soaChallenges] are listed.
   </span>
  </p>
  <h3 id="h.e9c8xwqz4owm">
   <span>
    Applications of chatbots
   </span>
  </h3>
  <p>
   <span>
    [LABEL:soaApplications]
   </span>
  </p>
  <p>
   <span>
    Conversational Agents can be applied in all the situations where there is a repetitive exchange of information with the user. The advantages can be both for users and companies.
   </span>
  </p>
  <p>
   <span>
    On the side of the user, using applications can be quite frustrating sometimes. Every company has a different app that needs to be installed, configured and learnt to be used. The usage itself, however the efforts of the designers, may result cumbersome.
   </span>
   <span>
    User interface forces you to fill up information in a form-like structure. When finally you try to submit, you find out that you missed one required field. A conversational interface could simplify a lot this process, even though you are simply doing slot-filling and asking one input after the other.
   </span>
   <sup>
    <a href="#cmnt3" id="cmnt_ref3">
     [c]
    </a>
   </sup>
  </p>
  <p>
   <span>
    Other
   </span>
   <span>
    advantages of conversational interfaces can be found when interacting with structured data whose criterion of navigability and search are not well known. Having an interlocutor that progressively helps refining our search, instead of filling a large form in a app, can help being more productive. Moreover, voice commands can be used also without need to look and use hands, for situations where our attention is needed for other tasks.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    From the point of view of the companies, relations with the customer is the main channel to acquire and maintain customers. It is where the effort to understand user's needs should be maximum, and in many cases opting for offshore call centers may not be good and not very cheap. In these situations, having an automated responder could provide the service in the way you want in a scalable way.
   </span>
  </p>
  <p>
   <span>
    The advantages of having those systems instead of dedicated people in
   </span>
   <span>
    call centers
   </span>
   <span>
    for customer support are many. First of all, the companies can support more customers because of the high scalability of those systems. For the majority of support requests, an automated response can be helpful. In case the system does not understand well the requests, a backup human solution can be called into action: in this way, also some very specific responses that were not designed in the requirements can be provided. The second major advantage for companies is that in this way they can have a set of responses more aligned with their guidelines. Establishing how the system should answer some types of questions once and for all, instead of having possible disinformation and misalignment of human employees. Eventually, when the automated system detects some difficulties due to the specificity of some issues with the user, an human responder can be used as backup solution: the chatbot will provide a first-line service and manage all the trivial interactions with the customers, using the human employees only when necessary.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Chatbots are also used in the field of
   </span>
   <span>
    virtual assistants
   </span>
   <span>
    . Using voice or text, we can interact on mobile devices with something that can quickly do things for us: adding events to the calendar, making phone calls and searching information on the web. Virtual assistants provide a fast way to interact with the device, useful in situations where the users can be impossibled to interact in the visual way, for example while driving.
   </span>
  </p>
  <p>
   <span>
    Also virtual assistants mainly fall into this category. On pre-determined domains (such as agenda management, meteo, sending messages) or also with external domains integrated by third-party apps (such as Alexa Skills
   </span>
   <sup>
    <a href="#ftnt70" id="ftnt_ref70">
     [70]
    </a>
   </sup>
   <span>
    , Cortana integration
   </span>
   <sup>
    <a href="#ftnt71" id="ftnt_ref71">
     [71]
    </a>
   </sup>
   <span>
    , ...) the assistant can help on a set of intents.
   </span>
  </p>
  <h3 id="h.bruuy6iym2m7">
   <span>
    Existing classifications
   </span>
  </h3>
  <p>
   <span>
    [LABEL:soaClassificationsExisting]
   </span>
  </p>
  <p>
   <span>
    Since the fields of application and the possibilities are many, a search of classifications has been done in order to divide and find the main characteristics of this kind of bots.
   </span>
  </p>
  <p>
   <span>
    Chatbots, in their larger definition, are software agents with whom you can carry a conversation. Accordingly to
   </span>
   <span>
    Franklin et Graesser
   </span>
   <sup>
    <a href="#ftnt72" id="ftnt_ref72">
     [72]
    </a>
   </sup>
   <span>
    , there exist three main types of autonomous software agents that have different
   </span>
   <span>
    objectives
   </span>
   <span>
    : task-specific, entertainment and viruses, as can be seen in figure [REF FIG:franklinClassification].
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 313.33px;">
    <img alt="" src="https://lh4.googleusercontent.com/n8acarQ3hEyrd8SZJQieNdUe_-JFmmduV7-bjv1PJpTqzQ14oAy7Ugf4Mjnjpnto_tYWVdq4cVECb1qQ23VwUJy6zBsKB-ZkCJwz73_eXCASwXX2TZ2ZajGkxbzwXAcryg_AR1Wl" style="width: 602.00px; height: 313.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    [FIG:franklinClassification CAPTION:Natural Kinds Classification of Autonomous Agents
   </span>
   <sup>
    <a href="#ftnt73" id="ftnt_ref73">
     [73]
    </a>
   </sup>
   <span>
    ]
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Another division can be done by looking at the type of control mechanism: from rule-based systems that only follow handcrafted rules, to machine learning ones that learn dynamically from examples how to interact.
   </span>
  </p>
  <p>
   <span>
    A very important characteristic to be considered when classifying conversational agents is the
   </span>
   <span>
    initiative
   </span>
   <sup>
    <a href="#ftnt74" id="ftnt_ref74">
     [74]
    </a>
   </sup>
   <span>
    . With System Initiative only, the conversation is lead by the bot and the user has only the possibility to say a finite set of things and cannot change directly the current state of the dialogue. It is the case when the user always replies to questions made by the bot, or when the only possibility of inputs is given by buttons. In dialogues between humans, the initiative usually shifts from one participant to the other, and the same should apply in interactions with autonomous systems. For this reason,
   </span>
   <span>
    there exist
   </span>
   <span>
    mixed-initiative systems that try to take into consideration the user requests for tracking the dialogue state (and not imposing it).
   </span>
  </p>
  <p>
   <span>
    Yet another division can be done by considering if the interaction scheme is single-turn (question followed by a response that only depends on the last question) of if the memory is required to perform a multi-turn dialogue. Multi-turn enables to refine searches and refer to entities previously mentioned, and are more natural than single-turn.
   </span>
  </p>
  <p>
   <span>
    Other classifications that have been found are based on features such as believability and intelligence
   </span>
   <sup>
    <a href="#ftnt75" id="ftnt_ref75">
     [75]
    </a>
   </sup>
   <span>
    . Those features are reflected on how the user perceives the agent and are somehow similar to the goal of the imitation game: giving the illusion to talk to a person.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Given briefly those classification options, the choice has been to focus on two main dimensions for doing a classification: the first one (section [REF:soaClassificationContents]) considers the contents of the dialogue, and is inspired by the objectives of Franklin et Graesser
   </span>
   <sup>
    <a href="#ftnt76" id="ftnt_ref76">
     [76]
    </a>
   </sup>
   <span>
    . The second one (section [REF:soaClassificationApproaches]) instead focuses on the approaches that are used.
   </span>
  </p>
  <h3 id="h.qw2bbdmuem77">
   <span>
    The contents
   </span>
  </h3>
  <p>
   <span>
    [LABEL:soaClassificationContents]
   </span>
  </p>
  <p>
   <span>
    This first dimensions considers the contents of the conversation. Retaking the three objectives by Franklin et Graesser
   </span>
   <sup>
    <a href="#ftnt77" id="ftnt_ref77">
     [77]
    </a>
   </sup>
   <span>
    (task-specific, entertainment, viruses) of autonomous software agents and removing the last one that is not inherent to conversational agents, the entertainment one is mapped to a generic chit-chat content (maintaining a general conversation with the user), while the task-specific one corresponds to domain specific knowledge (the bot provides it to the user acting as a natural language interface).
   </span>
  </p>
  <p>
   <span>
    A third content type has been added to be able to categorize all those type of system that deal with different sources of information and combine them to provide rich answers. This may be called Encyclopedic Knowledge content, and the main ambition of those systems is to provide real answers to complex questions related to the world knowledge.
   </span>
  </p>
  <p>
   <span>
    Those three main content types are explained in the following paragraphs, before going to discuss the existing approaches.
   </span>
  </p>
  <h4 id="h.xtuxn5hkhwl2">
   <span>
    Chit-chat
   </span>
  </h4>
  <p>
   <span>
    The first type of content, that correspond to the native setting of the turing test because the goal is to entertain the user making him believe that the machine really understands the conversation, the history of chatbots is quite long.
   </span>
  </p>
  <p>
   <span>
    The first chatbot ELIZA
   </span>
   <sup>
    <a href="#ftnt78" id="ftnt_ref78">
     [78]
    </a>
   </sup>
   <span>
    , that was built in 1966, is of this kind. It was created mainly to demonstrate the superficiality of communications and the illusion to be understood by a system that is simply applying a set of pattern-matching rules and a substitution methodology.
   </span>
  </p>
  <p>
   <span>
    ELIZA simulates a psychotherapist and, thanks to the trick of present again to the interlocutor some contents that have been previously mentioned, keeps the conversation without having an understanding of what really is said.
   </span>
   <sup>
    <a href="#ftnt79" id="ftnt_ref79">
     [79]
    </a>
   </sup>
   <span>
    At the time when ELIZA came out, some people even attributed human-like feelings to the bot. A lot of other computer programs have been inspired by ELIZA. Even a markup language has been created to express the rules that drive the conversation (see AIML in
   </span>
   <span>
    [REF:aiml]
   </span>
   <span>
    ).
   </span>
  </p>
  <p>
   <span>
    A competition has been created to reward the progresses in this field: the Loebner Prize, an annual challenge that stands in the format of the Turing tests, but restricting the topics of conversation. Judges keep parallel conversations, one with a human and one with a computer program. Judges are chosen with the criterion that they “should respond naturally, as they would in a conversation with another person”, in a way to avoid excessive sophistication. At the end the winner is the computer program that mostly convinced the judges (even without passing the Turing test). There are two more prizes in this competition: one is for the Turing test in text only interactions, the last one (the biggest one) is for passing the Turing test including visual and auditory inputs. This competition was first held in 1991, and with the years advancing the challenges have became more and more complex.
   </span>
  </p>
  <p>
   <span>
    The Loebner Prize has been criticized by experts in the field because it rewards the usage of some tricks
   </span>
   <sup>
    <a href="#ftnt80" id="ftnt_ref80">
     [80]
    </a>
   </sup>
   <span>
    , pointing the focus on imitation instead of intelligence.
   </span>
  </p>
  <p>
   <span>
    The commonly used tricks are
   </span>
   <sup>
    <a href="#ftnt81" id="ftnt_ref81">
     [81]
    </a>
   </sup>
   <span>
    :
   </span>
  </p>
  <ul>
   <li>
    <span>
     Let the conversation to be driven by the interlocutor. This works because most people like to talk about themselves and only want someone that listens
    </span>
   </li>
   <li>
    <span>
     Give the illusion to be listening and understanding, by including substrings of the user input
    </span>
   </li>
   <li>
    <span>
     Changing the topic when not understanding, that sometimes can be seen as paranoid
    </span>
   </li>
   <li>
    <span>
     Simulated typing, delaying the responses
    </span>
   </li>
  </ul>
  <p>
   <span>
    The details of rule-based chatbots will be explained in more detail in [REF:soaRuleApproach].
   </span>
  </p>
  <p>
   <span>
    The evolution of chit-chat bots has recently evolved from a rule-based one to a generative approach. The key idea is to imitate existing conversation, and generate a response in a way that reflects both the training corpus both the current user turn (used as a stimulus). This approach, that will be discussed deeper in
   </span>
   <span>
    [REF:soaGenerativeApproach]
   </span>
   <span>
    , has its roots in using Statistical Machine Translation. The importance of the dialogue corpus, that dynamically establishes how the responses are generated, makes their availability a key element for a successful bot. For this reason these models are trained on the few datasets publicly available: tweets, reddit discussions, ubuntu dialog corpus.
   </span>
  </p>
  <p>
   <span>
    An example of this kind of systems is the mobile application Replika
   </span>
   <sup>
    <a href="#ftnt82" id="ftnt_ref82">
     [82]
    </a>
   </sup>
   <span>
    , that wants to provide a chat companion for its users. The conversation has no specific goals, and the system entertains the user with long discourses and games.
   </span>
  </p>
  <p>
   <span>
    Focused on psychological support, an example of conversational system is Woebot.
   </span>
   <sup>
    <a href="#ftnt83" id="ftnt_ref83">
     [83]
    </a>
   </sup>
   <span>
    Entertaining the user
   </span>
   <span>
    with dialogues, it analyses the user mood and provides tips to feel better against depression and anxiety
   </span>
   <sup>
    <a href="#ftnt84" id="ftnt_ref84">
     [84]
    </a>
   </sup>
   <span>
    .
   </span>
  </p>
  <h4 id="h.lofizn2zbeuf">
   <span>
    Goal oriented
   </span>
  </h4>
  <p>
   <span>
    Another completely different type of bots is the one that provides a service on a restricted domain. It can be a natural language interface to a set of static Frequently Asked Questions, or can be linked to some dynamic content using some defined APIs. Those systems need to define the list of things they can do, and in some way map the user requests to some actions.
   </span>
  </p>
  <p>
   <span>
    Using those systems, the user should be aware of what the system can do and what cannot. Letting the user know that the bot he is interacting with can provide information only on the domain it is built for, should be a goal of the ideators. Responses for a little chit-chat conversation can also be added to the system, but they should be used only to avoid a general “
   </span>
   <span>
    I don’t know
   </span>
   <span>
    ”/”
   </span>
   <span>
    I didn’t understand
   </span>
   <span>
    ”.
   </span>
  </p>
  <p>
   <span>
    Those kind of bots can provide a more natural way of interacting with companies, both in the field of customer support and in search for information. The information can be statically defined as pairs of questions-answers. In this case the bot has to classify the user requests and provide the answer that mostly fits it. Or there can be some dynamic information that need to be extracted from the request. In this case, in addition to sentence classification, a parameter extraction needs to be performed. These tasks are the ones that this thesis will later focus on, and are the main part of the Natural Language Understanding process, that is described better in
   </span>
   <span>
    when introducing the approaches in [REF:soaNLUIntro])
   </span>
   <span>
    and analyzed in detail in
   </span>
   <span>
    the section [REF:soaNLU]. The NLU approaches are quite handcrafted on the possible sentence types, and for this reason some
   </span>
   <span>
    more generative approaches are coming also in this field, inspired by the chit-chat domain. But what characterizes the conversational agents that provide this kind of content, is the presence of a goal from the user: this is the reason that makes them called
   </span>
   <span>
    goal oriented
   </span>
   <span>
    agents.
   </span>
  </p>
  <h4 id="h.gpa5vfh7ly2m">
   <span>
    Encyclopedic Knowledge
   </span>
  </h4>
  <p>
   <span>
    As last type of chatbots, we can consider a more rich version of the domain specific bot. Instead of limiting on a very restricted domain, and predetermining the abilities with a static design of intents, this kind of bots tries to provide a natural language interface towards a linked Knowledge Base. The fixed-structure intent based approach seems not to be optimal when the number of possible intents is very high and cannot be predetermined. If the system allows questions with a high level of complexity, requiring the linking of information, the number of intents quickly can explode.
   </span>
  </p>
  <p>
   <span>
    Instead of doing a classification of sentences on some pre-defined intents, the goal of those systems is to transform the user sentence in a database query (for structured knowledge) or also to extract information from unstructured knowledge (such as documents expressed in natural language). This requires a strong relation between the NL module and the Information Retrieval module. Structured Knowledge can be composed of entities and relations between them. Information is in the form of RDF (Resource description framework) which have links to other entities with specific roles.
   </span>
  </p>
  <p>
   <span>
    The sentences are analyzed using some parsers and are mapped to a set of linguistic patterns. After this translation (from human natural language to computer-understandable queries), the interrogation is performed on Semantic Web. Those Question-Answering systems are actually very complex and involving different tasks and challenges
   </span>
   <sup>
    <a href="#ftnt85" id="ftnt_ref85">
     [85]
    </a>
   </sup>
   <span>
    .
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    A commonly used system that provides this kind of content is the Google Knowledge Graph, that explores the queries done on the search engine and provides linked information. Another example of them is the Wolfram Search, that can be asked questions like “
   </span>
   <span>
    who is the USA president?
   </span>
   <span>
    ”. The focus here is not on managing complex dialogues, but understanding complex queries and exploring Knowledge Graphs.
   </span>
  </p>
  <p>
   <span>
    There are however examples of encyclopedic knowledge systems that try to convey the conversational abilities of chit-chat dialogues empowering them with richer contents. This content ingestion is at the basis of the prototypes participating to the
   </span>
   <span>
    Alexa Prize
   </span>
   <sup>
    <a href="#ftnt86" id="ftnt_ref86">
     [86]
    </a>
   </sup>
   <span>
    . The goal that participants should try to achieve in this prize is to reach 20 minutes of dialogue with the user. Given a set of services (such as ASR, NLU, TTS), the teams have to design the dialogues and make it possible to carry on the conversation for the longest amount of time. Keeping such a long dialogue cannot be done by only providing simple responses using the tricks of ELIZA. Reasoning on topics requires having knowledge of them, so this kind of conversational agents are connected to knowledge bases.
   </span>
   <sup>
    <a href="#cmnt4" id="cmnt_ref4">
     [d]
    </a>
   </sup>
  </p>
  <h3 id="h.ihayguhqsggl">
   <span>
    The approaches
   </span>
  </h3>
  <p>
   <span>
    [LABEL:soaClassificationApproaches]
   </span>
  </p>
  <p>
   <span>
    The other dimension that is considered to make this classification is relative to the approach used. As will be seen shortly, bots require different stages (understanding, getting some information, answering) and different approaches can be applied to all of them.
   </span>
  </p>
  <p>
   <span>
    As can be seen from the discussion on the contents of the dialogue, there is an evolution of the approaches, leading to data-driven ones. The first wave of chatbot, as mentioned before, simply matched the input sentences with some keywords and responded in a way to make the conversation look natural. From this, a lot of other agents grew up with the same technique: pattern matching. With this approach, lots of bot have been built and even a specific markup language has been developed to express the patterns and link them to some responses.
   </span>
  </p>
  <p>
   <span>
    More recent approaches instead tend to abandon the handcrafted rules towards an automatic learning from a dialogue corpus. This can be applied both to the part of natural language understanding (turn the user sentences into structured data) or also to the information retrieval and response generation.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    We can do a classification of chatbot systems based on how much has to be specified and defined in a static way versus a dynamic approach that learns from the examples provided. This choice can be done in the three main tasks: understanding, information retrieval, response generation. Each of them can have three levels of dynamicity: the first one is completely rule-based, the second one is mixed, usually presenting a dynamic categorization over pre-defined types, while the third one is completely dynamic. In the following table [REF TABLE:approachCombination] the levels are shown for each task.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <a id="t.d0fb09237e66600b0a53388ee8fa6162a974d9eb">
  </a>
  <a id="t.0">
  </a>
  <table>
   <tbody>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Dynamicity level
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Understanding
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Information retrieval
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Response generation
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        0
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Pattern based (entities and intents)
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        None / fixed data
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        rule-chosen template
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        1
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Machine Learning (entities and intents)
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Fixed API
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        dynamically chosen template: retrieval based
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        2
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Encoding (sentence embeddings)
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        dynamic exploration
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        generative
       </span>
      </p>
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   <span>
    [TABLE:approachCombination CAPTION:The different approaches to Understanding, Information Retrieval and Response Generation]
   </span>
  </p>
  <p>
   <span>
    The understanding task is the one related to turning the natural language sentences into something that can be understood by the machine: its output can be a pattern match for level 0, structured data for level 1 (NLU) or an encoded representation for level 2. More details about those possibilities will be described shortly.
   </span>
  </p>
  <p>
   <span>
    The information retrieval task instead, if present, can use a fixed set of APIs or be more dynamic, interacting with information and their relations (graph knowledge bases).
   </span>
  </p>
  <p>
   <span>
    Instead the response generation can be fully governed by handcrafted rules (e.g. if a set of conditions apply, say that). Or decide the template response from a finite set using statistical approaches (using some proximity measures like tf-idf, word2vec, skip-thoughts). Or being generative, with responses that are built on the fly by a seq2seq model.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 209.33px;">
    <img alt="" src="https://lh4.googleusercontent.com/hLQoU0u3zlWWpmYybWRv3BN7C0ru8hCPcmVuXM0B4JSuisGZf6baIMTYAAFSDEInk8luqlGWHUWQjHjI5oeFh1Xl7_AIBsPd6m0uc_aGPSCRYZZmc2nEZqnXm1UHPCPKgXntGzhX" style="width: 602.00px; height: 209.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    [FIG:approachesCombination CAPTION:How the approaches for each task can be connected together]
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    The choices are not independent however, as can be seen from figure [REF FIG:approachesCombination]: since the tasks are put in a chain (first understanding, then retrieving the information and ultimately formulating a response), the approach chosen on the first stages has consequences on the following ones. Choosing to use NLU instead of dynamic understanding precludes the possibility to have generative responses, because the user utterances will be categorized on a finite set of types (intent types) unless a hybrid understanding is performed (NLU to find the intent types with a parallel sequence-to-sequence model for chit-chat dialogues). For this reason and being the understanding stage the first one, the details on the approaches will be analyzed mainly on this first task, and for each level in it some details will be given on the possibilities for the other two choices.
   </span>
  </p>
  <p>
   <span>
    The choice between those possibilities has to be done based on the available data (e.g. if transcripts of existing human conversations exists), and on the main content the system will provide (see above, between chit-chat, domain-specific, encyclopedic knowledge) because the approach performance depend a lot on it. It is also necessary to keep in mind that complex dialogues actually can perform better if at dialog management level an handcrafted approach is kept. Machine learning can help reconduct on sentence types, but actually structured topic dialogues need handcrafting. As of today, deductive (from rules) decision-taking in conversation gives better performances with respect to inductive (only learning from examples), because it gives more control to the designers of dialogues.
   </span>
   <sup>
    <a href="#ftnt87" id="ftnt_ref87">
     [87]
    </a>
   </sup>
  </p>
  <p>
   <span>
    Moving the level of dynamicity higher, the
   </span>
   <span>
    complexity
   </span>
   <span>
    moves from handcrafting very specific features, that can be very specific to the domain of application and therefore provides a solution that difficulty can be ported to new domains, to the complexity of the approach itself. The transition usually is towards approaches that involve machine learning techniques like neural networks: the engineering of those networks is quite complex with respect to writing patterns to cover a set of sentences. In addition, also more computing power is required to run those heavy systems.
   </span>
  </p>
  <p>
   <span>
    Changing the selected approach (especially on the understanding and generation processes) also has consequences on the additional work that has to be done to port the agent to
   </span>
   <span>
    other languages
   </span>
   <span>
    . A rule-based understanding is highly coupled with the used language. Using NLU reduces a lot this dependence because instead of manually deciding how to categorize the sentences, the system will find automatically how to do that even in another language, if the appropriate annotated training examples are provided and if the network structure itself is sufficiently language independent. As will be described in the NLU chapter, using features of words that are more related to their meaning (contextual and semantic, following the “distributional semantics” concept) instead of their grammatical role, helps generalizing and being less dependent on the selected language (see usage of word embeddings and how they can be easily generated for any language).
   </span>
  </p>
  <p>
   <span>
    Summing up, a more dynamic, end-to-end approach has an higher complexity but leads to easily adaptable solutions, in terms of new domains and new languages.
   </span>
  </p>
  <h4 id="h.vywjvd4e8uos">
   <span>
    Rule based
   </span>
  </h4>
  <p>
   <span>
    [LABEL:soaRuleApproach]
   </span>
  </p>
  <p>
   <span>
    This approach has both the understanding part and the response generation specified as rules. This approach was firstly used for chit-chat systems, and the number of rules can be quite big. Those rules can define theirself a response or can refer to other rules in order to reduce the amount of different actions. However this strategy is not very good because a lot of rules must be written in order to span all the possible variations of sentences. Moreover, since the majority of the work is to write those rules, and the rules highly depend on the language analyzed, this approach can hardly be applied on different languages and the process of migration from one to another requires the rewriting of the whole set of rules.
   </span>
  </p>
  <p>
   <span>
    Chit-chat systems historically don’t use knowledge bases to provide responses. The information that they provide are statically written as part of their personality and background.
   </span>
  </p>
  <p>
   <span>
    This approach was firstly used by ELIZA
   </span>
   <sup>
    <a href="#ftnt88" id="ftnt_ref88">
     [88]
    </a>
   </sup>
   <span>
    . Then when used by ALICE, an extended version of ELIZA, there has been the release of AIML, that is the markup language used for expressing the rules. With the release of AIML, lots of implementations of chatbots used this approach.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    [LABEL:aiml]
   </span>
  </p>
  <p>
   <span>
    AIML
   </span>
   <span>
    stands for Artificial Intelligence Markup Language, and is a language that describes how the bot replies to certain inputs. Inputs are evaluated against a set of patterns and the best match (category) is chosen. The actions performed by the category can be a simple response, or can also set variables and call other categories.
   </span>
  </p>
  <p>
   <span>
    Those rules determine together the classification of the sentences and the response generation.
   </span>
  </p>
  <p>
   <span>
    A rule (in the AIML jargon called category) is made up of a pattern and a template. As can be seen in [REF FIG:aimlExample], the pattern is responsible to identify the sentences that activate the rule, while the template is the part that manages the response: a template can be both a sentence that will be sent to the user or can be a redirection towards another rule (srai rules).
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 184.00px;">
    <img alt="" src="https://lh6.googleusercontent.com/GqSbiRxnFERo1rKffnuMpc2UK1QiFOk1_ZND8YruS2JImYjjWT2irzndCS3DsLYmP2iNd7fbNQBghBpIIb1_lxt3YTv2Twh4oB5UC0MBoWV0-Vcda2V9iNSJCIgYlij9biZUbFk9" style="width: 617.53px; height: 184.00px; margin-left: -7.77px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    [FIG:aimlExample CAPTION:An example of the rule-based AIML]
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    In addition to those two elements, some actions to manage the state of the conversation can be added: variables can be set and read to manage the FSM and to conditionally activate rules.
   </span>
  </p>
  <p>
   <span>
    While writing those rules can be quite easy, the problem is that a lot of them must be written to manage the variations on the natural language. In fact the strategy used for managing the development cycle of bots empowered by AIML is a cyclical process: starting with a model with some rules, the system is tested against a group of test users collecting the interactions; at the end of the test session, the logs are analyzed and the botmasters modify and add rules to provide suitable responses. This process is called
   </span>
   <span>
    targeting
   </span>
   <span>
    . The botmaster role can be separated from the role of tester, or testers can be given privileges to modify the rules on their own.
   </span>
  </p>
  <p>
   <span>
    The disadvantages of using AIML are many. First of all, being a rule based system, a big set of rules need to be built and so a big fraction of time is spent analyzing the possible variation of the sentences instead of leaving it for more important tasks, such as focusing on the data available. A lot of rules are also needed to perform reduction on other rules. Those additional rules make very difficult to understand what is wrong when multiple reduction rules have been applied. Other problems come when the sentences contains complex queries. In this situation, it’s possible that more parameters need to be extracted from a single sentence, and since every pattern can contain a maximum of one wild character some tricks must be used.
   </span>
  </p>
  <p>
   <span>
    AIML is used on bots based on ALICE. Being the markup language released under GNU GPL license, a lot of open source tools exist, only requiring the botmasters to produce a set of AIML rules. Even online services exist that can be used to deploy an AIML bot in very few steps (for example pandorabots
   </span>
   <sup>
    <a href="#ftnt89" id="ftnt_ref89">
     [89]
    </a>
   </sup>
   <span>
    ).
   </span>
  </p>
  <p>
   <span>
    Also considering that AIML was born for generic chit-chat systems and the Loebner Prize, it has been used also in domain specific bots. For example, to provide answers to Frequently Asked Questions in the domain of student support in Universities
   </span>
   <sup>
    <a href="#ftnt90" id="ftnt_ref90">
     [90]
    </a>
   </sup>
   <span>
    . The information contained in the FAQ are relative to admissions and courses information. The AIML rules for managing conversations on these topics have been added to a base ALICE bot, and a change in the state transition management has been done in order to make the conversation stay on the desired topics, using weighted transitions that make the bot prefer to stay on the university domain. This system has been tested on students, establishing some evaluations on their satisfaction and also on the topic switching rate, in order to have some measures on the state transition management.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Other examples of rule-based systems are bots that manage the interaction with the user by only providing
   </span>
   <span>
    buttons
   </span>
   <span>
    . In this way, the dialogue is completely managed by the bot and the interlocutor can only decide on a finite set of choices at every turn. This is a very unnatural dialogue scheme, where the initiative belongs completely to the bot. A lot of bots today belong to this category, because in this way the dialogues are strongly reliable and the state of conversation can be tracked and managed easily without unexpected utterances. About the information retrieval, those bots usually interact with a fixed set of APIs (as most domain-specific bots do).
   </span>
  </p>
  <h4 id="h.62szn9nueahy">
   <span>
    NLU
   </span>
  </h4>
  <p>
   <span>
    [LABEL:soaNLUIntro]
   </span>
  </p>
  <p>
   <span>
    Another approach that can be used for the understanding task, instead of manually matching sentences with patterns, is to have a Natural Language Understanding engine that performs this classification in a smarter way. In this approach, instead of building rules and analyzing the structure of the sentence in order to span the different variation of sentences that should be grouped together, the objective is to have a classifier that can be trained to make this task automatically using machine learning techniques (feeding training examples, letting the internal parameters to be adjusted automatically to obtain the desired outputs).
   </span>
  </p>
  <p>
   <span>
    Natural Language Understanding however applies only to the task of understanding the sentences (turn natural language sentences into structured data), and not to the response generation. NLU is composed of two subtasks: sentence classification, also called intent detection, that finds for each input sentence a predetermined type that expresses what the user wants to communicate or ask; slot filling, also called entity extraction, that is responsible to obtain from sentences some parameters that have been previously defined in their type.
   </span>
  </p>
  <p>
   <span>
    This approach is born for domain specific bots and responds to a problem that is inborn to this kind of bots: exists a big difference between the natural language and the information with fixed structure stored in Knowledge Bases. Using Natural Language Understanding techniques, an intermediate representation can be built starting from the sentences in natural language and from this representation, composed of intent and entities, it is easier to interface with the fixed structure information that are usually reachable using some statically defined APIs (information retrieval) and also manage the state of the dialogue, recording which intents and entities have occurred (dialogue manager).
   </span>
  </p>
  <p>
   <span>
    NLU can also be used when the dialogue manager is not statically defined, but is trained to provide the correct responses and call the right APIs over a predefined set. In this solution there are not rules defining how to track the state of the conversation, because they are inferred from the training corpus. This technique for example is used by the open source company RASA
   </span>
   <sup>
    <a href="#ftnt91" id="ftnt_ref91">
     [91]
    </a>
   </sup>
   <span>
    , that provides a model that not only takes care of NLU (handling intents and entities), but also can manage a dynamic state tracking. Based on a fixed set of patterns provided by the developer and actions (that may call external APIs), the state tracker can be trained both by providing dialogue examples or by doing online learning (asking for feedback after every turn) for a quick annotation of new corpuses.
   </span>
  </p>
  <h4 id="h.b9affvl7ooaj">
   <span>
    Encoder-Generative models
   </span>
  </h4>
  <p>
   <span>
    [LABEL:soaGenerativeApproach]
   </span>
  </p>
  <p>
   <span>
    Going further with automation techniques that learn from examples, if the management of the state of the dialogue and the information retrieval and the response generation are managed without writing static rules, the approach is said to be generative. Using this approach removes the intermediate point of contact between natural language and rule based systems that is used in NLU systems. Everything is trained together in a end-to-end fashion: from the input sentences to the output sentences.
   </span>
  </p>
  <p>
   <span>
    This approach was born in the chit-chat domain, and has its strength when combined with a huge corpus of dialogues. Given the dialogues, that are sequences of sentences said by different actors, the system is trained to generate the next sentence given the current dialogue history.
   </span>
  </p>
  <p>
   <span>
    The first implementation of this strategy was trained on tweets
   </span>
   <sup>
    <a href="#ftnt92" id="ftnt_ref92">
     [92]
    </a>
   </sup>
   <span>
    , but other works have been trained on Ubuntu Dialogue Corpus
   </span>
   <sup>
    <a href="#ftnt93" id="ftnt_ref93">
     [93]
    </a>
   </sup>
   <span>
    and many other datasets.
   </span>
  </p>
  <p>
   <span>
    The technology used is the one that is used to provide translations, with some modifications. The model for the generation is trained on the corpus and learns how to generate the next sentence from the previous ones with an encoder-decoder structure. This structure, as will be seen applied for slot tagging in [REF:soaSeq2Seq]
   </span>
   <span>
    , uses LSTM cells, that can capture features of sequences adaptively using some kind of memory. The encoder network (can be seen as the understanding part) collects the useful features while the decoder, given an initial state from the encoder and given a stimulus, begins the generation of the new sentence and stops when appropriate.
   </span>
  </p>
  <p>
   <span>
    A even more complex approach is the one described in
   </span>
   <sup>
    <a href="#ftnt94" id="ftnt_ref94">
     [94]
    </a>
   </sup>
   <span>
    that uses a hierarchical RNN: the lower level is applied on words, both in the encoder and the decoder, while a coarse-grained RNN applied on the sentences is used to model the topic propagation over the turns.
   </span>
  </p>
  <p>
   <span>
    The main advantage of this kind of approach is that responses are generated completely in automatic way and thanks to LSTM cells can contain elements that were previously mentioned in the conversation. Being generative, they always produce an output, also with unexpected inputs. This can be good, but is in principle unpredictable. This unpredictability is the main disadvantage, that also reflects on the grammatical structure of the output sentences: also with huge corpus errors can occur. Another big disadvantage is the quantity of dialogues required to have good performances.
   </span>
  </p>
  <p>
   <span>
    To overcome those problems, some reinforcement learning strategies can be applied, in order to learn in a online settings also from the dialogues that occur at runtime. This is a good strategy in general, but can lead to some unwanted effects of polarity of the responses if there isn’t a supervision: the example of Microsoft Tay, that after 16 hours on Twitter was shut down because of the offensive tweets that was generating
   </span>
   <sup>
    <a href="#ftnt95" id="ftnt_ref95">
     [95]
    </a>
   </sup>
   <span>
    .
   </span>
  </p>
  <p>
   <span>
    Having a model trained on a wide variety of dialogues also rises the problem of the coherence of the responses: it’s easy that, without filtering and post-processing the responses, the bot replies in different ways to questions inherent to the same topic. For this reason a persona-based model can be built in order to consider also some more inputs to the generator of the responses, that can model some features of the speaker, such as background information and speaking style
   </span>
   <sup>
    <a href="#ftnt96" id="ftnt_ref96">
     [96]
    </a>
   </sup>
   <span>
    . Those features can be encoded into distributed embeddings and be provided to the generator both in the training and in the inference time.
   </span>
  </p>
  <p>
   <span>
    Another common problem of this approach is that it tends to generate commonplace responses, because is trained to maximize the likelihood of the output for the given input. It is like providing an average response that fits the current input. A proposed solution to reduce this effect is to use Maximum Mutual Information as objective function in the neural model. This will try “
   </span>
   <span>
    to take into account not only the dependency of responses on messages, but also the inverse, the likelihood that a message will be provided to a given response
   </span>
   <span>
    ”
   </span>
   <sup>
    <a href="#ftnt97" id="ftnt_ref97">
     [97]
    </a>
   </sup>
   <span>
    . Using this solution, a clear decrease in the proportion of generic responses can be observed on the outputs.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    This generative approach has shown good results for non-specific task dialogues, because the goal is simply to continue the dialogue and there is no need to introduce some task-specific information (KBs).
   </span>
  </p>
  <p>
   <span>
    As an example of this approach in action, Google Smart Reply applies it to generate short email responses that can selected and completed. The implementation takes in account the incoming email to generate responses that may be used to replay
   </span>
   <sup>
    <a href="#ftnt98" id="ftnt_ref98">
     [98]
    </a>
   </sup>
   <span>
    .
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    In the field of domain specific bots, usually the responses are not generated completely from sequence to sequence models. The approaches usually make use of templates, that can be chosen by rules or by a dynamic classifier. But in some way templates are always used to provide the answers to domain-specific questions.
   </span>
  </p>
  <p>
   <span>
    The dynamism can also reach higher levels on the information retrieval task, without requiring fixed APIs but allowing simple key-value data storage to be queried in natural language: this approach has been proposed by Eric and Mannings
   </span>
   <sup>
    <a href="#ftnt99" id="ftnt_ref99">
     [99]
    </a>
   </sup>
   <span>
    .
   </span>
  </p>
  <h3 id="h.axg7qx6bt0dr">
   <span>
    The challenges
   </span>
  </h3>
  <p>
   <span>
    [LABEL:soaChallenges]
   </span>
  </p>
  <p>
   <span>
    For the different contents and approaches
   </span>
   <span>
    that have been analyzed
   </span>
   <span>
    , different challenges arise with bots that should provide some kind of valuable content.
   </span>
  </p>
  <p>
   <span>
    The most important point is to classify the user sentences and to reconduct them to a set of questions that have a response by design. This challenge of
   </span>
   <span>
    understanding
   </span>
   <span>
    the user is made up of two components: one is relative to the natural language understanding of the current sentence, while the other one is relative to the contextualization of the current sentence in the environment of the dialogue. While talking, humans tend to refer to previously mentioned things, and the understanding of sentences is often difficult without placing it into its
   </span>
   <span>
    context
   </span>
   <span>
    . Some approaches to solve this problem are explained in the section [REF:soaBeyondSingle], explaining the concept of multi-turn and dialogue tracking.
   </span>
  </p>
  <p>
   <span>
    Another challenge that arises, especially on generative approaches, is the
   </span>
   <span>
    coherence
   </span>
   <span>
    of the sentences that are said. Since the training of those models is done on dialogues between multiple users with different visions and different behaviour, it may be difficult to establish a coherent personality of the bot. This issue is not present if the approach chosen makes the response generation in a static way: in this case it is sufficient to keep a coherent personality among the people writing the response templates.
   </span>
  </p>
  <p>
   <span>
    Another big issue is how to deal with
   </span>
   <span>
    unexpected questions
   </span>
   <span>
    , for which a response has not been prepared. This problem is present in rule-based approaches, in NLU-based bots and even in generative approaches.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    The next section will investigate the Natural Language techniques that are used for domain specific bots, focusing on the tasks of intent recognition [REF:soaIntent] and slot filling [REF:soaSeq2Seq] and how to manage a dialogue in a multi-turn environment [REF:soaBeyondSingle].
   </span>
  </p>
  <h2 id="h.cvjh6pimblwn">
   <span>
    Natural Language Understanding
   </span>
  </h2>
  <p>
   <span>
    [LABEL:soaNLU]
   </span>
  </p>
  <p>
   <span>
    As introduced previously, NLU is the process of turning sentences into structured information.
   </span>
   <span>
    The two tasks that involves are intent classification and slot filling. Usually those tasks are performed in statistical way, not applying a static set of rules as was done with AIML and similars. Natural Language Understanding belongs to the NL Processing family. NLP components are involved in text processing in very different ways.
   </span>
  </p>
  <p>
   <span>
    A brief introduction on the Natural Language Processing family is given, looking at the possible achievements that can be reached in this field.
   </span>
  </p>
  <p>
   <span>
    Starting from the easiest NLP component, tokenizers are part of this family. The role of a tokenizer is to split texts into tokens, that correspond to words and also punctuation (if punctuation is relevant for the current problem, or can be removed by the tokenizer).
   </span>
  </p>
  <p>
   <span>
    Other important components of NLP are Part Of Speech recognizer and Dependency Parser, that are responsible to parse (this term is also used when elaborating source code; some approaches can be taken from source code parsing, but usually the natural language grammar is more complex and ambiguous and is better analyzed by statistical parsing
   </span>
   <sup>
    <a href="#ftnt100" id="ftnt_ref100">
     [100]
    </a>
   </sup>
   <span>
    ) the sentence and individuate the grammatical roles of the words.
   </span>
  </p>
  <p>
   <span>
    Another commonly used component of NLP is the Lemmatizer: its role is to reconduct to the base form all the words that derive from others. This is used to reduce the cardinality of vocabularies and simplify some tasks, for example finding the topic of the conversation.
   </span>
  </p>
  <p>
   <span>
    Another thing that is done using NLP is Named Entity Recognition: this finds occurrences in the text of some entities that belong to some types: common types are Person, Organization, Country, Location, Date, Time, Numbers, Currencies. This component usually mixes an approach based on the POS combining them with a Knowledge Base that enumerates in some way what belongs to each entity type. The Slot Filling task is quite similar to NER, and the differences will be analyzed soon. The NER can have some complications when the talker refers to previously mentioned entities by using explicit signs (such as pronouns or definite nominals) or implicit references (such as ellipsis): this is the non trivial problem of
   </span>
   <span>
    coreference resolution
   </span>
   <span>
    , that is analyzed in [REF:soaCoreference].
   </span>
  </p>
  <p>
   <span>
    Other components are more focused on the sentence as a whole: Sentiment Analysis that tells if a sentence is positive negative or neutral, Summarization, Sentence Classification to find topic or complexity level for example. The intent detection task can be put in this last group.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    For this work we are interested mainly in two very specific tasks (intent classification and slot filling that are described soon) that usually are denoted by the Natural Language Understanding label. The other NLP tasks (syntactical analysis, POS tagging) are not main interests here, however some of them, such as sentiment analysis, can be used to detect the mood of the writer, as will be discussed in the personalization
   </span>
   <span>
    section [REF:soaPersonalization]
   </span>
   <span>
    .
   </span>
  </p>
  <h3 id="h.xsg4ur5pp4av">
   <span>
    Goals
   </span>
  </h3>
  <p>
   <span>
    As mentioned before, from the sentences NLU wants to:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Classify the intent (what the user wants, against a set of predefined ones)
    </span>
   </li>
   <li>
    <span>
     Extract the entities mentioned (we may want to care about only a specific set of entity types that are relevant to the abilities of the bot). This task is somewhere called slot filling, because the words that we want to extract may also not be entities. “The slot filling task is to search a document collection to fill in values for predefined slots (attributes) for a given entity.” Moreover, slots in a sentence can be designed to have a role (in order to be able to discriminate between multiple instances of the same type of entity). This feature is not present in the Named Entity Recognition task because the objective is only to recognize the entities, not to give them a role as a parameter in a programming language. For this reason the name of this task can be both “slot filling” and “entity extraction” with preference for the first one that gives more the idea of roles, while “named entity recognition” is quite unappropriate.
    </span>
   </li>
  </ul>
  <p>
   <span>
    For example, for the sentence “is Turin a supported city?” we may want to categorize it as a question that asks if a city is supported (this is the intent) and extract the word “Torino” that is the city the user is looking for (this is a slot that corresponds to an entity of type CITY or simply LOCATION depending on the granularity of entities defined).
   </span>
  </p>
  <p>
   <span>
    The first task corresponds to a classification of the user sentences to decide what is the intention of the user. Since a bot is usually designed to answer to different types of questions, this stage is responsible for finding the type of question. The slot filling task instead is a process that annotates some parts of the input sentences with the name of the corresponding slot. A slot can be thought as a field in an online form. While the intent represents the type of question, the slots of a sentence are values that the bot must be able to extract from the sentences because they are used as parameters for the application logic.
   </span>
  </p>
  <p>
   <span>
    For intent classification different approaches can be used and are analyzed in [REF:soaIntent]:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Key words
    </span>
   </li>
   <li>
    <span>
     Syntactic based: the input features are hand-crafted (for example verb + object are taken into account)
    </span>
   </li>
   <li>
    <span>
     Sequence modeling (see RNN)
    </span>
   </li>
  </ul>
  <p>
   <span>
    Instead for slot filling, the approaches analyzed can be:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Syntactic-based: from the structure of the sentence (usually as a tree), a model is built to observe and learn where usually a certain type of entity is
    </span>
   </li>
   <li>
    <span>
     Statistical-based: instead of syntactically parsing the sentence, a model is built without hand-crafted features, only providing input sequences and output label sequences
    </span>
   </li>
   <li>
    <span>
     Semantic-based: the values of the entities are used to detect the entities. The entity lookup and linking is a key point in this approach
    </span>
   </li>
  </ul>
  <p>
   <span>
    Once the intent and the slots have been analyzed on the current sentence, we can have some rules at the application level that can put some constraints. For example, for a certain type of question, one or more slots can be compulsory. In this case, the conversation should make the user provide the values by prompting him some questions. These types of constraint make the interaction model more complex and require an analysis over multiple user and machine turns, as will be seen in section [REF:soaBeyondSingle].
   </span>
  </p>
  <p>
   <span>
    The two tasks can be done independently, but some recent studies (like
   </span>
   <sup>
    <a href="#ftnt101" id="ftnt_ref101">
     [101]
    </a>
   </sup>
   <span>
    and
   </span>
   <sup>
    <a href="#ftnt102" id="ftnt_ref102">
     [102]
    </a>
   </sup>
   <span>
    ) have shown that there can be benefits if they are performed together.
   </span>
  </p>
  <h3 id="h.w8phjdq702eg">
   <span>
    Recurrent Neural Networks
   </span>
  </h3>
  <p>
   <span>
    This section contains a description of the different structures of neural networks that have reached the State of the Art condition and have been described in literature.
   </span>
  </p>
  <p>
   <span>
    A neural-network approach can be used for the tasks of slot filling and intent detection. The first thing that needs to be done is to point out what are the inputs and outputs of the system. Once defined the inputs and outputs, we can think of a component that implements the desired functionalities. The inputs to the network are the words contained in the sentence and the outputs are the intent label (for the intent classification task) and the slot labels (for the slot filling task).
   </span>
  </p>
  <p>
   <span>
    As slot labels, the slot names can be used directly but, in order to better handle multi-word slots, a commonly used format is the IOB, where the O indicates “other”, the B is the beginning of a slot and the I is the label associated with a word that continues the slot of the previous word. The IOB labels are prepended to the slot name.
   </span>
  </p>
  <p>
   <span>
    An example of inputs and expected outputs, showing the IOB annotation scheme, is represented in figure [REF FIG:iob]
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 242.67px;">
    <img alt="" src="https://lh6.googleusercontent.com/w7Tcrya9EF2ukC91jhoGI_JDY6JvYZr53jSY8OWvL80WKjU4_6yRSHcaYVmXpyukwY8VeKhhamFN3SlDquU0ySSBLXWn0oVcQWSFUQruDmZubAmR8GNjChSLyCeRINN-fciHYbmR" style="width: 602.00px; height: 242.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    [FIG:iob CAPTION:The Inside Out Beginning annotation scheme]
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Instead of using simple feed-forward networks, a lot of studies make use of networks with recurrent components. The core idea behind RNN is to make use of sequential information. In feed-forward networks the assumption is that each input-output pair is independent from the others. For RNN, the recurrence stays in performing the same task for every element of the sequence, making the output depend on the previous steps. This idea can be seen as the RNN having memory that keeps information from the past iterations. Specifically on the ability of remember and use in the correct way their memory, a lot of studies have been done to develop particular cells (a cell is the basic building block for RNNs).
   </span>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 616.04px; height: 241.00px;">
    <img alt="" src="https://lh5.googleusercontent.com/gLvM9DamxVaJjjtRFvOtUKhnVY1dwBvZytpuIogbrjAwXYr_kdGQs0KE_fvBRDCbsUCJjZK-_BosH8uDHq1OMwD52iLuHpsJvlYfPrwwlGKAEhN-NokMiASI_34yd55u06Draskw" style="width: 616.04px; height: 241.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    [FIG:rnnUnfold CAPTION:The temporal unfolding operation on a simple RNN structure]
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    A recurrent cell is a type of cell that takes as input also its previous output. Those types of neural networks have been designed for problems where the order of inputs matters, and the length of input sequence can vary. For example, they are commonly used with sequence of words, characters, frames in a video and their goodness stands in modeling features that belong to the sequence. Unlike feed-forward nets, that consider the fixed set of inputs to generate the outputs, the recurrent nets are applied in different timesteps to elements belonging to a sequence and, thanks to the loops of their cells, retain information from previous timesteps and the output depends on them too.
   </span>
  </p>
  <p>
   <span>
    Since the same network (and cells) are used in different timesteps, the analysis of RNN is usually performed on the unfolded version of the network (see figure [REF FIG:rnnUnfold]): the single elements are repeated different times (one for each timestep) and the looping links are now going from the element in the previous time to the next time. In this way a recurrent network is transformed into a multi-layer feedforward network, but keeping the same weights on the unfolded elements that generate from the same recurrent cell.
   </span>
  </p>
  <p>
   <span>
    The unfolding operation sometimes is also called unrolling, because of the similarity to the loop unrolling.
   </span>
   <sup>
    <a href="#ftnt103" id="ftnt_ref103">
     [103]
    </a>
   </sup>
  </p>
  <h4 id="h.2p7ylki847vn">
   <span>
    Backpropagation
   </span>
  </h4>
  <p>
   <span>
    Backpropagation is the algorithm used in neural networks to update the weights, that is used also for the training of recurrent networks, with a slight modification.
   </span>
  </p>
  <p>
   <span>
    The goal of the backpropagation training algorithm is to modify the weights of a neural network in order to minimize the error of the network outputs compared to some expected output in response to corresponding inputs. It is a supervised learning algorithm that allows the network to be corrected with regard to the specific errors made.
   </span>
  </p>
  <p>
   <span>
    The general algorithm is as follows:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Present a training input pattern and propagate it through the network to get an output
    </span>
   </li>
   <li>
    <span>
     Compare the predicted outputs to the expected outputs and calculate the error
    </span>
   </li>
   <li>
    <span>
     Calculate the derivatives of the error with respect to the network weights
    </span>
   </li>
   <li>
    <span>
     Adjust the weights to minimize the error
    </span>
   </li>
   <li>
    <span>
     Repeat with other training samples
    </span>
   </li>
  </ul>
  <p>
   <span>
    In this way the Neural Network, which is a combination of layers with tunable parameters, learns to predict the expected output of the training samples. The goodness of the network is to obtain correct outputs also for reasonably similar samples that were not seen in training time.
   </span>
  </p>
  <p>
   <span>
    With RNNs the training mode is very similar. The name of the modified algorithm is BackPropagation Through Time (BPTT)
   </span>
   <sup>
    <a href="#ftnt104" id="ftnt_ref104">
     [104]
    </a>
   </sup>
   <span>
    and is basically the standard algorithm applied on the temporally-unfolded version of the network. The only difference is that, since the layers correspond to different timesteps of the same cell, the weight updates in each instance are summed together. In other words the temporally-unfolded neural network is a deep neural network with shared weights.
   </span>
   <sup>
    <a href="#ftnt105" id="ftnt_ref105">
     [105]
    </a>
   </sup>
  </p>
  <p>
   <span>
    As can be see in figure [REF FIG:bptt], the loss function in backpropagation for each output affects the current and previous timesteps with partial derivatives. For example if we want to consider the gradient of
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7BE%7D_%7B2%7D"/>
   <span>
    with respect to
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bx%7D_%7B2%7D"/>
   <span>
    ,
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bx%7D_%7B1%7D"/>
   <span>
    and
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bx%7D_%7B0%7D"/>
   <span>
    , we can apply the chain rule:
   </span>
  </p>
  <p>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5C+%5Cfrac%7B%E2%88%82%7BE%7D_%7B2%7D%7D%7B%E2%88%82%7Bx%7D_%7B2%7D%7D%3D%5Cfrac%7B%E2%88%82%7BE%7D_%7B2%7D%7D%7B%E2%88%82%7Bs%7D_%7B2%7D%7D%5Ccdot%7B%7D%5Cfrac%7B%E2%88%82%7Bs%7D_%7B2%7D%7D%7B%E2%88%82%7Bx%7D_%7B2%7D%7D"/>
  </p>
  <p>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5C+%5Cfrac%7B%E2%88%82%7BE%7D_%7B2%7D%7D%7B%E2%88%82%7Bx%7D_%7B1%7D%7D%3D%5Cfrac%7B%E2%88%82%7BE%7D_%7B2%7D%7D%7B%E2%88%82%7Bs%7D_%7B2%7D%7D%5Ccdot%7B%7D%5Cfrac%7B%E2%88%82%7Bs%7D_%7B2%7D%7D%7B%E2%88%82%7Bs%7D_%7B1%7D%7D%5Ccdot%7B%7D%5Cfrac%7B%E2%88%82%7Bs%7D_%7B1%7D%7D%7B%E2%88%82%7Bx%7D_%7B1%7D%7D"/>
  </p>
  <p>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5C+%5Cfrac%7B%E2%88%82%7BE%7D_%7B2%7D%7D%7B%E2%88%82%7Bx%7D_%7B0%7D%7D%3D%5Cfrac%7B%E2%88%82%7BE%7D_%7B2%7D%7D%7B%E2%88%82%7Bs%7D_%7B2%7D%7D%5Ccdot%7B%7D%5Cfrac%7B%E2%88%82%7Bs%7D_%7B2%7D%7D%7B%E2%88%82%7Bs%7D_%7B1%7D%7D%5Ccdot%7B%7D%5Cfrac%7B%E2%88%82%7Bs%7D_%7B1%7D%7D%7B%E2%88%82%7Bs%7D_%7B0%7D%7D%5Ccdot%7B%7D%5Cfrac%7B%E2%88%82%7Bs%7D_%7B0%7D%7D%7B%E2%88%82%7Bx%7D_%7B0%7D%7D"/>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 298.62px;">
    <img alt="" src="https://lh6.googleusercontent.com/9JqkEozxSYOrurvm21ZlXkDd5l5UDyM5qolTZCmUnf937HXjDGR96jeKwEEq2y_Wfl3nkM1SnjQHTdug9avc-0T_gDFhX7lJLx9pR8VoR4ocXTY0agN74UmeD9nB23sKZx4Vxk_L" style="width: 602.00px; height: 298.62px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    [FIG:bptt CAPTION:The Backpropagation Through Time: how the errors are propagated]
   </span>
  </p>
  <h3 id="h.wp9j82dyzgo5">
   <span>
    Cell types
   </span>
  </h3>
  <p>
   <span>
    In this paragraph the mainly used cell types are presented. Those cells are the basic unit used for the network architectures explained successively. As a layer in feedforward NN is composed of neurons, a layer in a RNN is composed of a recurrent cell. The cell holds the matrix parameters to compute outputs and next state given the current state and current input.
   </span>
  </p>
  <h4 id="h.1oe97uu0zy2z">
   <span>
    Simple RNN
   </span>
  </h4>
  <p>
   <span>
    The simplest recurrent cell type is a block with two inputs and two outputs. As can be seen in figure [REF FIG:simpleRNN], one input is the actual input at the current timestep while the other one comes from the previous timestep (or from initialization on the first time). The two outputs of the cell are equivalent, and is simply to put emphasis on the fact that one of them will go as input to the next timestep (this corresponds to the loop in the not-unfolded representation) and the other one can be passed to the next layer or used as output after applying some other functions (usually a softmax) and/or other layers (recurrent or not).
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 483.56px; height: 276.00px;">
    <img alt="" src="https://lh3.googleusercontent.com/W991PcKAwGFbOFtO5CxE43ZyC76JCZRMrxQdZSTdaiLniVGNN-GrbTC1N_ayu6tOW2pbPn3LD_UaKt89KnJNo1iLruTBq0FeWJkizFPM6PQ8wEwSZJ7_FWx8UyJa5-IpOlcuPMQ9" style="width: 483.56px; height: 276.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    [FIG:simpleRNN CAPTION:A simple RNN with a single layer inside]
   </span>
  </p>
  <p>
   <span>
    The two inputs are concatenated and passed through a single feed-forward layer, that corresponds to a linear transformation plus a non-linear function (e.g.
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=tanh"/>
   <span>
    ,
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%CF%83"/>
   <span>
    ).
   </span>
  </p>
  <p>
   <span>
    The next state is computed as
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bh%7D_%7Bt%7D%3Dtanh%28W%5Ccdot%7B%7D%5B%7Bh%7D_%7Bt-1%7D%2C%5C+%7Bx%7D_%7Bt%7D%5D%5C+%2B%5C+b%29"/>
   <span>
    where the notation of square brackets denotes the concatenation operation, and
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=W"/>
   <span>
    and
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=b"/>
   <span>
    are respectively weights and biases.
   </span>
  </p>
  <p>
   <span>
    Since the same cell is applied many times in time, and the recurrence loop feeds back the output as inputs, there can easily be two kinds of problem due to the fact that the weight matrix coefficients are multiplied at each timestep:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Exploding gradient: if some coefficients are greater than 1, the output values can become soon very big, making the network insensible to new inputs because is in some way saturated. The solution to this problem is using some non-linear function that limits the values not to be over the value of 1
    </span>
   </li>
   <li>
    <span>
     Vanishing gradient: if some coefficients are near to 0, the network will quickly forget previous inputs and the output won’t depend on them
    </span>
   </li>
  </ul>
  <p>
   <span>
    This can also be seen in the formulation of BPTT. There are two factors that can affect the magnitude of gradients - the weights and the activation functions (or more precisely, their derivatives) that the gradient passes through.
   </span>
  </p>
  <p>
   <span>
    If either of these factors is smaller than 1, then the gradients may vanish in time; if larger than 1, then explosion might happen. For example, the
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=tanh"/>
   <span>
    derivative is smaller than 1 for all inputs except 0; sigmoid is even worse and is always ≤0.25.
   </span>
  </p>
  <p>
   <span>
    Those problem have the same origin: the simple RNN is not able to manage long-term dependencies. This problem has been analyzed in detail by Bengio, et al. (1994)
   </span>
   <sup>
    <a href="#ftnt106" id="ftnt_ref106">
     [106]
    </a>
   </sup>
   <span>
    , Hochreiter et al. (1998)
   </span>
   <sup>
    <a href="#ftnt107" id="ftnt_ref107">
     [107]
    </a>
   </sup>
   <span>
    and other types of cells have been proposed.
   </span>
  </p>
  <h4 id="h.blhs0tl2bxox">
   <span>
    LSTM
   </span>
  </h4>
  <p>
   <span>
    LSTM
   </span>
   <sup>
    <a href="#ftnt108" id="ftnt_ref108">
     [108]
    </a>
   </sup>
   <span>
    is a solution that came out in 1997 in which a more complex cell is considered. The main idea is to have some gates that decide how much of the previous cell state to keep, and how much of the current input to consider for the calculation of the current state and current output.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 556.00px; height: 286.00px;">
    <img alt="" src="https://lh6.googleusercontent.com/NXSsiRbN93_Zj9844AAPxGf9SQ6r4egP3LqkTI0JficiVMWLejwDoes9MY04ir4Nf1g6lm40iTzxDBzmS13x8bcvB1-k4_49obF2skN4Db7JIQsUveHCMz9Wd5kgUp_8yYfTZXGX" style="width: 556.00px; height: 293.18px; margin-left: 0.00px; margin-top: -3.59px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    [FIG:LSTM CAPTION:The Long Short-Term Memory cell]
   </span>
  </p>
  <p>
   <span>
    In addition to the simple RNN cell, we can see in figure [REF FIG:LSTM] that three more
   </span>
   <span>
    gates
   </span>
   <span>
    are added:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Forget gate
    </span>
    <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bf%7D_%7Bt%7D"/>
    <span>
     : decides how much of the previous hidden state to keep.
    </span>
    <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bf%7D_%7Bt%7D%3Dtanh%28%7BW%7D_%7Bf%7D%5Ccdot%7B%7D%5B%7Bh%7D_%7Bt-1%7D%2C%5C+%7Bx%7D_%7Bt%7D%5D%5C+%2B%5C+%7Bb%7D_%7Bf%7D%29"/>
   </li>
   <li>
    <span>
     Input gate
    </span>
    <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bi%7D_%7Bt%7D"/>
    <span>
     : decides how much of the current input to consider.
    </span>
    <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bi%7D_%7Bt%7D%3Dtanh%28%7BW%7D_%7Bi%7D%5Ccdot%7B%7D%5B%7Bh%7D_%7Bt-1%7D%2C%5C+%7Bx%7D_%7Bt%7D%5D%5C+%2B%5C+%7Bb%7D_%7Bi%7D%29"/>
   </li>
   <li>
    <span>
     Output gate
    </span>
    <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bo%7D_%7Bt%7D"/>
    <span>
     : decides how much of the hidden state is exposed to the output.
    </span>
    <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bo%7D_%7Bt%7D%3Dtanh%28%7BW%7D_%7Bo%7D%5Ccdot%7B%7D%5B%7Bh%7D_%7Bt-1%7D%2C%5C+%7Bx%7D_%7Bt%7D%5D%5C+%2B%5C+%7Bb%7D_%7Bo%7D%29"/>
   </li>
  </ul>
  <p>
   <span>
    All those gates are implemented with single layer feedforward networks. This type of RNN is able to manage better the long-term dependencies, at the expenses of having four times the parameters. But with sufficient training examples, the network is able to learn how to output the correct values and how to mix the different inputs.
   </span>
  </p>
  <p>
   <span>
    Of this cell exist many implementation, the most common is the basic LSTM that is shown in the picture. Many variations exists (with peephole connections, or other variations on the gates
   </span>
   <sup>
    <a href="#ftnt109" id="ftnt_ref109">
     [109]
    </a>
   </sup>
   <span>
    ).
   </span>
  </p>
  <p>
   <span>
    The gates are combined with the previous cell state
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bc%7D_%7Bt-1%7D"/>
   <span>
    and hidden state
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bh%7D_%7Bt-1%7D"/>
   <span>
    in the following way: the input gates
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bi%7D_%7Bt%7D"/>
   <span>
    are used to scale the outputs of a
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=tanh"/>
   <span>
    layer (the layer that was used in simple RNN) and produce the state update candidate
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%C4%89%7D_%7Bt%7D"/>
   <span>
    . Then the forget gates
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bf%7D_%7Bt%7D"/>
   <span>
    decide how much to keep of the previous cell state
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bc%7D_%7Bt-1%7D"/>
   <span>
    and produce the new state
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bc%7D_%7Bt%7D%3D%7Bf%7D_%7Bt%7D%5Ccdot%7B%7D%7Bc%7D_%7Bt-1%7D%2B%7Bi%7D_%7Bt%7D%5Ccdot%7B%7D%7B%C4%89%7D_%7Bt%7D"/>
   <span>
    . At the end the new hidden state is computed using the output gate:
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bh%7D_%7Bt%7D%3D%7Bo%7D_%7Bt%7D%5Ccdot%7B%7Dtanh%28%7Bc%7D_%7Bt%7D%29"/>
   <span>
    .
   </span>
  </p>
  <p>
   <span>
    The LSTM addresses the problem of vanishing gradients very specifically.
   </span>
   <span>
    In the recurrency of the LSTM the activation function is the identity function (the addition from
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bc%7D_%7Bt-1%7D"/>
   <span>
    to
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bc%7D_%7Bt%7D"/>
   <span>
    ) with a derivative of
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=1.0"/>
   <span>
    . So, the backpropagated gradient neither vanishes or explodes when passing through, but remains constant. The effective weight of the recurrency is equal to the forget gate activation. So, if the forget gate is on (activation close to
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=1.0"/>
   <span>
    ), then the gradient does not vanish. Since the forget gate activation is never greater than
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=1.0"/>
   <span>
    , the gradient can't explode either.
   </span>
  </p>
  <h4 id="h.32ny97nh7agm">
   <span>
    GRU
   </span>
  </h4>
  <p>
   <span>
    Later studies
   </span>
   <sup>
    <a href="#ftnt110" id="ftnt_ref110">
     [110]
    </a>
   </sup>
   <span>
    have proposed a new type of cell/unit GRU that has only two gates: reset gate and update gate that adaptively control how much each hidden unit remembers or forgets while processing a sequence. The hidden state and the state cell are merged together and therefore the output gate is no more required.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 557.00px; height: 383.19px;">
    <img alt="" src="https://lh5.googleusercontent.com/wkbQpuSAS9LvTk_lGhncKbtfJ8V2QoOflYDYUD673lh09O087EOdLf2pEXNJFmhc8KyvE61YqvlNRb0bg8K8CxBQnXS1bJCDUmzqlya6MJGbv5M4XvA68FAM6UKvFYwCF2iYP1Wc" style="width: 557.00px; height: 383.19px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    [FIG:GRU CAPTION:The Gated Recurrent Unit cell]
   </span>
  </p>
  <p>
   <span>
    The advantage of this type of element with respect to LSTM is that less parameters are used.
   </span>
  </p>
  <p>
   <span>
    Figure [REF FIG:GRU] shows the internal composition of the cell.
   </span>
  </p>
  <p>
   <span>
    The analysis of how the internal layers are connected can be done by taking as reference the simple RNN with only the
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=tanh"/>
   <span>
    layer. GRU firstly adds the reset gate
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Br%7D_%7Bt%7D"/>
   <span>
    that modulates how much of the previous state
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bh%7D_%7Bt-1%7D"/>
   <span>
    is passed to the basic
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=tanh"/>
   <span>
    layer. The other gate, the update one
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bi%7D_%7Bt%7D"/>
   <span>
    decides the weighting between the basic
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=tanh"/>
   <span>
    output and the previous state
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bh%7D_%7Bt-1%7D"/>
   <span>
    : if the values are close to
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=1"/>
   <span>
    , the cell works similarly as a simple RNN that suffer from long-dependencies irrelevance (fading gradients), but as the values of this gate decrease the behavior of the cell tends to keep its state unchanged.
   </span>
  </p>
  <p>
   <span>
    It is important to notice that this cell has only one state vector: no distinction between visible
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bc%7D_%7Bt%7D"/>
   <span>
    and hidden
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bh%7D_%7Bt%7D"/>
   <span>
    , the architecture is simpler.
   </span>
  </p>
  <p>
   <span>
    Being more recent, less studies have used them, but from the performance point of view, they seem to be of the same order of LSTM (like
   </span>
   <sup>
    <a href="#ftnt111" id="ftnt_ref111">
     [111]
    </a>
   </sup>
   <span>
    and
   </span>
   <sup>
    <a href="#ftnt112" id="ftnt_ref112">
     [112]
    </a>
   </sup>
   <span>
    ).
   </span>
  </p>
  <h3 id="h.kqx6twisad4j">
   <span>
    Word embeddings
   </span>
  </h3>
  <p>
   <span>
    [LABEL:soaWordEmbeddings]
   </span>
  </p>
  <p>
   <span>
    Now that the bases of recurrent networks have been described, let’s focus on a very important point: inputs. The choice of the inputs to the neural network is very important and can affect a lot its performances.
   </span>
  </p>
  <p>
   <span>
    It might seem that we already have the inputs to the Neural Network: sentences. Sentences are made up of words and we would like to use those words as inputs to the classifier network.
   </span>
  </p>
  <p>
   <span>
    But since all neural networks only work with numbers, there must be a layer at the beginning that, given the input words, transforms them in numerical form.
   </span>
  </p>
  <p>
   <span>
    The most simple and naive approach is to consider the “one-hot” vector of the words. This representation is an array with length corresponding to the length of the input dictionary and contains values that are all zeros except for the one whose index corresponds to the index of the word in the vocabulary. In other words, a dictionary is built and the representation of the i-th word in the dictionary is an array with a single non-zero value on the i-th value. This is straightforward to implement, but has some problems because highly depends on the input dictionary: the length of one-hot vectors is the same as the length of the dictionary. Any network following this representations will have a big problem: different words, with similar meanings, will be completely different so any parameter that can be learnt on one words will not be applicable to a similar word. This approach can’t work with words that are not contained in the training set.
   </span>
  </p>
  <p>
   <span>
    Other solutions may employ techniques like stemming or lemmatization, that try to reduce the vocabulary by stripping out word suffixes and reconduce words to their roots. Stemming, applies brutally some rules to remove the suffixes, while lemmatization extracts the lemma with more powerful and studied rules. However those techniques remove informations that could in some way be useful, and make human language very rich.
   </span>
  </p>
  <p>
   <span>
    A better approach is to use a representation of words that considers semantics and syntactic information. The hypothesis behind this method is the Distributional Semantics
   </span>
   <sup>
    <a href="#ftnt113" id="ftnt_ref113">
     [113]
    </a>
   </sup>
   <span>
    : words that appear in the same context (the context is there defined as the surrounding words) are considered similar, because somehow they can be exchanged the one for the other since they appear in similar contexts. With this hypothesis, each word is mapped to a dense real vector with a fixed dimension,  where the values are optimized to represent the semantical distribution of the corresponding words. Their dimension is a lot smaller than the size of the input dictionary. The word embeddings are usually pre-trained on large corpuses of unlabeled data (for example wikipedia
   </span>
   <sup>
    <a href="#ftnt114" id="ftnt_ref114">
     [114]
    </a>
   </sup>
   <span>
    or the bigger CommonCrawl
   </span>
   <sup>
    <a href="#ftnt115" id="ftnt_ref115">
     [115]
    </a>
   </sup>
   <span>
    ).
   </span>
  </p>
  <p>
   <span>
    With these vectors it’s possible to perform different things:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Find neighbors and compute the similarity of words
    </span>
   </li>
   <li>
    <span>
     Visualization on 2d plane
    </span>
    <sup>
     <a href="#ftnt116" id="ftnt_ref116">
      [116]
     </a>
    </sup>
   </li>
   <li>
    <span>
     Mathematical operations with words that represent analogies of relationships between words: “king” + “woman” - “man” = “queen”
    </span>
   </li>
  </ul>
  <p>
   <span>
    Using word embeddings as inputs to the neural network has the advantages
   </span>
   <sup>
    <a href="#ftnt117" id="ftnt_ref117">
     [117]
    </a>
   </sup>
   <span>
    :
   </span>
  </p>
  <ul>
   <li>
    <span>
     Reduced size of input arrays, no “curse of dimensionality”
    </span>
   </li>
   <li>
    <span>
     Semantic and syntactic similarities of words are considered
    </span>
   </li>
  </ul>
  <p>
   <span>
    The embeddings can be part of the model (in this case the weights of the embedding layer are trainable) or can be pre-computed on external bigger corpus. The first option is preferred when the size of the used corpus is big enough, and is thought to be comprehensive enough in terms of word coverage (no unexpected new words in prediction time). Instead when the corpus of the considered problem is not big enough to model the word distribution in terms of syntax and semantics, it’s better to use pre-trained word embeddings.
   </span>
  </p>
  <p>
   <span>
    Pre-trained word embeddings can also be fine-tuned. Fine-tuning has as main advantage to reduce the loss on the desired task, but can also have some disadvantages: if two words are “near” in the pre-trained embedding space, and only one of them is used in a neural network and fine-tuning is applied, the position in the embedding space can change and move far from the other word, not trained in the model. If the other word occurs in model testing (inference), the network will have some problems because the two words are not anymore similar.
   </span>
  </p>
  <h4 id="h.i1ebf8cnkswp">
   <span>
    The algorithms
   </span>
  </h4>
  <p>
   <span>
    There are different algorithms for producing word vectors, that fall into two families: count based and direct prediction. For the count based ones, the idea at the basis is to build a co-occurrence square matrix with the dimension of the vocabulary. Then, chosen a window size (usually between 2 and 5) the chosen corpus is processed
   </span>
   <span>
    by sliding this window over the words
   </span>
   <span>
    and the counts are collected. Once this matrix is filled up, some strategies for dimensionality reduction are applied. Usually
   </span>
   <span>
    Singular Value Decomposition
   </span>
   <sup>
    <a href="#ftnt118" id="ftnt_ref118">
     [118]
    </a>
   </sup>
   <span>
    is used, a particular matrix factorization technique based on the usage of eigenvalues and eigenvectors. Without entering in the mathematical details, we can view that as a trick to translate the co-occurrence matrix into the word vectors, keeping the idea that words with similar occurrence count correspond to similar word vectors, and therefore similar meaning due to the distributional hypothesis.
   </span>
  </p>
  <p>
   <span>
    The second family instead, direct prediction, puts the calculation of those vectors under the machine learning technique: the values for each word are used as tunable parameters that need to be optimized through a learning procedure. The problem is defined as follows: considering the same sliding window of words as the count based, for each center word the contextual words inside the window are considered, and the objective is to predict the one from the other. In detail, two different models exist: the CBOW (Continuous Bag of Words) that predicts the center word from the outside context words and the skipgram that instead predicts the context words from the central one. These two possibilities exist for the
   </span>
   <span>
    Word2Vec
   </span>
   <span>
    algorithm described in
   </span>
   <sup>
    <a href="#ftnt119" id="ftnt_ref119">
     [119]
    </a>
   </sup>
   <span>
    , and what emerges is that the first one is better on smaller datasets because uses all the surrounding words to perform a single observation smoothing the distributional informations, while the second one is able to produce more detailed predictions over large dictionaries because treats each word in the context is treated as a new observation.
   </span>
  </p>
  <p>
   <span>
    Other algorithms do a hybrid approach, mixing count-based with direct predictions. This is the case of GloVe
   </span>
   <sup>
    <a href="#ftnt120" id="ftnt_ref120">
     [120]
    </a>
   </sup>
   <span>
    . All these solutions work on the word level only. Instead
   </span>
   <span>
    some
   </span>
   <span>
    work has been done also to enrich the representations with subword features. In
   </span>
   <sup>
    <a href="#ftnt121" id="ftnt_ref121">
     [121]
    </a>
   </sup>
   <span>
    , following the idea that similar groups of letter convey similar meanings, each word is mapped to a set of
   </span>
   <span>
    n-grams
   </span>
   <span>
    and the skipgram model is changed to consider each word vector as the sum of its n-grams. In figure [REF FIG:fastText]. The learning of the embeddings therefore is not done on the words but on the groups of letters. In this way it is possible to compute word vectors also on unknown words.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 425.50px; height: 155.50px;">
    <img alt="" src="https://lh6.googleusercontent.com/wFO2Kud4HQcqPJ-p6OlSOzpIEwl6aYanZuatmt2ooN_uZoSHMdI2TgGCJfYimdIfMeJRCGcXMPTuanX1oW0WGYPGM_elfCaSLDveZVfbsjlasmFXW3NsP0iWe53Ejn2k5VSuoQS7" style="width: 425.50px; height: 155.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    [FIGURE:fastText CAPTION:An example of the n-grams derivation in FastText
   </span>
   <sup>
    <a href="#ftnt122" id="ftnt_ref122">
     [122]
    </a>
   </sup>
   <span>
    ]
   </span>
  </p>
  <p>
   <span>
    Other studies have tried a different approach to provide word vectors for words that are not present in the training corpus. For example in
   </span>
   <sup>
    <a href="#ftnt123" id="ftnt_ref123">
     [123]
    </a>
   </sup>
   <span>
    the authors try to learn subword informations on the single characters from word-level embeddings, reversing in some way the flow. from the similarity to known words or from the context the word belongs or as in fastText considering the character n-grams
   </span>
   <sup>
    <a href="#ftnt124" id="ftnt_ref124">
     [124]
    </a>
   </sup>
   <span>
    ).
   </span>
  </p>
  <h4 id="h.kss1p88ixws">
   <span>
    The vocabulary issues
   </span>
  </h4>
  <p>
   <span>
    Having good algorithms for word embeddings is a good starting point. But there may be some problems when deciding how to preprocess the corpora. What we want to consider as input features? Only the words lowercased, cleaned up from all the non alphabetical characters, or we may consider other hints from the user typing such as capital letters or punctuation?
   </span>
  </p>
  <p>
   <span>
    The choice is usually to clean deeply the inputs in order to reduce the vocabulary. For capitalization it means to lowercase everything. But the casing feature may be important in some way to help recognizing entities (think about a place name with capital letter, or an acronym uppercase). On the other side, beginning of sentences have capital letters that should be lowercased because that is not a wanted feature, it’s only required by writing rules. For this reason some invented a truecasing model
   </span>
   <sup>
    <a href="#ftnt125" id="ftnt_ref125">
     [125]
    </a>
   </sup>
   <span>
    that aims at reconstructing the correct word casing.
   </span>
  </p>
  <p>
   <span>
    The other critical point is punctuation: a decision is needed about whether to keep those signs as additional dictionary entries or simply drop them. A very critical one is the apostrophe: the words around it are usually modified, so not only they have to be considered separately, but also some reconstruction could be required (example: “we’re” as a single word
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5B%22we%27re%22%5D"/>
   <span>
    or separating by the apostrophe
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5B%22we%22%2C%5C+%22%27re%22%5D"/>
   <span>
    or reconstructing the original words
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5B%22we%22%2C%5C+%22are%22%5D"/>
   <span>
    in order to have the same entry for the verb
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%22are%22"/>
   <span>
    in the dictionary (or applying a stemmer with similar results).
   </span>
  </p>
  <p>
   <span>
    An approach based only on only alphabetical lowercased words may be good for scenarios where the users don’t have the time to write complex punctuations. But when additional features are provided (such as punctuation and word casing in text, and accents and tone in voice), it would be a pity to throw away things that may be useful for doing a better classification.
   </span>
  </p>
  <h3 id="h.c5fozo2z8lca">
   <span>
    Intent classification
   </span>
  </h3>
  <p>
   <span>
    [LABEL:soaIntent]
   </span>
  </p>
  <p>
   <span>
    Let’s talk here about approaches for the first task of NLU, intent classification, that takes as input the sentence and provides on output a label that corresponds to the intent type of the sentence. It is a multi-class classification. It takes in input a sequence of words and produces a single label, that is a trait value of the input sentence.
   </span>
  </p>
  <h4 id="h.4hmj3bjv7gq3">
   <span>
    Keyword-based
   </span>
  </h4>
  <p>
   <span>
    The first approach that can be considered is keyword-based. In this approach, for each intent type we determine a set of keywords that, if present in the current input, give a score to the selected intent type. For example a naive classifier can be built to build keyword groups from training sentences and classify accordingly to their presence by doing a “majority vote” on the words of the current sentence.
   </span>
   <sup>
    <a href="#ftnt126" id="ftnt_ref126">
     [126]
    </a>
   </sup>
   <span>
    This approach dynamically determines the keywords for each class, but is not good enough because it looks only at some words in the current input sentence.
   </span>
  </p>
  <p>
   <span>
    A better idea would be to compute a sentence-level representation that summarizes all the words and the meaning of the sentence, and from this vector do a classification on the output labels (intent types). For this sentence vector a lot of different approaches can be applied that are explored in the following paragraphs.
   </span>
  </p>
  <h4 id="h.dazxotng1x74">
   <span>
    Average of word vectors
   </span>
  </h4>
  <p>
   <span>
    First of all, given the word vectors for each word contained in the sentence, an average can be done. As shown in figure [REF FIG:intentAverage], after the average is computed over the fixed-length embedding values for the input words, a simple feed-forward layer can be used to map to the intent space, producing logits that estimate the probability to belong to a certain class (intent type).
   </span>
  </p>
  <p>
   <span>
    This strategy however is not good because it does not consider the order and the relationships between the words. The order matters a lot in natural language.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 356.00px;">
    <img alt="" src="https://lh4.googleusercontent.com/ujR0rI_tBmLLaQ7A3ahz_hW0zJt8ta_UuhYcqtDJgIXq7KyqQnT1j0Mppku6w5i9IrlwbURCz4k6rVRCcznmXGasQCsbdHBNusECbQQrcvMb-OB6I-pwoFTeFSY5BFeMewXPsO_J" style="width: 602.00px; height: 356.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    [FIG:intentAverage CAPTION:Intent classification by average of word vectors]
   </span>
  </p>
  <p>
   <span>
    Another problem of this approach is that actually every word counts the same, so if stopwords are not eliminated they can bias the output basing on irrelevant inputs.
   </span>
  </p>
  <h4 id="h.6dcfrbpjww3p">
   <span>
    RNN approach
   </span>
  </h4>
  <p>
   <span>
    To consider the order, a RNN can be used to summarize the sentence and produce a sentence representation that can be used in another layer to classify on the intent types. The output of the RNN is taken only at the end of the sequence. This approach is able to capture some more information about the sequence that consider the relative order of words. Considering both a forward and a backward RNN, the output of the summarized output is not biased towards the end of the sentence as would happen for a forward-only RNN. Figure [REF FIG:intentBidirectionalRNN] shows how the bidirectional encoding of the sentence substitutes the average operation with respect to the previous approach. The output projection layer finally performs the mapping to the intents spaces.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 444.00px;">
    <img alt="" src="https://lh6.googleusercontent.com/PD86ayCvDXjfZMP8nLVDBrF0-Hh9K_Jc_wEbSvXmtSxfhIq9JjHVl9isQL77yP2OPLrckFHBdtpCy8JTwBHqT2pp7WQAjP8UjQI3xd5qIVtj2FH00FyNS8D30fSmP5G3AGxb6YFf" style="width: 602.00px; height: 444.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    [FIG:intentBidirectionalRNN CAPTION:Intent classification by applying a bidirectional RNN]
   </span>
  </p>
  <p>
   <span>
    What makes this technique to work well is that the LSTM is very good at capturing sequence-level informations, considering with different importance the input words. The problem of stopwords is defeated and the Recurrent Network learns to discard the irrelevant words.
   </span>
  </p>
  <h4 id="h.fiazs7yxt02f">
   <span>
    RNN + attention + other stuff
   </span>
  </h4>
  <p>
   <span>
    TODO
   </span>
  </p>
  <p>
   <span>
    An attention mechanism can be used to learn a distribution over time of words that tells how much relevant is this word for the task.
   </span>
  </p>
  <p>
   <span>
    Attention used for sentiment analysis: see there
   </span>
   <span>
    <a href="https://arxiv.org/pdf/1703.03130.pdf">
     https://arxiv.org/pdf/1703.03130.pdf
    </a>
   </span>
   <span>
   </span>
  </p>
  <p>
   <span>
    Universal Sentence Representations:
   </span>
   <span>
    <a href="http://aclweb.org/anthology/D/D17/D17-1071.pdf">
     http://aclweb.org/anthology/D/D17/D17-1071.pdf
    </a>
   </span>
   <span>
   </span>
  </p>
  <h3 id="h.58yo5bxnkjji">
   <span>
    Sequence to sequence models for slot tagging
   </span>
  </h3>
  <p>
   <span>
    [LABEL:soaSeq2Seq]
   </span>
  </p>
  <p>
   <span>
    The task of slot tagging instead consists of generating an output sequence in which each element corresponds to a tag for the corresponding input word. The tag can be directly the entity type or the IOB format can be used. IOB can be useful when there is need to deal with multi-word slots, as in the example of figure [REF FIG:iob].
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    A lot of the architectures listed below have been created for the task of Neural Machine Translation. This task takes as input the sequence of words in the source language and outputs a sequence of words in another language. This is somehow similar to the task of sequence labeling, because it maps an input sequence to an output sequence, but has some differences that make it a lot different and require a different approach. The differences will be explained in details during the analysis.
   </span>
  </p>
  <p>
   <span>
    A common characteristic of them is the presence of two key elements: an encoder and a decoder. The encoder is responsible of collecting all the useful features on the input sequence. The decoder instead must generate the output sequence. The differences between the different models are on the way that the encoder provides input to the decoding stage. The general composition of such encoder-decoder approaches is shown in figure [REF FIG:encoderDecoder]
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 149.33px;">
    <img alt="" src="https://lh5.googleusercontent.com/RXTPtSc0BY8GuxGL6JrUJx39otZQGHJ5ul3qXC7N9fHlQ268VP3_07uk-_nJ0iOdR5-vbg7_sOdEi7ab61nc_HVLAm1ZQdDZVz99jzX6kztYLLidS6b4MQ_swyhE5Giaa9763GKQ" style="width: 602.00px; height: 149.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    [FIG:encoderDecoder CAPTION:The connections between Encoder and Decoder components for sequence translation]
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    A very critical point when generating the output sequence is the correlation between the IOB labels. For example a “
   </span>
   <span>
    I-ent
   </span>
   <span>
    ” is always preceded by a “
   </span>
   <span>
    B-ent
   </span>
   <span>
    ” or another “
   </span>
   <span>
    I-ent
   </span>
   <span>
    ”. Also there can be some patterns that highlight that after a certain entity it is more probable to find another one. This is called output dependency, that occurs at the decoding stage, just before the production of the outputs.
   </span>
  </p>
  <p>
   <span>
    Modeling the output dependencies can be done in different ways:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Local choice: using this approach, each decoding step produces an output that is then projected to the labels. The decision about which label to assign is done locally, performing a simple softmax operation followed by a sampling;
    </span>
   </li>
   <li>
    <span>
     Feed the previous output together with the current input to decoding timesteps: (similarly to a Jordan Network
    </span>
    <sup>
     <a href="#ftnt127" id="ftnt_ref127">
      [127]
     </a>
    </sup>
    <span>
     ) in this approach, the output is fed back as input to the next timestep for the decoder network, and the network in training learns the output dependencies;
    </span>
   </li>
   <li>
    <span>
     Linear-chain CRF: an alternative to RNN for modeling the output dependencies is using a linear-chain CRF. This can substitute the decoder network, and use the encoder to produce its input features. CRF finds the paths with higher energy in a very similar way to RNN. In other words, it finds the output sequence that is mostly probable;
    </span>
   </li>
   <li>
    <span>
     Beam search: not only a single candidate is kept for subsequent decoding timesteps, but a set with fixed size, in order to be able to perform decoding without always following the local optimum.
    </span>
   </li>
  </ul>
  <p>
   <span>
    In the following paragraphs we give a description of some sequence to sequence approaches, from the most simple ones towards the most suitable for the slot labelling task.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h4 id="h.ahu5xsb0fuvt">
   <span>
    Simple Encoder-Decoder
   </span>
  </h4>
  <p>
   <span>
    The most simple approach, illustrated in figure [REF FIG:encoderDecoderRNN] is the one where the encoder collects all the word vectors and using a RNN computes a final representation of the sentence (as in the intent classification task). This representation is passed to the decoder RNN that for each timestep produces an output word.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 209.33px;">
    <img alt="" src="https://lh5.googleusercontent.com/gjUQUxTYVWoD-UCWKQdsX47OojqszuVqe0OgH4jGmgCusObB4V2fHJ31HbXnd9879VJy7tQXzzROkuKtglL1lEkeJPsE36OGEaZ5aFjKxl1j0wHJcFj9B_ehy6F7NrSvDtfs6pLv" style="width: 602.00px; height: 209.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    [FIG:encoderDecoderRNN CAPTION:The usage of RNN for a simple Encoder-Decoder approach]
   </span>
  </p>
  <p>
   <span>
    This model has some problems. First of all, the decoding part depends only on the sentence vector c, that must be able to keep all the information on the input sequence, whose length can vary, in a fixed size. Then, the decoding steps may easily lose the relevant information from this vector, since the decoding can take a lot of decoding timesteps. But a much greater problem comes from the lack of constraint on the output sequence length: in translation between different languages, this can be a good feature, but in our task of sequence labeling we want an output sequence with fixed length. The last observation we can make is that there is no alignment model between the input and output sequences. All the output depend on all the inputs, without having different encoded information for the decoding of the output sequence.
   </span>
  </p>
  <h4 id="h.w9da5qrt3zos">
   <span>
    Encoder-decoder keeping sentence vector
   </span>
  </h4>
  <p>
   <span>
    This model comes from a study done in 2014
   </span>
   <sup>
    <a href="#ftnt128" id="ftnt_ref128">
     [128]
    </a>
   </sup>
   <span>
    on the task of neural machine translation. It is a enhanced version of the previously considered model because the sentence vector coming from the encoding stage is passed to all the decoding stages (see figure [REF FIG:encoderDecoderKeepC]).
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 209.33px;">
    <img alt="" src="https://lh4.googleusercontent.com/Y80Mda3h58C3FCr_94lU_zt31Uy5bG8lTx96g16PC9R_i9HncIN9tQMTDdJ5owv0kVlfQIpytr3ugBzQoSoEe1-OiJyvLvmmgZesH0P6iC4IfRWff-1YTgrDrl_R_V_cZ5izztZt" style="width: 602.00px; height: 209.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    [FIG:encoderDecoderKeepC CAPTION:Passing the sentence vector to all the decoding steps]
   </span>
  </p>
  <p>
   <span>
    This approach helps the decoding stages that, receiving the sentence vector directly, can perform better in their task. However some problems are not solved: the output sequence length has no constraints and there is no alignment model.
   </span>
  </p>
  <h4 id="h.u4i2e8o9kcra">
   <span>
    Encoder-decoder with aligned inputs
   </span>
  </h4>
  <p>
   <span>
    This model was not born for the task of translation between languages, but has been proposed
   </span>
   <sup>
    <a href="#ftnt129" id="ftnt_ref129">
     [129]
    </a>
   </sup>
   <span>
    specifically for the sequence labeling problem (that can be applied to slot filling, POS tagging). In this model the encoder sends some information to the decoder for each input word instead of sending a single vector at the end (see figure [REF FIG:encoderDecoderAligned]).
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 457.50px; height: 394.42px;">
    <img alt="" src="https://lh5.googleusercontent.com/Z3tVtoKzrGOHj5hZLp-dnCRiqtEWPCPxWKU3V5Ot_lF0p3fpdGupjB9y4RfjAcRLg21qLxIaczsbnqmJZGGSYh-cZNPcUxhPRJp7bDO3Lcf6qZtILIof8b3l2nyw-idAfmq1kyJ1" style="width: 457.50px; height: 394.42px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    [FIG:encoderDecoderAligned CAPTION:The Encoder-decoder with aligned inputs: each decoding step is influenced by the contextual representation of the aligned word]
   </span>
  </p>
  <p>
   <span>
    This model fixes the output sequence length to the length of the input sequence. The alignment model is fixed: the decisions in decoding are done looking at current input word in the current left+right context.
   </span>
  </p>
  <h4 id="h.g5uc72zc01b8">
   <span>
    Encoder-decoder with attention
   </span>
  </h4>
  <p>
   <span>
    The idea of attention empowers recent studies on translation. The purpose is to decide which output of the encoder are more relevant for the current decoding step dynamically. On the previous model, always the aligned encoded input is used, but for language translation this can be a limitation.
   </span>
  </p>
  <p>
   <span>
    Using the attention
   </span>
   <sup>
    <a href="#ftnt130" id="ftnt_ref130">
     [130]
    </a>
   </sup>
   <span>
    , that provides a dynamic alignment model, it’s possible to:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Determine which are the encoded inputs that are more relevant
    </span>
   </li>
   <li>
    <span>
     Use them to provide a better translation
    </span>
   </li>
  </ul>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 483.70px; height: 484.50px;">
    <img alt="" src="https://lh4.googleusercontent.com/LRTqL6_7i6Gh5nhUKsa3UWT1HgCljPck6pCIwuPCYKj03gv3EDOvp7iRo74X4pfoY8EEkkV_WQ_zpXFprfHcAHNP5Hiy83R0zzmeiPtGr5OHcskN5J-S1DTt08_On6jkM5DQ1nc-" style="width: 483.70px; height: 484.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    [FIG:encoderDecoderAttention CAPTION:The aligned Encoder-decoder with the addition of attention mechanism]
   </span>
  </p>
  <p>
   <span>
    The attention, as can be seen in figure [REF FIG:encoderDecoderAttention], adds two blocks: one is the context vectors memory, that stores the output of the bidirectional RNN at each timestep, and the other one is the attention block that is responsible to pick up the correct mix of the encoded vectors by doing a weighted sum of them and provide that to the decoder RNN.
   </span>
  </p>
  <p>
   <span>
    By looking inside at the
   </span>
   <span>
    attention
   </span>
   <span>
    mechanism, in figure [REF FIG:attention] we can see that the encoded vectors
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=e"/>
   <span>
    are used together with the previous hidden state of the decoder cell
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bh%7D_%7Bi-1%7D"/>
   <span>
    in some matricial multiplications and finally pass through a softmax. This part is responsible to learn the weights for each timestep of the decoder that represent how much relevant are the input words for the determination of the current output word. The matrices
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7BW%7D_%7B1%7D"/>
   <span>
    ,
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7BW%7D_%7B2%7D"/>
   <span>
    and
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=V"/>
   <span>
    are used together with a
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=tanh"/>
   <span>
    layer to determine dynamically those weights. This block, put inside a complex network architecture, learns all the parameters thanks to the backpropagation algorithm being end-to-end differentiable. The output
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bc%7D_%7Bi%7D"/>
   <span>
    is then computed doing a weighted sum between the
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Be%7D_%7Bj%7D"/>
   <span>
    encoded vectors.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 481.33px;">
    <img alt="" src="https://lh5.googleusercontent.com/FRrmazouaylbcVtMJjBkbjV4-tD43sJ046OfTnNY60YCimmZ25rWSLYR8h-RfQqgcia8QGFTltDD1u_tZYLGIPvZTrLOMr-UWvZGlgjOEq05EGryZUXZL45yaOdkQnNbhV1jiFpn" style="width: 602.00px; height: 481.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    [FIG:attention CAPTION:How the Attention block computes a weighted sum, learning the weights dynamically]
   </span>
  </p>
  <p>
   <span>
    With this network, used for translations, it is useful to show how the attention maps the words in the two languages. Figure [REF FIG:attentionVisualization] shows a matrix representation that says which input words were used to provide the corresponding output words.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 646.67px;">
    <img alt="Screen-Shot-2015-12-30-at-1.23.48-PM.png" src="https://lh4.googleusercontent.com/4unbZt4D8t_7wToG1hZX7rAWjSgW5s28WptmDgQXCRznBdibjM1oWCsn1PnYVNkhsryroQ2GzmmLA01H6wGmbPI86tdpRkUO9Aj2Xnv2gdmFqgc5csPzjEOYcO31DfbD2SVUx_PR" style="width: 602.00px; height: 646.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
   <span>
    [FIG:attentionVisualization CAPTION:A visualization of the attention scores in translation
   </span>
   <sup>
    <a href="#ftnt131" id="ftnt_ref131">
     [131]
    </a>
   </sup>
   <span>
    ]
   </span>
  </p>
  <p>
   <span>
    The attention model is quite advanced, and since a lot of other parameters are there in the network, a lot more of training samples must be used.
   </span>
  </p>
  <p>
   <span>
    For the task of sequence tagging it seems to be too much. The output labels depend only on the current word context, that is already given by the bidirectional encoder RNN, so the expected values for the attention distribution is that it will keep the inputs and outputs aligned. It can be kept together with the aligned model just to provide more features.
   </span>
  </p>
  <h3 id="h.2ijwzgcp6f7s">
   <span>
    Joint SLU
   </span>
  </h3>
  <p>
   <span>
    The two tasks can be combined together in one single network in different ways and with a wide variety of additional things that can be added (for example attention mechanism, output dependencies). The approaches found in literature try to use a common encoding stage and then differentiate on the decoding, one for each task. The network has to fork at some point because the shape of the outputs is different: one single label for the intent and many slot labels, considering a single input sentence.
   </span>
  </p>
  <p>
   <span>
    In
   </span>
   <sup>
    <a href="#ftnt132" id="ftnt_ref132">
     [132]
    </a>
   </sup>
   <span>
    there are two different proposed architectures: one is based on the encoder-decoder adding the intent output, while the other collapses all in one single compact structure.
   </span>
  </p>
  <p>
   <span>
    The first one, as can be seen in figure [REF FIG:jointSLUAligned]
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 349.33px;">
    <img alt="" src="https://lh5.googleusercontent.com/7FL3TzIJ2iKDvSNLKb2SIipqOeB-3mvIfCu3JMmN2KOaBBtUUsvct2R_UE-aje-SHIcJyb2fNVOr9jH7JAWbGHBTCHhxMDqtwP1nou5yn8s8LaalVOGC8vzP-VP03S5SEUE_cPiN" style="width: 602.00px; height: 349.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    [FIGURE:jointSLUAligned CAPTION:The Joint Encoder-decoder model with Aligned Inputs in
   </span>
   <sup>
    <a href="#ftnt133" id="ftnt_ref133">
     [133]
    </a>
   </sup>
   <span>
    ]
   </span>
  </p>
  <p>
   <span>
    The intent classifier is added as a branch that takes the last state of the encoder and then projects to the intent space with a single layer feedforward.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    The other network proposed in the paper is a bit different. Instead of having a separate RNN for encoder and decoder, it has a single bidirectional RNN, that in the forward direction also has the modeling of slot label dependencies. The intent classification is done on top of the bidirectional RNN output, doing a mean pooling on the states at each timestep or, if attention is enabled, by using a weighted average.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 458.00px; height: 492.00px;">
    <img alt="rnn (15).png" src="https://lh6.googleusercontent.com/YB8miahDVN3fKIuWwNHowwbCGuQaswaAdV8iAUIaDXW8MvTlQFSYEc7GNJEGgYmflnBnHsZ0hH4kJe7AYLcMKSa4sr2laDq3BnqccIazWugKDUATsSDtI3BbXMtF7_8O_tltz9UT" style="width: 458.00px; height: 492.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    TODO FIGURE jointSLUrnn
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h3 id="h.rx2vs1atz3u0">
   <span>
    Beyond the single sentence
   </span>
  </h3>
  <p>
   <span>
    [LABEL:soaBeyondSingle]
   </span>
  </p>
  <p>
   <span>
    All the previously described networks only care about the current sentence, but human natural language understanding does not limit itself on this strict form. What happens when we meet some people that are already talking about something? It may take some time to understand the topic of the discussion, we need to collect some information as the interaction goes on to get the point of the discussion. This happens because most of the sentences are not standalone, but belong to a context that goes beyond the single sentence. This feature can be seen as some kind of memory that actors in a conversation need to keep. The context is necessary to understand the dialogue at different levels:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Understand the role of some words in a sentence given some information contained in other sentences (e.g. “To the beach” is the name of a restaurant, because Mary asked John what was the name of it) → slot filling multiturn
    </span>
   </li>
   <li>
    <span>
     Understand the meaning of the current sentence (e.g. in this sentence John told Mary that the restaurant was not as good as he expected) → intent classification multiturn
    </span>
   </li>
   <li>
    <span>
     Understand the topic of the conversation (a synthesized attribute, e.g. John and Mary are talking about their last experience in a restaurant) → domain classification multiturn
    </span>
   </li>
   <li>
    <span>
     Linking the meanings of all the things that were discussed, and being able to answer to some questions (test) → dialogue state tracking, QA
    </span>
   </li>
  </ul>
  <p>
   <span>
    All those levels of understanding are challenges that are felt by the NLP community and have not a general established solution. They fall under different names:
   </span>
  </p>
  <ul>
   <li>
    <span>
     “multi-turn interactions”: specific to virtual assistants, is a name for the first level of understanding: the goal is to identify the intent and entities in a dialogue where the user is not using a single sentence to ask for information. It is the case where after the initial question of the user the
    </span>
    <span>
     agent asks back for some clarifications / parameters
    </span>
    <span>
     to refine the search. Example: “Send an email to Bob” “What would you like to write?” “Hi Bob, How are you?”. In this case the assistant should put together those sentences in a structure that says: send_email(to=”Bob”, body=”Hi Bob, How are You?”). Other common cases are
    </span>
    <span>
     user follow-up questions
    </span>
    <span>
     . Receiving some results from the agent, the user could ask for more details or to change some parameters. Example: “Find me a chinese restaurant” “Those are the results for chinese restaurants” “What about italian ones?”. In this case, the user refers to the previous query and is only changing the type of restaurant.
    </span>
   </li>
   <li>
    <span>
     “Dialogue state tracking”: mostly known because of the Dialogue State Tracking Challenge. This refers to the last level of understanding a dialogue: represent the dialogue state and updating it as the conversation keeps going on. TODO MORE ON DIALOG STATE TRACKING
    </span>
   </li>
  </ul>
  <p>
   <span>
    Different solutions exist and depend on where we want the neural network to come in contact with the application logic (what to establish as manual rules and what is inferred by the neural-network approach). Literature shows case studies where there is a strict separation between the understanding module and the management of the dialog state, but also cases where everything is put together in an end-to-end fashion in a way more independent from domain rules.
   </span>
  </p>
  <p>
   <span>
    In the first type of systems, the NLU module contains the recurrent neural network stuff and produces in output intent+entities. Once they are extracted from the dialogue, another module “dialogue state tracker” keeps trace of the conversation and applying some handwritten rules decides the flow of the conversation and provides responses back to the user.
   </span>
  </p>
  <p>
   <span>
    Instead in the end-to-end architectures, all the components are trained by dialog examples. The positive points is that there is no need to handwritten rules for the dialogue state tracker. The rules are inferred by the dialog corpus and the system learns what is the most appropriate answer to provide. Some parts are still not part of the trainable model: the module accessing the data exposes some operations via some API. The trainable model learns when to issue API calls.
   </span>
  </p>
  <h4 id="h.wttb2qhy3rtz">
   <span>
    Multi-turn SLU
   </span>
  </h4>
  <p>
   <span>
    As mentioned before, the goal of multi-turn SLU is simply to extract the intent and the entities when the user is not providing a single sentence with all the required parameters. In a second moment, could be after the agent asks back for some parameters, other sentences complete the initial one with more entities that are used to refine the search. It can be seen as an iterative filling of a fixed-structure form. For this reason, a naive approach could be simply to classify the intent on the first sentence and then collecting the parameters in a key-value fashion (the key is the name of the slot, that is asked by the agent, and the value is the answer taken “as is”).
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 326.00px; height: 136.00px;">
    <img alt="" src="https://lh4.googleusercontent.com/pdSLTBgzqcYwLkOBPsLY2t1jtzclpNaoeXCgcqmAd0Kt7sKL_InEE_qGYUBIaPL_W9o3Fw_2ULN-1wT-F8VCds2fgwq5IiMSFeUc8Y-JvHLadfvyklsobBqz_urhzYVTWXIs7fb2" style="width: 326.00px; height: 136.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    TODO FIGURE vectorial
   </span>
  </p>
  <p>
   <span>
    This approach is quite simple but has some problems: this is not natural language. Natural language dialogue has not this fixed structure, and the system must be able to receive a sentence that contains more than a single entity. Another issue is that the user may change his mind in the middle of the interaction and start a new intent. For those reasons the SLU task should be expanded to the multi-turn environment by a more complete approach.
   </span>
  </p>
  <h5 id="h.h5tesva7dlyr">
   <span>
    Contextual SLU
   </span>
  </h5>
  <p>
   <span>
    This is an approach
   </span>
   <sup>
    <a href="#ftnt134" id="ftnt_ref134">
     [134]
    </a>
   </sup>
   <span>
    that has been studied for the multi-turn SLU. The idea is to additionally incorporate contextual knowledge during slot tagging.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 324.00px;">
    <img alt="" src="https://lh3.googleusercontent.com/3ae-Y63logbUiiUI-dpTT5F-hNHhWR1mrt9UDxG0yGv3KMXf4sYqxSvu-vW99N6exGBRRT2jA3Q75x1rKs8r1KiVjAZzknJIFLr9u_f5y2DIcZlwZtTzuv4LLje9IJoCvcNzeMrU" style="width: 602.00px; height: 324.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    TODO FIGURE vectorial
   </span>
  </p>
  <p>
   <span>
    The main difference with the encoder-decoder structures that were previously explained is that also information from the previous sentences are provided to the decoding stage. Those information are computed by doing a weighted sum of all the previous encoded sentences. To compute the weights, that represent the level of attention that has to be kept to the relative sentences, an inner product is performed to obtain a similarity score between the current sentence and the old ones. The hypothesis behind this scoring is that sentences that have similar encoded representation should be considered more than the ones that are different.
   </span>
  </p>
  <p>
   <span>
    The results of this approach have been evaluated on a proprietary dataset and therefore the results cannot be compared with other studies on the field. Furthermore in the paper the intent network is not described at all. However, this study is interesting in the perspective of how the previous sentences are considered together with the current one for the slot-tagging task.
   </span>
  </p>
  <p>
   <span>
    A problem of this architecture is that the agent sentences are not considered. But the agent sentences could contain some keywords that may help to identify the slot in the decoding stage. For example if a trip requires a source and a destination, the agent asked for the source and the user answered with that, it is very relevant for the task of slot filling to know that the agent asked for source and not for the destination.
   </span>
  </p>
  <h5 id="h.hywob0csablx">
   <span>
    Considering time and roles
   </span>
  </h5>
  <p>
   <span>
    Another study
   </span>
   <sup>
    <a href="#ftnt135" id="ftnt_ref135">
     [135]
    </a>
   </sup>
   <span>
    by the same group has been done on the value of time and roles in conversation. About the time, the idea is that most recent sentences count more, and an attention score is given with values that fade out as the time is more remote. Instead for the roles, two similar networks are used, one for each role.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 258.67px;">
    <img alt="" src="https://lh4.googleusercontent.com/EW59TRmjAztp0pSFyaGysYbl0iJx0jbQuSU8095V6vqWn4lmV9Uafbllus6LLRnlBMmpPTp7FW-t9__cswTsN-kYRk39KRY0wGzH1A9-cDPXQ4dcMh-BzI2mlRrIPvflL8cMssbi" style="width: 602.00px; height: 258.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    TODO FIGURE vectorial
   </span>
  </p>
  <p>
   <span>
    We can see from this picture that the history of sentences is encoded with different networks, one for the tourist and one for the agent. Two different mechanism of attention are used: one is the content-aware, that is computed in the same way as in the previous mentioned paper with a dot product in the embedding space; the second one is the time-aware, that gives higher importance to recent sentences.
   </span>
  </p>
  <p>
   <span>
    In this case the work has been done on the DSTC4 dataset that contains human-to-human dialogues. One of them is a tourist and the other one is a guide.
   </span>
  </p>
  <h5 id="h.ohlmco880bb0">
   <span>
    Other contextual SLU
   </span>
  </h5>
  <p>
   <span>
    The following two papers show other ways to include information from previous sentences to perform some work on the current one.
   </span>
  </p>
  <ul>
   <li>
    <span>
     Contextual domain classification in SLU using RNN:
    </span>
    <span>
     <a href="https://www.clsp.jhu.edu/~puyang/papers/rnn_dom.pdf">
      https://www.clsp.jhu.edu/~puyang/papers/rnn_dom.pdf
     </a>
    </span>
    <span>
     the previous domain classification is fed together with every word input to the CNN classifier
    </span>
    <span>
    </span>
    <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 526.00px; height: 338.00px;">
     <img alt="" src="https://lh6.googleusercontent.com/eloQnhOcbiuZD4YhNqG-ZQCBbyYPpU4oTldCI_mV_cLSqBjlvPCaN7kEQ6trxUZug6fCEgUE-AWcerLK8tZKvb0ra-6ItoRCmmVHwmxG8UNn53UebG5tOpTG_pHxG74FE7lV3Nob" style="width: 526.00px; height: 338.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
    </span>
   </li>
   <li>
    <span>
     Easy contextual intent prediction and slot detection: using CRF and HMM
    </span>
    <span>
     <a href="https://pdfs.semanticscholar.org/cd2f/0c6aac6cdf98f96984ac37d106afc599d99d.pdf">
      https://pdfs.semanticscholar.org/cd2f/0c6aac6cdf98f96984ac37d106afc599d99d.pdf
     </a>
    </span>
    <span>
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    --
   </span>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 519.00px; height: 418.00px;">
    <img alt="" src="https://lh3.googleusercontent.com/4yGWJAgkDoSB0Z9WGTBAQHq03HT6Zhk6-fj9HZ_kb_B2gpCbEgtc-PljtmEjTRRKcOCxVPMB-BnyCnncjG9lgjanV1yNCXq1g3oa8BJTD1atUXcrEP1VQTCgflTw3Dns4_8XlZ3R" style="width: 519.00px; height: 418.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 527.00px; height: 458.00px;">
    <img alt="" src="https://lh5.googleusercontent.com/M16aOfNZkvZB3swYsEkvEdc-XQpxveBoV5gex0aJmKsjxfvtjzwqezLfw34Qg5mV9DC6H9TpW5zrKgwv8g3gHrBN8Cw0GqkHtCwcMOVKWqQvtzmVJMGPFgxmvu9s6DXpWve0Jcmq" style="width: 527.00px; height: 458.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <h4 id="h.aqyuoy61n2un">
   <span>
    Coreference resolution
   </span>
  </h4>
  <p>
   <span>
    [LABEL:soaCoreference]
   </span>
  </p>
  <p>
   <span>
    The problem of coreference resolution is very big in NLP
   </span>
  </p>
  <p>
   <span>
    TODO this part is for entity coreference resolution.
   </span>
  </p>
  <p>
   <span>
    <a href="https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30">
     https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30
    </a>
   </span>
   <span>
    has different papers cited.
   </span>
  </p>
  <p>
   <span>
    Paper
   </span>
   <span>
    <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43407.pdf">
     https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43407.pdf
    </a>
   </span>
   <span>
   </span>
  </p>
  <p>
   <span>
    Demo
   </span>
   <span>
    <a href="https://huggingface.co/coref/">
     https://huggingface.co/coref/
    </a>
   </span>
   <span>
   </span>
  </p>
  <p>
   <span>
    Source
   </span>
   <span>
    <a href="https://github.com/huggingface/neuralcoref">
     https://github.com/huggingface/neuralcoref
    </a>
   </span>
   <span>
   </span>
  </p>
  <h4 id="h.13sbtqu88z5j">
   <span>
    Dialogue tracking systems
   </span>
  </h4>
  <p>
   <span>
    Explain something about DSTC.
   </span>
  </p>
  <p>
   <span>
    The role of memory in interaction
   </span>
  </p>
  <p>
   <span>
    Get machines to reason, remember things, turn text and facts into operational knowledge.
   </span>
  </p>
  <p>
   <span>
    Memory-augmented networks:
   </span>
  </p>
  <p>
   <span>
    Recurrent neural networks that are augmented by memory that is a separate module but can also be trained.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 539.00px; height: 71.00px;">
    <img alt="" src="https://lh6.googleusercontent.com/2h2uKHN6Fa-2Bd3rLpruXzHvz1-DZ-aFoAR4lU0EFgzPrCJlIc5UN208NHksf1FZs1ORJSiQMjEgXgEmXCE5A-J1lu5Ddl0ajBwLxQTJ6csBzuLHtnSZpJcxS4_0_Wkoxe7TQ3Sx" style="width: 539.00px; height: 71.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    Store each turn in memory
   </span>
   <sup>
    <a href="#ftnt136" id="ftnt_ref136">
     [136]
    </a>
   </sup>
   <span>
    . Each turn is composed of a triple: subject+verb+object
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    The main goal for those systems is to keep track of a dialogue in terms of what is being said. The architecture makes use of some memory, that is a kind of “soft” hash table. It stores key-values pairs. The output of a lookup is a weighted sum of the values, where the weights are attention weights derived by the training procedure.
   </span>
  </p>
  <h4 id="h.5h19s2dnfiyp">
   <span>
    End to End Goal-oriented dialog
   </span>
  </h4>
  <p>
   <span>
    As mentioned before, this approach wants to overcome the need of domain-specific handcrafting in favour of a more generic system that can fastly be used for new domains. This approach has its origins in another type of setting: general chit-chat dialogs. From corpuses of discussions (forum threads / movie conversations) the system is trained in a end-to-end fashion to predict the next sentence. The porting from this scenario to goal-oriented ones has some issues, that generate from the fact that the conversation is intrinsically different in the purpose: the system is interrogated by the user and has to provide some information back. Beyond keeping the conversation smooth, the agent has to ask questions to the user to have a better formulation of the request, query the Knowledge Bases, interpreting the results and provide them to the user in the correct shape in order to complete a transaction.
   </span>
  </p>
  <p>
   <span>
    In this type of system the prediction are both the agent responses and the API calls.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Using this approach, not only the understanding part is turned from handcrafted rules to an
   </span>
   <span>
   </span>
   <span>
    optimization problem, but also the replying part. The API to call are determined without using fixed rules, but with a model that is determined by the training corpus. The same applies to the response generation: there aren’t fixed rules deciding what to say in responses.
   </span>
  </p>
  <p>
   <span>
    TODO continue
   </span>
  </p>
  <p>
   <span>
    Learning end-to-end goal-oriented dialog
   </span>
  </p>
  <p>
   <span>
    Paper
   </span>
   <span>
    <a href="https://arxiv.org/pdf/1605.07683.pdf">
     https://arxiv.org/pdf/1605.07683.pdf
    </a>
   </span>
   <span>
   </span>
  </p>
  <p>
   <span>
    Implementation
   </span>
   <span>
    <a href="https://github.com/vyraun/chatbot-MemN2N-tensorflow">
     https://github.com/vyraun/chatbot-MemN2N-tensorflow
    </a>
   </span>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    This system needs a fixed set of API to navigate the KB. Instead at Stanford they experimented with a data exploration that can be explored as a set of key-value pairs. In this way the system, trained in an end2end fashion will be able to explore the KB in a more dynamic way, in a content-aware way with structured data
   </span>
   <sup>
    <a href="#ftnt137" id="ftnt_ref137">
     [137]
    </a>
   </sup>
   <span>
    .
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Those kind of system make use of the memory, as the dialogue tracking systems, but with some differences. While the DST are only tracking the state of the dialogue to provide an answer at the end, that corresponds to a value in the memory, those systems are linked to a KB and can issue some API calls or simply give back the next sentence.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Reinforcement learning techniques to avoid lack of train data
   </span>
  </p>
  <p>
   <span>
    TODO
   </span>
  </p>
  <p>
   <span>
    Quick reinforcement learning
   </span>
   <span>
    <a href="https://medium.com/rasa-blog/a-new-approach-to-conversational-software-2e64a5d05f2a">
     https://medium.com/rasa-blog/a-new-approach-to-conversational-software-2e64a5d05f2a
    </a>
   </span>
   <span>
    done with interactive learning mode, providing feedback on each step to quickly reinforce the model for calling actions.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h3 id="h.iqkeiq1d8ogq">
   <span>
    NLU as a service
   </span>
  </h3>
  <p>
   <span>
    As of today, there are a lot of platforms that provide this as a service. Big companies have decided to invest on it, to be the ones that hold the technology. They made those topics extremely easy for the developers, that through a web interface can create dialogue flows and annotate manually the data. There is no need to know about the technology that is inside. All you need is to understand the jargon used by the platform and configure your black box.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    TODO enumerate services
   </span>
  </p>
  <p>
   <span>
    TALK about wit.ai, dialogflow, LUIS, IBM, …, kitt.ai
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Features provided:
   </span>
  </p>
  <p>
   <span>
    On the features provided by those platforms, usually only the natural language understanding is provided, with an approach that is not trained in a end-to-end way. The provided features turn the user sentences in structured data. Some platforms offer a way to force the user to provide some parameters (compulsory entities for some intents), managing the questions that the bot should ask when they are missing and call the fulfillment endpoint (the service owned by the developers of the bot, receiving the structured data) only when those parameters have been provided. The programmers are left to the role of handling the dialogue state, retrieving the information and providing a response.
   </span>
  </p>
  <p>
   <span>
    But this does not scale well when the dialogue can have lots of different states and the user is left to change the topic of conversation in every moment. In those situations the task of handcrafting rules to track the state of the conversation can become quite difficult. When something in the behaviour of the bot is not correct, the investigation of the problem is quite difficult.
   </span>
  </p>
  <p>
   <span>
    Instead if the dialogue-state-tracking and the dialogue itself is trained end-to-end, changing the behaviour is as easy as adding a new example to the training corpus.
   </span>
  </p>
  <p>
   <span>
    This kind of end-to-end dialogue is supported on the shelf by RASA, an open source tool that has two main components: the NLU and the core. The first one turns sentences into intents and entities, while the second one manages the state of the conversation and determines responses and API calls without handcrafted rules. The developers of a bot using this libraries have to define the intents, entities, the actions (that correspond to some methods) and templates (for the responses) and provide examples for training both the NLU and the Core.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Measurement of the performances:
   </span>
  </p>
  <p>
   <span>
    There have been some efforts to compare the performances of all those systems by SNIPS:
   </span>
   <span>
    <a href="https://snips.ai/content/sdk-benchmark-visualisation/">
     https://snips.ai/content/sdk-benchmark-visualisation/
    </a>
   </span>
   <span>
   </span>
   <span>
    <a href="https://medium.com/snips-ai/benchmarking-natural-language-understanding-systems-google-facebook-microsoft-and-snips-2b8ddcf9fb19">
     https://medium.com/snips-ai/benchmarking-natural-language-understanding-systems-google-facebook-microsoft-and-snips-2b8ddcf9fb19
    </a>
   </span>
   <span>
   </span>
   <span>
    <a href="https://www.slideshare.net/KonstantinSavenkov/nlu-intent-detection-benchmark-by-intento-august-2017">
     https://www.slideshare.net/KonstantinSavenkov/nlu-intent-detection-benchmark-by-intento-august-2017
    </a>
   </span>
   <span>
    that tested a lot of those platforms to compare the results on intent and slots. Emphasis also on the detection of out-of-domain samples (things that the agent was not trained for), learning curve, language support, response time. Dataset used:
   </span>
   <span>
    <a href="https://github.com/snipsco/nlu-benchmark">
     https://github.com/snipsco/nlu-benchmark
    </a>
   </span>
   <span>
   </span>
  </p>
  <p>
   <span>
    EXTEND THIS SECTION
   </span>
  </p>
  <p>
   <span>
    How to bootstrap a new NLU to a new domain with limited data? Transfer Learning, pretrained embeddings,
   </span>
  </p>
  <p>
   <span>
    <a href="https://www.researchgate.net/profile/Avraham_Shinnar/publication/321664993_Bootstrapping_Chatbots_for_Novel_Domains/links/5a29ff24a6fdccfbbf81994a/Bootstrapping-Chatbots-for-Novel-Domains.pdf">
     https://www.researchgate.net/profile/Avraham_Shinnar/publication/321664993_Bootstrapping_Chatbots_for_Novel_Domains/links/5a29ff24a6fdccfbbf81994a/Bootstrapping-Chatbots-for-Novel-Domains.pdf
    </a>
   </span>
   <span>
    explores intent+slots from swagger documentation
   </span>
  </p>
  <hr style="page-break-before:always;display:none;"/>
  <p>
   <span>
   </span>
  </p>
  <h2 id="h.jphi3c5npn8z">
   <span>
    Personalization
   </span>
  </h2>
  <p>
   <span>
    [LABEL:soaPersonalization]
   </span>
  </p>
  <p>
   <span>
    This section analyzes the state of the art of personalization, starting on how to profile users and determine their features, and considering the existing approaches for recommender systems and their problems.
   </span>
  </p>
  <h3 id="h.2del0db8mgct">
   <span>
    User features and Personality analysis
   </span>
  </h3>
  <p>
   <span>
    A personalization or recommender system in order to provide some outputs needs in input some information about the user. Those information can be provided explicitly by the user, for example by asking him to fill up a questionnaire, or can be collected implicitly, analyzing his behaviour and doing some forecasts.
   </span>
  </p>
  <p>
   <span>
    While having those features provided directly by the user is straightforward to be done, the implicit discovery is a bit more challenging. However users may not want to tell their personal details to the system to receive targeted recommendation. Is something that is not seen as good, because personal details are sensible data and no one wants to share them. Also when receiving explicit self-judgement from users, a big problem is how to trust them. For those reasons the implicit discovery of some features may help in different ways: looking at some available data and behaviour of the user, personal details can be estimated and users can be clustered together considering some dimensions that may reflect sensible data and their personality.
   </span>
  </p>
  <p>
   <span>
    With the spread of social networks a lot of data is ready to be analyzed to build big models that are able to predict personal information. The most common nowadays, Facebook, stores information of any kind about users: from age, occupation, and other personal information to others relative to interests. This is the ideal setting for training models that predict some user features based on others, contained themselves on the social network or coming from other sources. Specially on the fields of social sciences and personality analysis
   </span>
   <sup>
    <a href="#ftnt138" id="ftnt_ref138">
     [138]
    </a>
   </sup>
   <span>
    .
   </span>
  </p>
  <p>
   <span>
    For example, one of the biggest datasets to perform personality analysis is the myPersonality dataset
   </span>
   <sup>
    <a href="#ftnt139" id="ftnt_ref139">
     [139]
    </a>
   </sup>
   <span>
    . Born as an application to take psychometrics tests and allowing users to give access to their personal data on Facebook, the Cambridge researchers built managed to build a model that, thanks to 6 milions volunteers, is able to predict some information of users simply by looking at what are the Likes or by looking at some text.
   </span>
  </p>
  <p>
   <span>
    The personality is usually analyzed in five dimensions, in a model that has been widely shared, discussed and applied both in academic environments and in empirical ones. The “Big-Five Factors”, formulated by McCrae and Costa
   </span>
   <sup>
    <a href="#ftnt140" id="ftnt_ref140">
     [140]
    </a>
   </sup>
   <span>
    and their development traced by Goldberg
   </span>
   <sup>
    <a href="#ftnt141" id="ftnt_ref141">
     [141]
    </a>
   </sup>
   <span>
    , are the following:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Openness to experience
    </span>
   </li>
   <li>
    <span>
     Conscientiousness
    </span>
   </li>
   <li>
    <span>
     Extraversion
    </span>
   </li>
   <li>
    <span>
     Agreeableness
    </span>
   </li>
   <li>
    <span>
     Neuroticism
    </span>
   </li>
  </ul>
  <p>
   <span>
    TODO some explanation of them
   </span>
  </p>
  <p>
   <span>
    Having good values for those traits is not an easy task, because self-judgement and also human judgement can be easily polarized. To obtain values that are somewhat objective, different criteria can be used. YouYou et Al.
   </span>
   <sup>
    <a href="#ftnt142" id="ftnt_ref142">
     [142]
    </a>
   </sup>
   <span>
    in their study used three different criteria.
   </span>
  </p>
  <p>
   <span>
    The first one, called “self-other agreement” is based on how much an external judger agrees with self-rating. The second one is the “interjudge agreement”, that evaluates the similarity of the ratings given by two external judger. The last one is the “external validity”, that measures the prediction on life outcomes: based on observation of facts that can be verified, a comparison is done between the personality score provided by the different actors. This last criterion is not easy to use and not always applicable.
   </span>
  </p>
  <p>
   <span>
    In this paper, their objective was to measure the accuracy of personality judgement done by computers against those made by humans. The dataset they used was
   </span>
   <span>
    myPersonality
   </span>
   <span>
    , based on user likes and attached personality tests. The results they produced established that computer-based judgements are more accurate, because they can take in account a very big quantity of data using them in statistical modeling, However human judgement can capture more subtle cues that may be ignored by automated systems.
   </span>
  </p>
  <p>
   <span>
    Another study
   </span>
   <sup>
    <a href="#ftnt143" id="ftnt_ref143">
     [143]
    </a>
   </sup>
   <span>
    , also exploiting the same dataset, makes use of the Facebook profiles in order to predict some private user information, such as ethnicity, gender and age that may be unpublished. Given other available digital traces, such as user likes, the system is able to predict accurately this kind of information. For recommender systems needing those features, it is no more necessary to explicitly ask them to the user, but simply knowing what a person likes those values can be inferred.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    All those works on unwilling profilation, especially considering details that users may want to hide (such as sexual and political orientation), implies a decrease in trust of online services by people that have a bit of knowledge about it.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Other works, explore ways of profiling the users given some text they produced. Different things can be discovered, from the sentiments the user is feeling to their personality
   </span>
   <sup>
    <a href="#ftnt144" id="ftnt_ref144">
     [144]
    </a>
   </sup>
   <span>
    . The requirement is having more than few words and know the environment the text belongs to, in order to remove the environmental bias. This approach applies better on social networks that are more text-based, like Twitter. Since usually the tweets are openly available, analysis on text can be easily done.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    <a href="https://web.stanford.edu/class/cs124/lec/emo2016.pdf">
     https://web.stanford.edu/class/cs124/lec/emo2016.pdf
    </a>
   </span>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h3 id="h.kfp5i756qgiq">
   <span>
    Recommendation Approaches
   </span>
  </h3>
  <p>
   <span>
    Given some representation of the users, different approaches can be used to recommend items to the users. The major ones we are presenting here are content-based filtering and collaborative filtering.
   </span>
  </p>
  <h4 id="h.gaetitna0e6a">
   <span>
    Content-based filtering
   </span>
  </h4>
  <p>
   <span>
    The content-based approach is based on some evidences of the user tastes, usually his previous ratings to some items. Analyzing those samples, a model of the user preferences is created. The assumption is that users will like items similar to the ones that have previously satisfied him, so the recommender will find this type of items and propose them to the user.
   </span>
  </p>
  <p>
   <span>
    So
   </span>
   <span>
    the
   </span>
   <span>
    important thing
   </span>
   <span>
    s
   </span>
   <span>
   </span>
   <span>
    are two:
   </span>
   <span>
    having an history of interactions (feedbacks can be binary, discrete-value ratings or even textual) and having a representation of items in terms of features. The similarity of items is evaluated over those features.
   </span>
  </p>
  <p>
   <span>
    The model is different for each user, and for this reason this approach suffers a lot the problem of cold start [REF:soaColdStart].
   </span>
  </p>
  <p>
   <span>
    From a high level view, content-based recommender require three steps, that are handled by different components
   </span>
   <sup>
    <a href="#ftnt145" id="ftnt_ref145">
     [145]
    </a>
   </sup>
   <span>
    :
   </span>
  </p>
  <ul>
   <li>
    <span>
     Content analyzer: analyzes the items and extracts the features from it. If items don’t have already a structured form (for example textual items like webpages) this step is very important to produce those features (keywords in the example of webpages). Those features will be used by the other two components.
    </span>
   </li>
   <li>
    <span>
     Profile learner: user models are built, usually using machine learning techniques, feeding as examples the item features with the associated feedback provided by the user.
    </span>
   </li>
   <li>
    <span>
     Filtering component: the user models are used to infer expected positive ratings on new items, evaluated using some kind of similarity, like cosine similarity between the prototype vector (the one built by the profile learner) and the new item features. The results of this stage is a ranked list of new items.
    </span>
   </li>
  </ul>
  <p>
   <span>
    The advantages of those systems are many. First of all, user independence: only feedbacks from the target user are needed, no matter on the number of different users. Also transparency can easily be added, listing the features that caused the item score. Finally, those systems can work with new items that haven’t been rated by any user.
   </span>
  </p>
  <p>
   <span>
    However there are also disadvantages. Being very dependent on the content, a domain knowledge is usually required in order to extract the item features. This problem is usually addressed by strategies like semantic analysis that make use of ontologies in order to catch references to external concepts. Another disadvantage is that the items that are recommended tend to be very similar, resulting in high accuracy but low serendipity. It is very difficult to provide unexpected recommendations, out of the bubble that surrounds the user. The last problem is with new users: until an history of feedbacks is collected, the system can hardly suggest other items that will be liked by the user.
   </span>
  </p>
  <h4 id="h.iizs3geatfe">
   <span>
    Collaborative filtering
   </span>
  </h4>
  <p>
   <span>
    TODO
   </span>
  </p>
  <p>
   <span>
    See
   </span>
   <span>
    <a href="https://link.springer.com/content/pdf/10.1007%252F978-0-387-85820-3.pdf">
     https://link.springer.com/content/pdf/10.1007%2F978-0-387-85820-3.pdf
    </a>
   </span>
   <span>
   </span>
  </p>
  <p>
   <span>
    What user will like based on similarities of other users. Doesn’t require “understanding” the items. Can be done using user similarity and/or items similarity.
   </span>
  </p>
  <p>
   <span>
    Assumption: people who agreed in the past will agree in the future
   </span>
  </p>
  <h4 id="h.h8jkc5d3q65k">
   <span>
    Hybrid
   </span>
  </h4>
  <p>
   <span>
    TODO
   </span>
  </p>
  <h3 id="h.j72i7ypyjhwq">
   <span>
    Cold start
   </span>
  </h3>
  <p>
   <span>
    [LABEL:soaColdStart]
   </span>
  </p>
  <p>
   <span>
    Another possible approach, without using the facebook data, would be to use a bootstrap in the
   </span>
   <span>
    cold start
   </span>
   <span>
    phase: a few questions to build a first model of the user. Then more information can be
   </span>
   <span>
    extracted as the conversation flows.
   </span>
  </p>
  <p>
   <span>
    An explicit preference elicitation at the beginning, not too long, that then becomes implicit.
   </span>
  </p>
  <p>
   <span>
    Other systems apply this procedure (e.g. netflix)
   </span>
  </p>
  <p>
   <span>
    TODO
   </span>
  </p>
  <p>
   <span>
    “Facing the cold start problem in recommender systems”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: find a good solution for the user-side cold start
    </span>
   </li>
   <li>
    <span>
     Other approaches to the problem:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Content-based: requires ratings by user. Cold start problem
    </span>
   </li>
   <li>
    <span>
     Collaborative-filtering: requires other users ratings
    </span>
   </li>
   <li>
    <span>
     Explicit interview to new user about items (adapting new questions to the answers provided)
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Implementation: three phases (Collaborative-filtering)
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Classification of the new user in a specific group (based on demographic data): using C4.5 algorithm (decision tree) and Naive Bayes
    </span>
   </li>
   <li>
    <span>
     Find “neighbours” of the new user inside the group: weighted average of demographic data
    </span>
   </li>
   <li>
    <span>
     Calculation of outcome: prediction techniques
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Dealing with the new user cold-start problem in recommender systems: A comparative review”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: compare existing algorithms (collaborative filtering) on the cold-start problem. Types of systems:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Uses additional data sources (e.g. demographic data): a limitation of these systems is that sometimes data is not available (because user did not associate social profile)
    </span>
   </li>
   <li>
    <span>
     Selects a group of analogous users (without additional data sources): construct a decision tree where nodes are questions. NHSM also takes into account the global preference of user behaviors, using three factors of similarity: Proximity (how much two ratings are near), Significance (how much distant from the median) and Singularity (how the two ratings are different from others). Limitations: how to choose the optimal number of groups and splitting criteria. Must have some rating from new user (bootstrap)
    </span>
   </li>
   <li>
    <span>
     Hybrid methods:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     FARAMS: using multiple approaches (fuzzy sets and association rules).
    </span>
   </li>
   <li>
    <span>
     HU-FCF: combines analysis on demographic data (fuzzy similarity matrix) and on rating data (hard similarity matrix)
    </span>
   </li>
   <li>
    <span>
     Limitations: irrelevant users are still included in the computation of similarities
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Results: NHSM is the best one, both for accuracy and for computational time
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Learning multiple-question decision trees for cold-start recommendation”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: find a solution to the cold-start recommendation problem that maximizes accuracy and minimizes user efforts. Problem of classic bootstrap is that user usually does not know items in the first interactions. The idea is to build a tree with multiple questions at each node.
    </span>
   </li>
   <li>
    <span>
     Results:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Accuracy is better than single-question based system (because for each node/page more information are extracted) and also better than linear-combination of multiple trees
    </span>
   </li>
   <li>
    <span>
     Time increment to answer more questions per screen is sublinear
    </span>
   </li>
   <li>
    <span>
     Time difference between rating scale and binary answer doesn’t have strong dependency on the number of questions
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Matrix Factorization Techniques for Recommender Systems”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Collaborative filtering Approaches:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Neighborhood methods:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     User-oriented: find similar users to the target user, and from them find the items that they like
    </span>
   </li>
   <li>
    <span>
     Product-oriented: find items similar to the ones liked by the target user
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Latent factor models: characterize items and users on some factors (user/item vector) inferred from rating patterns (explicit or implicit feedback). Predicted rating is the dot product between the two vectors
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Learning algorithms for extracting the factor vectors:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Stochastic gradient descent: easier and faster
    </span>
   </li>
   <li>
    <span>
     Alternating least squares: can be parallelized, better on sparse data
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Temporal dynamics: the model should be dynamic because some terms vary over time (item biases, user biases and user preferences)
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <h3 id="h.zh2zox2uk29p">
   <span>
    Proactive experience
   </span>
  </h3>
  <p>
   <span>
    TODO
   </span>
  </p>
  <p>
   <span>
    One step beyond personalization on demand can be done with proactive experiences. Those kinds of experiences can suggest actions and reminders smartly, providing intelligent support.
   </span>
  </p>
  <p>
   <span>
    <a href="http://www.kdd.org/kdd2016/papers/files/adf0295-sunA.pdf">
     http://www.kdd.org/kdd2016/papers/files/adf0295-sunA.pdf
    </a>
   </span>
   <span>
    get things done before you ask. “utilizes collaborative capabilities among users, and learns, for each user, a personalized dynamic system that effectively models the sequential correlation among contextual signals and intent”
   </span>
  </p>
  <hr style="page-break-before:always;display:none;"/>
  <p>
   <span>
   </span>
  </p>
  <h1 id="h.yhzrqys59zd3">
   <span>
    Approach
   </span>
  </h1>
  <p>
   <span>
    [LABEL:approach]
   </span>
  </p>
  <p>
   <span>
    This chapter describes the design and the approach that has been chosen in order to create a domain specific bot. Some parts of it directly address the target bot (such as the scenario in [REF:approachScenarios] or the information retrieval in [REF:approachIR]), others are more general (such as the choices for NLU in [REF:approachNLU] and personalization in [REF:approachPersonalization]), while others are more under a systemic point of view (like in [REF:approachModel]).
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    For this reason, an introduction of the target bot is introduced here. The domain is the urban mobility: the bot goal is to provide a natural language interface towards the bike sharing service. Bike sharing is one of the projects for sustainable mobility, that tries to address transportation replacement especially for the so called “last mile”: cover more capillary with a public form of transport areas or trips that would be uncomfortable without using a private vehicle. With respect to car sharing, bikes have much little impact on the environment and are 100%
   </span>
   <span>
    green
   </span>
   <span>
    .
   </span>
  </p>
  <p>
   <span>
    Since there is no agreement with the bike sharing providers, the information is only available in read mode, and only from public sources. For this reason the main things a user can do with the bot is to ask for information on the station and availability of bikes. No account linking is possible and users can’t unlock bikes using the bot. The main idea is to provide useful information to the users in a spatial context-aware setting. To provide this, the bot should analyze the area relative to the information and provide some suggestions to the user.
   </span>
  </p>
  <p>
   <span>
    A required characteristic for the bot is to support the Italian language too: the agent, being developed in Torino, is supposed to be able to understand not only the english language, but also the italian one. English may be considered the international language, but the support to local languages is essential. For the sentences of the agent, the solution is simply to have both italian and english template responses. Instead for the language understanding part, as will be seen especially for the
   </span>
   <span>
    Word Embeddings in [REF:approachWv]
   </span>
   <span>
    , it requires having the requested models for both the languages.
   </span>
  </p>
  <p>
   <span>
    It is important to notice that not all the decisions discussed in this chapter have been implemented. By comparing with the next chapter [REF:implementation] it can be seen that only the language understanding and the bike retrieval information have been brought to sperimental environment.
   </span>
  </p>
  <h2 id="h.fs7v46jijgtn">
   <span>
    Scenarios
   </span>
  </h2>
  <p>
   <span>
    [LABEL:approachScenarios]
   </span>
  </p>
  <p>
   <span>
    We explain there the main scenarios that we want to face with this conversational agent. The main goal is to provide an easy-to-use interface that feels more natural for the user by allowing the interrogation in natural language.
   </span>
  </p>
  <h3 id="h.5b37239td2lm">
   <span>
    Search for bike stations information
   </span>
  </h3>
  <p>
   <span>
    The main scenario is the one where a user asks for information about bikes and available parking slots. This can be expressed by a search of one station given some parameters or by a search that includes more stations.
   </span>
  </p>
  <p>
   <span>
    For the first case, the user may want to find an available bike or a free parking slot given the current user position or another location. Instead for the second case, the user may want to ask for direction between two points: this involves finding a bike near the source and finding a place where to leave it near the destination.
   </span>
  </p>
  <p>
   <span>
    The system, given simple queries expressed in natural language, should understand the request and extract the necessary parameters. The retrieval of information should include the interrogation of the bike sharing system or using cached versions, with the aim to find a path for the user that follows the constraints specified by the user and by the bike availability. Other sources of information that may be consulted are meteo information (to provide alerts for the given location) and routing information in order to return a path that can be used by bikes.
   </span>
  </p>
  <p>
   <span>
    The history of conversation is stored in a way to find temporal and spatial pattern for the user, in order to build a model that can be used for personalizing his experience for example by providing places suggestions, as will be seen in [REF:approachPersonalization]
   </span>
   <span>
    .
   </span>
  </p>
  <p>
   <span>
    If there is a place that can be suggested, with sufficient confidence of the recommending systems (telling that the suggestion is appropriate and relevant), additional information about it can be included in the response that is generated.
   </span>
  </p>
  <p>
   <span>
    Such response should have the form of natural language, with the possibility to include visual contents such as pictures or links to more detailed information about the results.
   </span>
  </p>
  <h3 id="h.rca3waw30vmb">
   <span>
    Search for supported cities
   </span>
  </h3>
  <p>
   <span>
    Another scenario is when a user wants to understand the availability of the information for a specific city. In this case the request, expressed in natural language, will contain the name of the city. The system extracts it and finds if any bike sharing providers have support in it. If the city is not supported, information about which nearby locations are supported can be provided in the response.
   </span>
  </p>
  <h3 id="h.4andgrew3fog">
   <span>
    Small talk, refining the user model
   </span>
  </h3>
  <p>
   <span>
    The last scenarios is the one of small talk: all the sentences that aren’t necessary to provide information about bikes, but are necessary to handle very basic chit-chat dialogues.
   </span>
  </p>
  <p>
   <span>
    The user may use greetings, generic responses like yes/no, questions about the bot and other sentences that can be easily replied to, just to make it seem more natural.
   </span>
  </p>
  <p>
   <span>
    Knowing that users expectations can be easily destroyed with a “I don’t understand” some answers have been added only to mitigate a very big problem that still is present and could only be solved with generative approaches.
   </span>
  </p>
  <p>
   <span>
    Having some dialogues of this genre with the user could help understanding more about him, and this kind of information could be useful for refining the personalization model. By having a mixed-initiative interaction instead of letting the system to reply only to the questions, the bot may also ask questions to the user about his interests or explicitly asking for some feedback. In this way the personalization would receive a boost.
   </span>
  </p>
  <h3 id="h.v3k960rc81yj">
   <span>
    Proactive messages
   </span>
  </h3>
  <p>
   <span>
    Extending the mixed-initiative interaction out of the single dialogue, the bot may be able to actively begin the conversation with the user after some time of inactivity (not possible to start conversations with new users of course). This can be seen as breaking the idea of the bot as a service and for this reason should be only done if there is an advantage for the user. The advantage can exist in two different situations.
   </span>
  </p>
  <p>
   <span>
    The first one is when, after having provided some kind of information to the user (e.g. a bike is available in a specific station) and before the user arrives there, the situation changes (e.g. the bike is no more available or the meteo is getting worse). In this case the bot could actively send a message to the user informing about the change.
   </span>
  </p>
  <p>
   <span>
    The second situation is when a pattern in the behaviour has been observed (e.g. the user always searches for a bike at 8am in a fixed location). The system (some minutes before the forecasted event) can send an unsolicited message to the user informing him about the bike availability.
   </span>
  </p>
  <p>
   <span>
    Those messages must be in some way controllable from the user. The suggested way of making them available is to test them once on the user and then getting the feedback: if the user reacts positively (measured by explicit response or interacting with the generated content) the feature can be kept on, otherwise the system will remember not to use the feature with the current user.
   </span>
  </p>
  <h2 id="h.pnr09fugspju">
   <span>
    High level model
   </span>
  </h2>
  <p>
   <span>
    [LABEL:approachModel]
   </span>
  </p>
  <p>
   <span>
    Given the main scenarios and given the fact that no publicly available corpus exists for the selected domain, the choice of the approach couldn’t be in favour of an end-to-end system with dynamic response generation, but towards a NLU-based understanding. About the Information Retrieval, the data comes from external fixed set of APIs so it has been chosen to keep this approach instead of turning all the information in explorable graph of connected entities. These choices, together with the delineed scenarios, require the presence and cooperation of different components.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 233.33px;">
    <img alt="" src="https://lh5.googleusercontent.com/BUJVgrPNZZ7u_BrhRzBnNrfHUDxNBZ8sAYyE0_xtMkUWCoTQzAoSsaDQnSZBkx2T_mnFSPM96qRDH2KibeYZoVX1OPYAvJu7BXH0wSklIa22Y4-_aRhNtlB4wTXms8r8lz6lrM5t" style="width: 602.00px; height: 233.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    First of all, one to interface with the different messaging platforms. This is responsible to make messages arrive to the “brain” of the bot and to deliver responses back. Something about this component will be told in the
   </span>
   <span>
    <a href="#h.t6vk6kn3atxd">
     next paragraph
    </a>
   </span>
   <span>
    and in the
   </span>
   <span>
    <a href="#h.8zj60ofw3oog">
     implementation chapter
    </a>
   </span>
   <span>
    . Then another component will do the NLU, providing intents and resolved entities. Passing through the dialogue manager, that contains the rules for managing the conversation, the flow will reach the personalization module that will collect episodic knowledge and together with the contextual knowledge will provide recommendations. The responses will be generated by combining the results and some templates will be filled.
   </span>
  </p>
  <p>
   <span>
    The main sources of information (Knowledge Bases) can be grouped together in two types: the Contextual and Episodic.
   </span>
  </p>
  <p>
   <span>
    To the first group belong the providers of information, that are used in read only mode, inherent to bike sharing, places, meteo. The data is provided by external web API, and may be cached locally for performance improvements. The Information Retrieval module is responsible to manage them providing a higher level API that can be easily used by the dialogue manager.
   </span>
  </p>
  <p>
   <span>
    Instead the Episodic Knowledge contains records captured from the interaction with the users. Aggregated information can be computed and put in the user model, that collects the user features that can be used for the personalization.
   </span>
  </p>
  <h3 id="h.uln4bayki92m">
   <span>
    Omnichannel Support
   </span>
  </h3>
  <p>
   <span>
    [LABEL:approachOmnichannel]
   </span>
  </p>
  <p>
   <span>
    Chatbots live on messaging platforms. Nowadays there are so many different messaging platforms available and every one of them has different features. There are even some of them that don’t allow bot users. For example, the most widely used chat platform WhatsApp strictly forbids non-human accounts, punishing with permanent banning.
   </span>
  </p>
  <p>
   <span>
    Starting with one of the first platform to support bots with official API, Telegram Messenger is surely the easiest platform to create bots on, allowing them since June 2015. Bot accounts can easily be created in few seconds with the only principle that the username should end with “bot” in order to be recognizable. User can interact with bots using text, voice, buttons of different kind, images (any kind of file can be sent) and including them also in chat groups. On mobile devices users can also send their location. This platform is the most friendly for bots, but is not widely used by non-geeks.
   </span>
  </p>
  <p>
   <span>
    A very commonly used platform is Facebook Messenger, that opened to bots in April 2016. This platform is widely spread because is part of Facebook, and is available for every devices, also from web browsers. On Facebook Messengers the features are text/voice exchange, buttons of different kind, images and even attaching the user location. Facebook Bots have their Facebook Page and a few configuration steps need to be done to be able to set up a new bot. Other chat platforms that allow bots are Skype, Slack and Kik.
   </span>
  </p>
  <p>
   <span>
    Other interesting communication channels are arising with expandible virtual assistants, that focus more on the use of the voice. Expandible in the sense that developers can develop some abilities for some domains and users are allowed to select them and add to the behaviour of the virtual assistant. We are talking about Alexa Skills and Cortana Skills. For these platforms the interaction with the user is quite different because instead of communicating directly with the user, the virtual assistant manages the conversation.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    There are so many different platforms that it would be quite restricted to focus only on a single one. Furthermore the APIs change a lot between them, and even between different versions of the same platform.
   </span>
  </p>
  <p>
   <span>
    Since all the details of the API are only an implementation detail, that will be fastly covered in the implementation chapter, here we will give only the motivation that lead the design of the system to be split into two parts: one will manage the channels and communication from and towards them, while the other will handle the important conversation stages (comprehension with NLU, dialogue state management, information retrieval, personalization). The message exchange between the two parts is done using a neutral representation, not dependent on any of the destination platforms.
   </span>
  </p>
  <p>
   <span>
    This is what is intended with the term Omnichannel Support: the core of the chatbot can be put in communication with any messaging platforms, given that the message-proxy component (the part of the two that depends on the specific platform) is correctly configured. This component can make use of one of the many available solutions to manage different channels: Microsoft Bot Framework, Recast.AI Bot Connector, or start from scratch the implementation of the different endpoints. As will be seen in the implementation chapter, having this component has many advantages.
   </span>
  </p>
  <h2 id="h.histkg36tof5">
   <span>
    NLU
   </span>
  </h2>
  <p>
   <span>
    [LABEL:approachNLU]
   </span>
  </p>
  <p>
   <span>
    Before describing the NLU module, it is important to remember the approach that has been chosen: it is NLU based, not generative. For this reason the dialogue management is still done using rules and the responses are provided by filling some templates.
   </span>
  </p>
  <p>
   <span>
    Without having an existing corpus to train the dialogues in an end-to-end fashion, this is the approach that best fits the situation. For example the fact that “to search for a bike you should provide a search criterion or the last known position should be known and recent (2hr or ask for confirmation)” is not manageable from a data-only focused approach. Inside the domain it is better to provide static rules that are handwritten in some configuration files. Although there are several studies on end-to-end goal-oriented dialog, for the implementation of this prototype a rule-based logic has been chosen. The neural network are only applied in the NLU module that is responsible to extract intent and entities from the current dialog, eventually using the multi-turn environment only to better identify intents and slots.
   </span>
  </p>
  <h3 id="h.c061oqibyojs">
   <span>
    Intent and slot types
   </span>
  </h3>
  <p>
   <span>
    [LABEL:approachTypes]
   </span>
  </p>
  <p>
   <span>
    From an analysis of the scenarios, an hypothesis of the common user intents has been done, and from them also the slot types have been modeled. Intent and slot types have been successively refined in successive iterations by observing the transcript of some test users.
   </span>
  </p>
  <p>
   <span>
    The results of some intents can be seen in the table [REF TABLE:nluTypes].
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <a id="t.5e682606462ad8f12660676dff54d11b868a73d9">
  </a>
  <a id="t.1">
  </a>
  <table>
   <tbody>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Intent type
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Example
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Slots
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        search_bike
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Find me a bike near Central Park
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        LOCATION(‘Central Park’)
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        search_slot
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Where can I leave the bike near Big Ben
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        LOCATION(‘Big Ben’)
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        plan_trip
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        I want to go from Piazza Castello to Porta Susa
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        FROM.LOCATION(‘Piazza Castello’) TO.LOCATION(‘Porta Susa’)
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        set_location
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        I am in Rue de France, Nice
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        LOCATION(‘Rue de France, Nice’)
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        greet
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Hi there!
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        info
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Who are you?
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
       </span>
      </p>
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   <span>
    [TABLE:nluTypes CAPTION:Intent types and examples]
   </span>
  </p>
  <p>
   <span>
    Other intent types have been designed:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Get information about a station
    </span>
   </li>
   <li>
    <span>
     Find closest station to a certain point (not limiting the search to available bikes/slots)
    </span>
   </li>
   <li>
    <span>
     Search other information (restaurant, cafe, …)
    </span>
   </li>
   <li>
    <span>
     Feedback (on recommendation/place/system)
    </span>
   </li>
   <li>
    <span>
     Setting preferences:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     User favorite places (role → place)
    </span>
   </li>
   <li>
    <span>
     Enable/disable unsolicited messages
    </span>
   </li>
   <li>
    <span>
     Customization
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    On the entities some work needs to be done by the entity resolver, in order to recognize some values (such as home/work) and to disambiguate in a way to provide the best result in the current location context.
   </span>
  </p>
  <p>
   <span>
    Entity resolver
   </span>
  </p>
  <ol start="1">
   <li>
    <span>
     from words to structured objects, e.g. place name → lat,lng,full_name,... → OK
    </span>
   </li>
   <li>
    <span>
     Disambiguation when multiple results → NO
    </span>
   </li>
   <li>
    <span>
     Contextualization to current city → NO, the city can change
    </span>
   </li>
   <li>
    <span>
     Variables (e.g. home, work, school) → NO
    </span>
   </li>
  </ol>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Location can be inside the utterance as entity or can be the user position. In this case it must be up-to-date (define a timeout of validity and a way to ask user: “are you still there?”).
   </span>
  </p>
  <h3 id="h.yu4arhcugltb">
   <span>
    Single-turn NLU
   </span>
  </h3>
  <p>
   <span>
    [LABEL:approachSingleTurn]
   </span>
  </p>
  <p>
   <span>
    The computational graph that has been chosen to perform the joint task of intent classification and slot filling is the encoder-decoder model proposed by Liu et Al.
   </span>
   <sup>
    <a href="#ftnt146" id="ftnt_ref146">
     [146]
    </a>
   </sup>
   <span>
    because of its “state of the art” condition. The focus has been on how to provide better inputs to the system in terms of word embeddings and on output dependency modeling.
   </span>
  </p>
  <h4 id="h.o3gm3qxbrx5s">
   <span>
    Word embeddings
   </span>
  </h4>
  <p>
   <span>
    [LABEL:approachWv]
   </span>
  </p>
  <p>
   <span>
    The paper presenting the used model is not describing the way the words are fed into the network. The implementation provided by the authors
   </span>
   <sup>
    <a href="#ftnt147" id="ftnt_ref147">
     [147]
    </a>
   </sup>
   <span>
    however shows that a word embedding layer is part of the model and the values are randomly initialized. Since the dataset on which the evaluation has been done is not very big, in the approach chosen fixed word embeddings, pretrained on bigger corpuses, have been used as inputs and this gave a little boost on performances (see section [RE:validationResults]) on all the used datasets as can be seen in the evaluation section. Using pre-trained static word embeddings as inputs to the neural network can help reducing the number of tunable parameters and therefore help training small corpuses, in addition to other advantages (word similarity evaluation possible also on words that were not contained in the training corpus of the domain)
   </span>
  </p>
  <p>
   <span>
    This is the most language-dependent part of the network and the goodness of the classification results highly depend on the quality of the embeddings.
   </span>
  </p>
  <p>
   <span>
    Natural Language Processing techniques depend a lot on the selected language and also available libraries come with differentiated support based on it. Features as Part of Speech, or Parse Trees, both if they come from statistical models or handcrafted ones, can be good only if a lot of work is done in annotation of big corpuses and therefore the efforts mainly fall on the English language.
   </span>
  </p>
  <p>
   <span>
    Instead approaches that consider word embeddings are less dependent on annotated corpuses, since those vectors can be built by simply having sentences in the desired language, that can be easily found over the web. The efforts here are time and space requirements to compute the embedding values.
   </span>
  </p>
  <p>
   <span>
    As mentioned before, the bot has as requirement to support also the italian language. For the english language several preprocessed word embeddings are available, trained on different corpuses and with different algorithms. Instead for italian, the found pre-trained embeddings are GloVe and word2vec trained by the Human Language Technologies of Pisa
   </span>
   <sup>
    <a href="#ftnt148" id="ftnt_ref148">
     [148]
    </a>
   </sup>
   <span>
    and the official fastText
   </span>
   <sup>
    <a href="#ftnt149" id="ftnt_ref149">
     [149]
    </a>
   </sup>
   <span>
    , all trained on the Wikipedia corpus. Another widely used corpus is CommonCrawl
   </span>
   <sup>
    <a href="#ftnt150" id="ftnt_ref150">
     [150]
    </a>
   </sup>
   <span>
    , that is the output of web crawlers that reached billions of webpages, splitted by their language and preprocessed to have deduped text-only.
   </span>
  </p>
  <p>
   <span>
    An important aspect to make word embeddings work at their best is to apply the same kind of tokenization both when training the word embeddings and when using them when feeding RNNs. Usually the tokenization is done by removing punctuation and by applying other transformations (such as replacing hyphens and apostrophes with spaces to split the words) but for this application the choice has been to include punctuation in order not to discard any useful hints from the input sentences. Implementation details about the tokenization can be found
   </span>
   <span>
    <a href="#h.hicafqiwqr81">
     here
    </a>
   </span>
   <span>
    .
   </span>
  </p>
  <h4 id="h.gorlkbt8j0xz">
   <span>
    Output dependencies
   </span>
  </h4>
  <p>
   <span>
    As mentioned in the state of the art section, modeling the output dependencies is very important in sequence labeling tasks. The different choices available are: feeding output labels as inputs to the decoding stage, or using a linear chain CRF. For its simplicity and also because it is mentioned in the referenced paper
   </span>
   <sup>
    <a href="#ftnt151" id="ftnt_ref151">
     [151]
    </a>
   </sup>
   <span>
    , the first one has been chosen.
   </span>
  </p>
  <p>
   <span>
    In the description of the network in the paper it is not described how the output sequence labels are fed into the decoder. Different methods can be used: using one-hot encoding, that has the problem of scalability when the output dictionary is big enough, and word embeddings, to have a size independent from the number of existing slot labels. This time the word embeddings, being the labels part of an independent dictionary with respect to the input words, have been included in the trainable parameters of the computational graph.
   </span>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 391.00px; height: 298.00px;">
    <img alt="" src="https://lh3.googleusercontent.com/aHgGVSdG8K2pSRRbMiqERWiZzgx8iW66krO69uA3hqoXSEC7lAZWxL16Lkj2IvZ-qjZFBD-B3XmEkxMTNs_pvh7wdsr38K-cY5Zo9p8Zd-YuMnwaZ0q722BuFmOz2rzNF5SU0svz" style="width: 391.00px; height: 298.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <h3 id="h.re7kk9b4p3yn">
   <span>
    Multi-turn NLU
   </span>
  </h3>
  <p>
   <span>
    [LABEL:approachMultiTurn]
   </span>
  </p>
  <p>
   <span>
    For solving the problem of multi-turn NLU, after analyzing solutions in the literature to the problem of keeping the context [TODO add refs and explanations], the decision has been to change the single-turn architecture to make it consider
   </span>
  </p>
  <p>
   <span>
    Architecture:
   </span>
  </p>
  <p>
   <span>
    TODO from HQA2018 approach
   </span>
  </p>
  <p>
   <span>
    TODO add picture
   </span>
  </p>
  <p>
   <span>
    TODO define better how to use masks to train slots only over user turns.
   </span>
  </p>
  <p>
   <span>
    TODO remove intent decoding RNN, explaining that the performances on single turn were better without it.
   </span>
  </p>
  <p>
   <span>
    The architecture chosen is basically the same of the encoder-decoder with aligned inputs for joint SLU. The part of the network for the intent, instead of taking simply the last state of the encoder, takes all the values of the encoder and summarizes them using a RNN. [Liu et al
   </span>
   <sup>
    <a href="#ftnt152" id="ftnt_ref152">
     [152]
    </a>
   </sup>
   <span>
    ] only used the last encoder state for the first proposed architecture, and in the second architecture used a mean pooling over the encoded states, additionally using an attention mechanism. An RNN was chosen because it can learn how to put together all the encoded states, and also because it is able to keep memory, that is what we want in a multi-turn environment.
   </span>
  </p>
  <p>
   <span>
    For the memory between turns, the hidden states of the RNN cells are saved for each user, and are restored back in the cells when a new sentence is received from the same user. To also consider the agent turn, the network receives as inputs not only the current user turn words, but also the agent turn words. Those won’t produce in output slot labels, because is not our interest, but will affect the RNN cells in the forward direction.
   </span>
  </p>
  <p>
   <span>
    Looking at the complete structure, it can be noticed that the memory of the states only initializes the forward cells. This is wanted, because in this way the new turn is affected by the old one, and the backward links are not connected through different turns because the decisions that have been taken in the past cannot be modified by next turns.
   </span>
  </p>
  <p>
   <span>
    A critical point where this model should prove the goodness of the chosen cell, is when two completely unrelated sentence follow each others (e.g. two different intents). In this case the LSTMs/GRUs should learn to forget their past state in order to provide the new values. In inference time we can never know whether a certain sentence is a follow-up or is a new intent without any logical linking with the previous one, so the only good option is to train the network to learn when this happens. For this motivation, instead of training on single independent sessions, the batch size has been reduced to 1 and all the sessions have been concatenated. In this way the model will be able to work better both on sentences that belong to the same session (for example answering back to a missing slot) and on other independent questions (for example an independent intent).
   </span>
  </p>
  <p>
   <span>
    Batch_size=1
   </span>
  </p>
  <ul>
   <li>
    <span>
     The order of samples is chosen in the program flow
    </span>
   </li>
   <li>
    <span>
     Can avoid padding:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     No mess with masks
    </span>
   </li>
   <li>
    <span>
     In stateful RNN padding is likely to deteriorate performances
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Performance decreases (only at training/evaluation time, not in usage because sentences to the bot are likely not to arrive all together)
    </span>
   </li>
  </ul>
  <h3 id="h.q9a05jk3u986">
   <span>
    Dialogue State Management
   </span>
  </h3>
  <p>
   <span>
    The Dialogue State Management module is responsible to handle the mapping from intents and available slots to the actions and responses. For this part, as mentioned before, a rule-based approach has been chosen.
   </span>
  </p>
  <p>
   <span>
    Each intent type is mapped to a specific set of actions. Additionally for each slot type that is used in a certain intent type a rule is configured to say if it is compulsory to have a value or not. In the case the slot is compulsory, the Dialogue State Manager must ask back for them, prompting a question to the user.
   </span>
  </p>
  <p>
   <span>
    The intents that have been designed however have another type of rule: the location entity can be replaced by the current position of the user, that can be sent as attachment or expressed with the corresponding intent ‘set_position’. If the value of the user position exists and is recent (last 2 hours), it can be used. Otherwise the user is required to provide it.
   </span>
  </p>
  <p>
   <span>
    Once the requirements have been checked, the corresponding actions are called and the response is generated.
   </span>
  </p>
  <h2 id="h.t1pa04y1x2e1">
   <span>
    Personalization
   </span>
  </h2>
  <p>
   <span>
    [LABEL:approachPersonalization]
   </span>
  </p>
  <p>
   <span>
    This section explains how the personalization techniques can be applied to the interactions with a chatbot, providing personalized content recommendations in a tailored communication fashion. This approach is composed of two main parts: content recommendation, that is usually analyzed in the RecSys community, and in this specific case the contents that are provided as recommendations are places around the user that he could be interested in. The second part instead is relative to the interaction itself: the goal is to change the behaviour of the bot relatively to the mood the user is in, in a dynamic way.
   </span>
  </p>
  <h3 id="h.avx4n4low1rk">
   <span>
    Content recommendation
   </span>
  </h3>
  <p>
   <span>
    [LABEL:approachRec]
   </span>
  </p>
  <p>
   <span>
    For giving content recommendations, the objective of the recommender is to provide interesting places for the user around his trip. The area of search is therefore established by the results of the information: different strategies could be used to determine this surface, but the fastest one, that helps also querying providers of places information (e.g. Foursquare, Facebook Places, Google Places), is to use a circle determined by its center (the mean point between the source and the destination) and its radius (chosen to cover the source and destination, with some margin).
   </span>
  </p>
  <p>
   <span>
    TODO add figure please
   </span>
  </p>
  <p>
   <span>
    Determined the valid surface, the places in it are candidates for the recommendation. To restrict the places list and provide one as recommendation, the strategy to be applied is to determine a category that the user could be interested in.
   </span>
  </p>
  <p>
   <span>
    To find this category, a bit of user modeling needs to be done.
   </span>
  </p>
  <h4 id="h.9j2yq7sjjkvs">
   <span>
    User modeling
   </span>
  </h4>
  <p>
   <span>
    First of all, it is important to understand which variables need to be collected to build the user model. Some manifest variables like gender, age, profession, religion, political orientation are used for many profiling techniques, but not always they are available. Moreover, from a bot that is giving bike sharing information it is strange to be asked for those values, and can only make the user stop using the bot because it’s asking strange questions. For these reasons, those information are not even asked to the user.
   </span>
  </p>
  <p>
   <span>
    As explicit data collection, some questions can be asked to the user in a more appropriate way, relatively to bike sharing. For example asking why the user makes use of bike sharing, to go to school, work or simply to run errands around the city. This type of information can be collected at the beginning as bootstrap questions, or also after some dialogue with the user has been done. In this way the user can soon use the service without being blocked by bootstrap questions, but after some minutes of inactivity he can be sent some questions in order to improve the service.
   </span>
  </p>
  <p>
   <span>
    Other explicit information can be collected when the recommendation is in act, as feedback to the suggestions: feedback can be positive or negative, collected through specific buttons or intercepting the user clicking on the information given about the place. The feedback can help refining the user interest and also understanding his attitude towards recommendations (as will be seen in the “tailored communication”).
   </span>
  </p>
  <p>
   <span>
    As implicit user modeling, observing the patterns (temporal and spatial) of the queries performed can help find users similarity and improve the collaborative filtering approach (see the following on recommendation paths). The patterns in the spatial dimension correspond to frequent places, while in the temporal dimension can be interpreted as habits.
   </span>
  </p>
  <p>
   <span>
    Other optional sources of data may come from social networks. On social networks people usually show their interests towards different kind of things: check-ins, places, music, films, and many others. This can be very helpful to user modeling, both providing user features both providing item (places in this case) features. The inputs in this case are the interests of the user, that can be provided by doing a login on the desired social network. This login is proposed to the user as optional and if denied must not preclude any functionalities of the bot. Simply the model for this user won’t be accurate and more generic recommendation will be provided.
   </span>
  </p>
  <p>
   <span>
    Given the social network footprints, a prediction on the Big5 personality traits can be done using some online available APIs: the big5 will be user features, together with “concentration” on different areas. Mention where those model come from.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Once these inputs are collected, different recommendation paths can be used to suggest the categories of places.
   </span>
  </p>
  <h4 id="h.bcvzzw13ygcz">
   <span>
    Recommendation paths
   </span>
  </h4>
  <p>
   <span>
    As will be explained in this section, the recommendations of categories are given following an hybrid recommendation approach, combining content-based with collaborative techniques.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 561.00px; height: 451.00px;">
    <img alt="" src="https://lh4.googleusercontent.com/gfkKUsOl5LQqgxCQHeQNKmRMkp9ojnlyyp02ODanU3dO4CqHaFB6oF7xkLY_6mM04yaXQoaxsxLpSXBHYkRKS0EdWcvMUPlzmOVJSkap1cI4HG6Ro416yOGSdEI1nHSHUPsVxlVD" style="width: 561.00px; height: 451.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    First of all, when a certain user enables the social login, the personalization module will access the user interests together with the places he visited. In this way both features of the user and features of the places are analyzed.
   </span>
  </p>
  <p>
   <span>
    From the side of the user, his interests are analyzed and the following features are collected:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Big5 personal traits
    </span>
   </li>
   <li>
    <span>
     Main interests, as top level categories
    </span>
   </li>
  </ul>
  <p>
   <span>
    Instead on the side of places that have been visited or have been “liked” by the user, the respective category is retrieved.
   </span>
  </p>
  <p>
   <span>
    Having this mapping, a trainable relation is built (a neural network layer) and trained with the data of every user. This relation has the meaning of: people that have those characteristics tend to like those kind of places. It is content-based because users that have expressed a positive opinion on a certain category, will receive recommendations with places that have similar characteristics. But it is also collaborative because the relation is trained with data of all the users, so similar user features map to similar places features.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Other ways of refining features are:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Using the feedback provided explicitly from the user, or captured implicitly when the user interacts with the recommendations
    </span>
   </li>
   <li>
    <span>
     Using the responses to the bootstrap questions (e.g. “why do you use bike sharing?”)
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Furthermore, the Episodic Knowledge can be used to provide a highly collaborative filtering: the history of usage can be analyzed to detect spatial and temporal patterns and finding users with similar behaviour. The concept behind this approach is that people that use the system in similar ways may have common interests.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    To face the cold start problem, that surely exists for the specified approach, generic recommendations can be given at the beginning and can be slowly refined by collecting the feedbacks and collecting more users that hopefully will accept to perform the social login.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    TODO Selection of category given a distribution: analyze how a concept of “temperature” can model more randomness versus a fixed output (sampling)
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <ul>
   <li>
    <span>
     Suggestion engine:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Trip suggestion and reminders (must not be invasive):
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Previously used stations
    </span>
   </li>
   <li>
    <span>
     Previous schemes/timetables
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Provide information on the city context (not strictly related to domain)
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     User relevant places on the way
    </span>
   </li>
   <li>
    <span>
     Time relevant (suggested place should be open)
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    How to measure the success of recommendations? Accuracy, diversity, serendipity
   </span>
  </p>
  <h3 id="h.vzxoff8ahoem">
   <span>
    Tailored communication
   </span>
  </h3>
  <p>
   <span>
    This part of personalization is not relative to the content of recommendation, but is about how to adapt the communication dynamically with the behaviour of the user.
   </span>
  </p>
  <h4 id="h.8tbwh64e2xnl">
   <span>
    Operative Mode
   </span>
  </h4>
  <p>
   <span>
    This may reflect on how the bot operates with the current user, the
   </span>
   <span>
    Operative Mode
   </span>
   <span>
    . This dynamic property reflects how much the user is willing to receive recommendation.
   </span>
  </p>
  <p>
   <span>
    Three operative modes can be designed:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Normal: the recommendation are provided when the system is sufficiently confident
    </span>
   </li>
   <li>
    <span>
     Straight to the point: the user expressed negative sentiment in response to a recommendation, or may be in a hurry and needs only the bike information. In this situation the bot is not allowed to send recommendations
    </span>
   </li>
   <li>
    <span>
     Conversational: the user is chatting with the bot, asking some questions about it. This may be the right moment to ask some information to the user in order to refine the model
    </span>
   </li>
  </ul>
  <p>
   <span>
    For deciding which operative mode should be applied, it is very important to collect signs from the users. Signs can be explicit in feedbacks: both textual or by using some buttons next to results. Signs can also be implicitly sent: in text, latent sentiment analysis can sometimes provide hints.
   </span>
  </p>
  <p>
   <span>
    Or signs can be collected from monitoring the destination pages that are linked in the results. For this reason the strategy of link rewriting can be used to intercept the user clicking on the details of the recommendation: an intermediate webserver can be set as the destination when creating the link; when it receives the request, it records the fact that the user has reached it and sends a redirect for the real destination. This strategy is widely used to measure click-through rates.
   </span>
  </p>
  <h4 id="h.r133th2co0hh">
   <span>
    Personal preferences
   </span>
  </h4>
  <p>
   <span>
    TODO explain
   </span>
  </p>
  <p>
   <span>
    Set favourite stations, set preferred time spans to receive automatically messages, set other preferences.
   </span>
  </p>
  <h4 id="h.phdcqfmwyzs7">
   <span>
    Linguistic Style
   </span>
  </h4>
  <p>
   <span>
    TODO explain
   </span>
  </p>
  <p>
   <span>
    Adapting dynamically to the jargon used by the user, formality level imitation.
   </span>
  </p>
  <p>
   <span>
    Example: [Jiwei Li, Michel Galley, Chris Brockett, Georgios P Spithourakis, Jianfeng Gao,and Bill Dolan. 2016.  A persona-based neural conversation model.The 54thAnnual Meeting of the Association for Computational Linguistics1, 994–1003].
   </span>
  </p>
  <h2 id="h.5jp7qco42re1">
   <span>
    Information Retrieval
   </span>
  </h2>
  <p>
   <span>
    [LABEL:approachIR]
   </span>
  </p>
  <p>
   <span>
    The Information Retrieval Module connects to different sources of data to provide information about the bike sharing systems, interfacing with directions APIs and retrieving information about the places (especially for the personalization component).
   </span>
  </p>
  <h3 id="h.abxpu7wzdus3">
   <span>
    Data Sources
   </span>
  </h3>
  <p>
   <span>
    Below are listed the different data sources that need to be queried to provide updated information to the users.
   </span>
  </p>
  <h4 id="h.5z1ti99xp363">
   <span>
    Bikes
   </span>
  </h4>
  <p>
   <span>
    Bike sharing is an expanding market, where every day more companies are investing in, and more people decide to use them for their relatively small cost and for their easiness in the urban environment. The majority of the providers have station-based systems, where the users begin and end their trip at fixed locations (the stations). However nowadays station-free systems are being spread in the major cities, with the advantage of being able to leave the vehicle in any responsible place.
   </span>
  </p>
  <p>
   <span>
    Given the multitude of companies and also some kind of competition, since in different cities multiple providers are available (e.g. in Torino there is the station-based toBike and the station-free oBike,
   </span>
   <span>
    GoBeeBike
   </span>
   <sup>
    <a href="#cmnt5" id="cmnt_ref5">
     [e]
    </a>
   </sup>
   <span>
    , MoBike), interfacing with them is quite difficult for two reasons:
   </span>
  </p>
  <ul>
   <li>
    <span>
     The bike sharing providers may not want to make available their data to third-party players, both for privacy issues (data disclosure can be quite a problem also in this domain
    </span>
    <sup>
     <a href="#ftnt153" id="ftnt_ref153">
      [153]
     </a>
    </sup>
    <span>
     ) and because of its value
    </span>
   </li>
   <li>
    <span>
     The data format is different for each bike sharing system
    </span>
   </li>
  </ul>
  <p>
   <span>
    For this reason, the only applicable approach to get the desired information is to use some open-source contributive library that is produced and maintained by a sufficient number of people in order to cover a big number of cities. The information may be extracted by publicly-available APIs or by web scraping techniques.
   </span>
  </p>
  <p>
   <span>
    Fortunately such library exists for station-based systems, as will be seen in the implementation part.
   </span>
  </p>
  <h4 id="h.hhd7s72d1atm">
   <span>
    Paths
   </span>
  </h4>
  <p>
   <span>
    Having information on the bike sharing and on the user position and desired destination, the bot should provide a navigation path with the selected constraints. Since many web APIs exist with good results, the approach is to find a service with the desired functionalities and use it.
   </span>
  </p>
  <p>
   <span>
    The main feature requested is to compute paths that are suitable for cycling: respecting the street laws, avoiding highways, preferring cycle lanes are just the main examples.
   </span>
  </p>
  <ul>
   <li>
    <span>
     Google Maps Directions API support for bicycles does not cover the city of Turin
    </span>
   </li>
   <li>
    <span>
     Mapbox → used because simpler and seems reliable
    </span>
   </li>
   <li>
    <span>
     osm
    </span>
   </li>
  </ul>
  <p>
   <span>
    A feature that would have been interesting to consider is the possibility to choose the path with criterions different from shortest or faster, for example considering the smells and happiness of the places in the route. This could potentially provide results that offer a better quality of the path, but have currently be done as an experiment only in the city of London
   </span>
   <sup>
    <a href="#ftnt154" id="ftnt_ref154">
     [154]
    </a>
   </sup>
   <sup>
    <a href="#ftnt155" id="ftnt_ref155">
     [155]
    </a>
   </sup>
   <sup>
    <a href="#ftnt156" id="ftnt_ref156">
     [156]
    </a>
   </sup>
   <span>
    . This feature was not considered, in favour of the usual optimal path.
   </span>
  </p>
  <h4 id="h.kbb2yw2mio7z">
   <span>
    Places
   </span>
  </h4>
  <p>
   <span>
    Foursquare
   </span>
   <span>
    <a href="https://developer.foursquare.com/docs/venues/explore">
     https://developer.foursquare.com/docs/venues/explore
    </a>
   </span>
   <span>
    can easily ask for suggestion on venues with specific category/property (120.000 requests per day, bigger coverage)
   </span>
  </p>
  <p>
   <span>
    Google places API
   </span>
   <span>
    <a href="https://developers.google.com/places/web-service/search">
     https://developers.google.com/places/web-service/search
    </a>
   </span>
   <span>
    provides similar features (1.000 requests per day)
   </span>
  </p>
  <p>
   <span>
    <a href="https://www.quora.com/What-are-the-pros-and-cons-of-each-Places-API">
     https://www.quora.com/What-are-the-pros-and-cons-of-each-Places-API
    </a>
   </span>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    TODO take from implementation
   </span>
  </p>
  <h4 id="h.s4795m4nagev">
   <span>
    Facebook GRAPH
   </span>
  </h4>
  <p>
   <span>
    To retrieve user information (interests) we chose to use Facebook because of its larger coverage
   </span>
  </p>
  <ul>
   <li>
    <span>
     Why facebook
    </span>
   </li>
   <li>
    <span>
     Already talking on facebook messenger, why doing login?
    </span>
   </li>
  </ul>
  <p>
   <span>
    The data that is available directly from the messenger API is the following: first_name,last_name,profile_pic,locale,timezone,gender,is_payment_enabled
   </span>
  </p>
  <ul>
   <li>
    <span>
     How (the flow)
    </span>
   </li>
  </ul>
  <p>
   <span>
    In order to be able to do personalization, there exists a way to link the messenger id (page-scoped) to other ids (also facebook id indirectly):
   </span>
   <span>
    <a href="https://developers.facebook.com/docs/messenger-platform/account-linking">
     https://developers.facebook.com/docs/messenger-platform/account-linking
    </a>
   </span>
  </p>
  <p>
   <span>
    Once the facebook id is retrieved, it is possible to use the facebook graph API.
   </span>
  </p>
  <p>
   <span>
    To the external account the messenger platform sends a request to an url specified by the developer, adding account_linking_token and redirect_uri (page to redirect user after login). The external site, if login successful, redirects to redirect_uri with authentication_code custom (maybe put there the id in order to allow the bot to do the join)
   </span>
  </p>
  <p>
   <span>
    From the other side, the oauth flow:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Redirect user to facebook oauth with client_id (the app id) and redirect_uri (where facebook will send the user)
    </span>
   </li>
   <li>
    <span>
     After that user gives permissions, fb will redirect to redirect_uri with other parameters
    </span>
   </li>
  </ul>
  <p>
   <span>
    The procedure seems quite complicated and requires a web server component that interacts on one side with messenger platform and on the other side handles the facebook login.
   </span>
  </p>
  <ul>
   <li>
    <span>
     Which details
    </span>
   </li>
  </ul>
  <p>
   <span>
    TODO update. Talk also about not saving personal details of users, only looking at likes and tagged places
   </span>
  </p>
  <h4 id="h.pbn2tcklf6dj">
   <span>
    Big5 computation
   </span>
  </h4>
  <p>
   <span>
    MagicSauce
   </span>
  </p>
  <h4 id="h.bfdsu7aum8vc">
   <span>
    Other
   </span>
  </h4>
  <p>
   <span>
    Bootstrap: how to ask question in chat without being invasive
   </span>
  </p>
  <p>
   <span>
    Meteo?
   </span>
  </p>
  <p>
   <span>
    Weather forecasts: if bad weather is expected, tell user
   </span>
   <span>
    <a href="https://www.wunderground.com/weather/api/">
     https://www.wunderground.com/weather/api/
    </a>
   </span>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h3 id="h.78zc2qf1izjb">
   <span>
    Data modeling
   </span>
  </h3>
  <p>
   <span>
    Relative to manipulation of information and the persistence.
   </span>
  </p>
  <p>
   <span>
    Some data modelling
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Request: {utterance: {userID, time, content}, NLP: {intent, entities:[type,value]}, context, info: {stations:[], places:[], meteo:[]}, recommendations: [type,value,confidence], decision: {}}
   </span>
  </p>
  <p>
   <span>
    Is filled along the pipeline. Each stage is adding an element to it
   </span>
  </p>
  <p>
   <span>
    NLP
   </span>
  </p>
  <p>
   <span>
    From utterance:
   </span>
   <span>
    utteranceID
   </span>
   <span>
    , userID, time, content
   </span>
  </p>
  <p>
   <span>
    To: {intent,entities[type,value]}
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Entity resolver
   </span>
  </p>
  <p>
   <span>
    From user model:
   </span>
  </p>
  <ul>
   <li>
    <span>
     User city
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     User position
    </span>
   </li>
   <li>
    <span>
     User places: home, work/school
    </span>
   </li>
  </ul>
  <p>
   <span>
    The location entities become references to places in the DB (using places API):
   </span>
  </p>
  <ul>
   <li>
    <span>
     From name to place (the name in the utterance is used as keyword to do a search in the city proximity)
    </span>
   </li>
   <li>
    <span>
     From position to place (when user sends position, find a relevant place in the proximity)
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Core
   </span>
  </p>
  <p>
   <span>
    Internal state:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Previous questions / topic (field of dialogue): to understand messages without explicit intent and to resume interaction (stack of interactions: “i want a bike” PUSH “where are you?” -&gt; “i am at XXX” -&gt; “ok got it” POP “you can find 3 bikes at YYY”)
    </span>
   </li>
   <li>
    <span>
     Special topic of bootstrap: activated at the beginning
    </span>
   </li>
  </ul>
  <p>
   <span>
    Core steps:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Intent
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     If the intent is not specified (or generic intent “approve”/”disapprove”), look at the entity provided and if a previous state is saved. Based on these, derive the intent
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Preparation of parameters for functions (take from entities) and check requirements
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     If requirements are ok, proceed with steps
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     If previous state, pop it
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     If some requirements are missing, save current state (push) and ask for requirement
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Bike sharing:
   </span>
  </p>
  <p>
   <span>
    Station:
   </span>
   <span>
    stationID
   </span>
   <span>
    , name, description
   </span>
   <span>
    , (lat,long), free_bikes, free_slots
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Personalization:
   </span>
  </p>
  <p>
   <span>
    User:
   </span>
   <span>
    userID
   </span>
   <span>
    , Name, Surname, sex, age
   </span>
  </p>
  <p>
   <span>
    Episodic KB:
   </span>
   <span>
    userID, location, time
   </span>
   <span>
    , action (was at/took bike/left bike)
   </span>
  </p>
  <p>
   <span>
    Explicit rating: collecting user feedback after recommendation / bootstrap / unsolicited rating → needs to be investigated
   </span>
  </p>
  <p>
   <span>
    User features (built by the model): favorite/recurrent places, sportiveness, art interest
   </span>
  </p>
  <ul>
   <li>
    <span>
     RecurrentAction:
    </span>
    <span>
     ID
    </span>
    <span>
     , userID, type, frequency (days in week pattern), time of day, userPlaceID (external key to UserPlace)
    </span>
   </li>
   <li>
    <span>
     UserPlaces:
    </span>
    <span>
     userID, placeID
    </span>
    <span>
     , role (home/work/school/other)
    </span>
   </li>
  </ul>
  <p>
   <span>
    The stimulus link from the result provider to the recommender contains: trip information / direct question to the recommending system
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    User_feature:
   </span>
   <span>
    userID, featureID
   </span>
   <span>
    , value ← output of user modeler, input to recommender
   </span>
  </p>
  <p>
   <span>
    Place:
   </span>
   <span>
    placeID
   </span>
   <span>
    , name, description, (lat,long), category, subcategory
   </span>
  </p>
  <p>
   <span>
    Place_feature:
   </span>
   <span>
    placeID, featureID
   </span>
   <span>
    , value
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h1 id="h.jwgojbcfjwyc">
   <span>
    Implementation
   </span>
  </h1>
  <p>
   <span>
    [LABEL:implementation]
   </span>
  </p>
  <p>
   <span>
    This section focuses on the bot prototype that has been developed to put in practise some of the studies that have been done. The implementation can be divided in three main areas:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Omnichannel implementation, with platform-dependent code completely decoupled from the core of the bot
    </span>
   </li>
   <li>
    <span>
     Natural Language Understanding, with the implementation of recurrent neural network for joint intent detection and slot filling in a multiturn environment
    </span>
   </li>
   <li>
    <span>
     Personalization and interaction with the FB graph API
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    TODO add details about which features have been implemented
   </span>
  </p>
  <p>
   <span>
    While the first two areas have a complete path in this work (from approach choice to implementation and measurements), the personalization has not reached the implementation stage. In this part of the thesis however some details on the APIs and methodologies that can be used to implement it are treated.
   </span>
  </p>
  <p>
   <span>
    The implementation focused more on experiments with NLU techniques and experimentation of them.
   </span>
  </p>
  <p>
   <span>
    TODO add link to implementation github?
   </span>
  </p>
  <h2 id="h.xhx4g788ab7d">
   <span>
    Interaction with chat platforms
   </span>
  </h2>
  <p>
   <span>
    [LABEL:implementationInteraction]
   </span>
  </p>
  <p>
   <span>
    To interact with the chat platforms it was mentioned that a separate component has been designed to adapt to the different messaging platforms. This component, the Bot Server, on one side communicates with the Bot Core, that can be physically hosted on another machine, and on the other side interfaces with messaging platforms.
   </span>
  </p>
  <p>
   <span>
    The architecture can be viewed in the following picture.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 198.67px;">
    <img alt="" src="https://lh5.googleusercontent.com/mOs5p5gHK8-xQucEjiboM8hfNqLdwGAA6QWAq_L3aJIekQboLlpfcmrvsgUUtCMhFiFJJJ6WtS5358PEPuoWvMf3pKULhBIjJKBfBLMg3JbagApHzfZYObNQ9u5gNC93XvzvgbwC" style="width: 602.00px; height: 198.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    THE CONNECTIONS
   </span>
  </p>
  <p>
   <span>
    The communication with the Core of the bot is done over a websocket to allow asynchronous messages to be sent in both directions, without using polling techniques that may consume resources and bandwidth. The Bot Server exposes two different endpoints for Bot Cores, one for the English version and one for the Italian one. Those endpoints thanks to HTTPS protocol offer a reliable channel that provides data encryption to protect data in motion and authentication through a symmetric key shared between the two parts.
   </span>
  </p>
  <p>
   <span>
    The data is transferred between those two components with the JSON standard over the websocket, with a format that does not depend on any specific messaging platform.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    On the other side, the Bot Server is not put directly in contact with the Messaging platforms because it would have required to implement all the webhook endpoints for them and also to translate from and to all the platform dialects. The most easy way to implement it has been found by interacting with a third-party component (Microsoft Bot Framework) that can be configured on a web portal to reach the different platforms. This removed a lot of implementation effort but brought to an architecture that is not very elegant and compact. The communication between the Bot Server and the Bot Framework is done in a way that is similar to the one required by messaging platforms: both components expose web endpoints (that must provide HTTPS connections with digital certificates that have as root a trusted CA) that are used for one-directional communications (the only response is is given is a status code that represent the outcome of the request) and each message is sent independently in its direction.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    BOT SERVER ROLES
   </span>
  </p>
  <p>
   <span>
    The main role of the Bot Server is to receive the messages on one side and provide them to the other one, by transforming their representation. The Bot Framework works well in the direction of receiving the messages from the user, but on the other directions does not provide all the features specific to the messaging platform. For instance, both on Facebook Messenger and on Telegram, buttons can be shown to the user in order to send the current position, while other platforms don’t support it. To send those types of messages, it is necessary to send attachments in the native form of the platform (‘quick_replies’ with ‘content_type’: ‘location’ for Facebook, ‘reply_markup’ with ‘request_location’: true for Telegram).
   </span>
  </p>
  <p>
   <span>
    Another role the Bot Server covers is the management of the Facebook Login: as will be explained in the personalization details, it exposes a dynamic web page that is reached after the user performs the login, and manages the retrieval of the user likes and user locations.
   </span>
  </p>
  <p>
   <span>
    Furthermore, this component also provides a web page where users can test the bot without using messaging platforms.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    WHY NOT A SINGLE COMPONENT
   </span>
  </p>
  <p>
   <span>
    Having two different custom components has many advantages: both conceptual and practical.
   </span>
  </p>
  <p>
   <span>
    The main conceptual advantage is that the coupling with messaging platforms is reduced: the bot core does not know anything related to them, and interacts with a representation of data that is human-readable and has no dependencies.
   </span>
  </p>
  <p>
   <span>
    The practical ones are many, generated from different needs. First of all: performance needs. The Bot Server does not require high computation resources because is simply a proxy that manipulates the representation of information. The Bot Core instead, running Neural Networks to classify the sentences, needs disk space and RAM (the language models are quite huge), together with a computing power that should be sufficient to produce outputs in short times.
   </span>
  </p>
  <p>
   <span>
    The Bot Server then requires to host a web server with a fixed hostname and with good certificates trusted from the Bot Framework. Configuring the Facebook Login requires a destination URL that contains a hostname (not an IP) that will receive as parameters the token that can be used to query the Graph API (as will be described later). The Bot Framework needs to be configured with webhooks endpoints to whom send POST requests with the incoming messages. The certificates for HTTPS need to be provided by a trusted Certification Authority.
   </span>
  </p>
  <p>
   <span>
    Having those needs and opting for a cheap feasible option (without buying a powerful VPS with all those features), the choice has been to split those components.
   </span>
  </p>
  <p>
   <span>
    The Bot Server is hosted on heroku
   </span>
   <sup>
    <a href="#ftnt157" id="ftnt_ref157">
     [157]
    </a>
   </sup>
   <span>
    with a free plan that has limitations in computing power and space, but fits the requirements for this component. Especially, the platform supports natively HTTPS that are covered by a wildcard certificate (every application receives a URL that matching with *.herokuapp.com is valid). Websites and Web Server applications can easily be created (using one of a large set of supported languages and frameworks) and pushed to the platform and the deployment will occur with no pains in configuration.
   </span>
  </p>
  <p>
   <span>
    Having the websocket endpoints exposed on the Bot Server, the Bot Core does not require to act as a server: no hostname is required, no certificate, no port forwarding on NATs, no ports open on the machine that runs it. Furthermore also the websockets are HTTPS as offered by the Bot Server, so no plaintext interactions that could be intercepted.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    AN EXAMPLE
   </span>
  </p>
  <p>
   <span>
    To make clear how all the components work together, let’s follow what happens from when a user sends a message until he receives back a response.
   </span>
  </p>
  <p>
   <span>
    First of all, the user connected to a messaging platform (Facebook Messenger) sends some text, that arrives to the Facebook Servers. From there, since the destination account is a page that has been configured to receive messages at a certain webhook location, the message is sent to the Bot Framework. From the Bot Framework, the message is forwarded to the Bot Server that is finally one of the two components that has been developed. The Bot Server, checking that a Bot Core is connected over websocket for the specific language, delivers the message. The Bot Core processes the message (NLU, Information Retrieval, Response Generation) and replies on the websocket. The Bot Server transforms the response for the dialect of the destination platform and forwards the message to the Bot Framework that delivers it to the Facebook Servers. Finally the user receives the message.
   </span>
  </p>
  <h2 id="h.tyotzig5oet">
   <span>
    Natural Language Understanding
   </span>
  </h2>
  <p>
   <span>
    [LABEL:implementationNLU]
   </span>
  </p>
  <p>
   <span>
    This chapter focuses on the implementation that has been done relatively to NLU.
   </span>
  </p>
  <p>
   <span>
    TODO overview: wit prototype, developing RNN, word embeddings, datasets
   </span>
  </p>
  <h3 id="h.khvhakjb4317">
   <span>
    Wit.ai exploitation
   </span>
  </h3>
  <p>
   <span>
    [LABEL:implementationWit]
   </span>
  </p>
  <p>
   <span>
    In the early stages of the bot, the Natural Language Understanding has been delegated to an external provider: wit.ai. It was chosen for its simplicity to use and for being a standalone product that just provides NLU that are usable through a web API.
   </span>
  </p>
  <p>
   <span>
    While giving the advantage to have an almost ready to use NLU (the configuration of intents and entities requires few steps and few examples), it also helps building a dataset. All the sentences that are sent for analysis are stored in a inbox, that can be analyzed and the responses can be interactively fixed and validated (inserted in the dataset). The dataset can also be downloaded, and because it is stored in comprehensible JSON format can be used as annotated dataset to later train another NLU system.
   </span>
  </p>
  <p>
   <span>
    A limitation of this service is that it handles only single turn interactions, so the datasets that have been collected for both italian and english languages can be only used for this type of interaction.
   </span>
  </p>
  <p>
   <span>
    This stage of development has been done in order to understand the main needs of the users, and progressively new intents and examples have been added. On the other side, this experimentation also helped understanding what the NLU tasks are, giving an orientation in the very big landscape of Natural Language Processing.
   </span>
  </p>
  <p>
   <span>
    This helped directioning the study of literature towards approaches that could do a similar sentence classification and entity extraction.
   </span>
  </p>
  <p>
   <span>
    It has been noticed that for the intent classification task a promising approach is to use Recurrent Neural Networks, so the studies (both theoretical and practical) went deeper in exploring this field.
   </span>
  </p>
  <h3 id="h.lp7vop78km60">
   <span>
    Neural Networks frameworks
   </span>
  </h3>
  <p>
   <span>
    [LABEL:implementationNN]
   </span>
  </p>
  <p>
   <span>
    As of today many open source libraries and frameworks are available for NLP tasks, using different programming languages and different approaches. The most important ones are:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Stanford Core NLP: a java library that provides Tokenizer, NER, POS, Dependency Parsing, Coreference Resolution for languages: English, Arabic, Chinese, French, German, and Spanish
    </span>
   </li>
   <li>
    <span>
     NLTK: a python library that focuses on Tokenizers, n-grams, POS, NER
    </span>
   </li>
   <li>
    <span>
     Syntaxnet: a neural-network NLP framework for TensorFlow. Helps building POS and Parsers. It contains pre-built models for POS and parsing. It is the most accurate parser for english, able to parse correctly garden-paths
    </span>
   </li>
   <li>
    <span>
     SpaCy: an open source NLP framework that provides Tokenizer, NER, POS, Dependencies, Word Vectors, Parsing for different languages with the goal of being ready to use with pre-built models, and also fast
    </span>
   </li>
  </ul>
  <p>
   <span>
    But from what has been explored, those libraries focus on a selected subset of NLP tasks, and we wanted to implement Neural Networks to have full control of the process.
   </span>
  </p>
  <p>
   <span>
    Experimenting with Neural Networks has become easier and easier in the last years thanks to open source libraries that provide high-level APIs and, being used more and more, the support is really easy to obtain on online communities (such as
   </span>
   <span>
    <a href="https://stackoverflow.com/">
     StackOverflow
    </a>
   </span>
   <span>
    ) or on the official documentation or directly on the official code repositories. The programming languages that are supported for Neural Networks are many, but the most used nowadays is Python. In Python the most used frameworks are Keras and TensorFlow.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    For choosing between Keras and Tensorflow, experiments have been done with both. It is important to say that the two frameworks are not uncorrelated, because Keras as backed uses TensorFlow as default and TensorFlow recognizes Keras as its high-level interface
   </span>
   <sup>
    <a href="#ftnt158" id="ftnt_ref158">
     [158]
    </a>
   </sup>
   <span>
    . So the problem is simply to choose the abstraction level desired.
   </span>
  </p>
  <p>
   <span>
    Initially, when trying to emulate a
   </span>
   <span>
    RNN approach for Intent Classification (see section [REF:soaIntent], Keras has been used to implement it.
   </span>
  </p>
  <p>
   <span>
    Without considering the data preprocessing part that is needed to get the desired shape and values for the inputs and outputs (as inputs word vectors corresponding to the words in the sentence and as outputs the corresponding intent type), the building of the computation graph (a LSTM layer followed by a densely connected layer) and its training just required few lines of code.
   </span>
  </p>
  <p>
   <span>
    But when trying to implement less regular models, such as the encoder-decoder by Liu et al.
   </span>
   <sup>
    <a href="#ftnt159" id="ftnt_ref159">
     [159]
    </a>
   </sup>
   <span>
    , the criticality of using only high level API came out, especially considering complex parts as output dependencies.
   </span>
  </p>
  <p>
   <span>
    For this reason a comparison has been done in order to choose what implementation path to follow. Table [REF TABLE:kerasVsTf] summarizes the main differences found
   </span>
   <span>
    between Keras and TensorFlow.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <a id="t.294588e8659ce0c2920741efb4cbbf1ec9cf419c">
  </a>
  <a id="t.2">
  </a>
  <table>
   <tbody>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        feature
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Keras
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Tensorflow
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Abstraction level
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        High
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Can go down to reach single operations, but also high level API provided by lots of classes
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Learning difficulty
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Quite easy
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        More difficult, lots of details to consider
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Support for complex operations
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Focuses more on simple regular layers and cells
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Lot of already defined classes, and also helpers function that can help building complex operations
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Best for
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Rapid prototyping
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Advanced operations
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Documentation
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Documentation good for what’s implemented
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Documentation gap between trivial and advanced
       </span>
      </p>
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   <span>
    [TABLE:kerasVsTf CAPTION:A comparison between Keras and TensorFlow]
   </span>
  </p>
  <p>
   <span>
    Let’s see in more details what is the support provided by both libraries for building recurrent neural networks.
   </span>
  </p>
  <h4 id="h.cf1pj9z5h9sg">
   <span>
    Keras Recurrent API
   </span>
  </h4>
  <p>
   <span>
    Also when working on recurrent networks, using Keras layers is like playing with LEGO: blocks can be easily stacked one on the top of the other (also forks and joins are quite easy to perform).
   </span>
  </p>
  <p>
   <span>
    Keras Recurrent API is described extensively there (
   </span>
   <span>
    <a href="https://keras.io/layers/recurrent/">
     https://keras.io/layers/recurrent/
    </a>
   </span>
   <span>
    ). Here a quick overview is performed.
   </span>
  </p>
  <p>
   <span>
    To define a recurrent
   </span>
   <span>
    layer
   </span>
   <span>
    in Keras the class keras.layer.RNN takes as argument a RNN cell. Some parameters can be used to define how the layer will behave.
   </span>
  </p>
  <p>
   <span>
    A
   </span>
   <span>
    RNN cell
   </span>
   <span>
    is a class that has a call method that takes as input the actual input and the current state and produces the output and the next state. Different implemented
   </span>
   <span>
    cells
   </span>
   <span>
    exist.
   </span>
  </p>
  <ul>
   <li>
    <span>
     SimpleRNNCell: a fully connected RNN
    </span>
   </li>
   <li>
    <span>
     GRUCell: Gated Recurrent Unit
    </span>
    <sup>
     <a href="#ftnt160" id="ftnt_ref160">
      [160]
     </a>
    </sup>
   </li>
   <li>
    <span>
     LSTMCell: Long-Short Term Memory layer
    </span>
    <sup>
     <a href="#ftnt161" id="ftnt_ref161">
      [161]
     </a>
    </sup>
   </li>
  </ul>
  <p>
   <span>
    Layers can be easily stacked and the creation of deep recurrent network is very easy. Networks that are a simple stack of different layers (Recurrent, Dropout, Dense) won’t find any problem in implementation. But when the networks are not so linear in the flow or when some advanced layers are required, there might not be an already implemented solution. For example there is no native implementation for seq2seq models and layers, no Attention mechanism and is not possible to have a ready to use help for doing output dependency modeling (feeding back the outputs in a decoding layer). They can be implemented with the existing API, but they are not part of the library itself (some implementations can be found online as
   </span>
   <span>
    <a href="https://github.com/farizrahman4u/recurrentshop">
     https://github.com/farizrahman4u/recurrentshop
    </a>
   </span>
   <span>
    , but the number of people working on it is order of magnitude smaller than the tensorflow community
   </span>
   <span>
    ).
   </span>
  </p>
  <h4 id="h.3rp3zhi54s79">
   <span>
    Tensorflow Recurrent API
   </span>
  </h4>
  <p>
   <span>
    Also tensorflow has classes for defining RNN layers and RNN cells. But there is more:
   </span>
  </p>
  <ul>
   <li>
    <span>
     More cells: instead of a single implementation of LSTM and GRU, a wide variety of cells can be chosen
    </span>
   </li>
   <li>
    <span>
     More wrappers: wrappers can be used to encapsulate cells. DropoutWrapper, AttentionWrapper and a lot more can be added to the cells
    </span>
   </li>
   <li>
    <span>
     Seq2Seq native support: classes for defining encoders and decoders, with some helpers for modeling output dependencies can make a lot easier to build complex models that are not a linear chain of layers but contain links that move backwards (as in output dependencies, where the output label is fed back to the decoder RNN). For example to build a decoder that considers output dependencies, in tensorflow 3 components are needed: a RNN cell (e.g. LSTMCell), a CustomHelper (that is responsible to provide values to the cell and take the outputs. All this by simply declaring three custom functions) and a BasicDecoder that combines the cell with the helper and builds a layer. By dividing the roles of the cell and of the helper, a complex decoder can be built without a big effort.
    </span>
   </li>
  </ul>
  <p>
   <span>
    More details about the available classes can be found in the official documentation
   </span>
   <span>
    <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq">
     https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq
    </a>
   </span>
   <span>
   </span>
  </p>
  <h4 id="h.a08x42jk1qdf">
   <span>
    Decision
   </span>
  </h4>
  <p>
   <span>
    The objectives of the decision are:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Ability to build the proposed network without too much headache. Especially for the decoding stage, some advanced wiring is needed and would like not to start from scratch (error-prone) but have some support from the library
    </span>
   </li>
   <li>
    <span>
     Code readability/maintainability: possibly using quite high level operations, no reinventing the wheel (in a buggy version)
    </span>
   </li>
   <li>
    <span>
     Quite good performances
    </span>
   </li>
  </ul>
  <p>
   <span>
    Considering those objectives, the choice has been to use the native TensorFlow APIs for building the solution, using the seq2seq package for faster implementation.
   </span>
  </p>
  <h3 id="h.gj9dqxa5vak1">
   <span>
    Graph details
   </span>
  </h3>
  <p>
   <span>
    [LABEL:implementationNNDetails]
   </span>
  </p>
  <p>
   <span>
    Implementing a neural network with tensorflow needs understanding some basic concepts.
   </span>
  </p>
  <p>
   <span>
    First of all, the difference between defining a computational graph and actually doing operations. This is the concept of
   </span>
   <span>
    tensors
   </span>
   <span>
    : they are a generalization of (multidimensional) vectors that will be used to contain values. Using and connecting them in computational graphs means defining how the values contained will be processed. The big difference with normal variables is when the computations defined will be executed.
   </span>
  </p>
  <p>
   <span>
    If in a programming language we have two variables
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=a%3D3%2Cb%3D1"/>
   <span>
    and we sum them, when the instruction of sum is executed
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=c%3Da%2Bb"/>
   <span>
    the resulting variable will contain the result
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=c%3D4"/>
   <span>
    . Instead defining a sum between tensors
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=a%3Dtf.Variable%283%29%2Cb%3Dtf.Variable%281%29"/>
   <span>
    , when the instruction of
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=c%3Da%2Bb"/>
   <span>
    is executed the resulting tensor won’t contain 4. It will be stored that c can be computed by summing what is contained in a and b. In this way the operations can be defined once and the same computational graph can be run different times. To actually run the graph is necessary to build a session, that is the execution environment.
   </span>
  </p>
  <p>
   <span>
    But in order to have different inputs to the same graph, there is the need to define the starting tensors not as Variables but as placeholders. Later when running the session, the placeholder values can be fed to the graph and the results can be evaluated.
   </span>
  </p>
  <p>
   <span>
    When using multi-dimensional tensors, the tricky part is to manipulate them in the correct way. A lot of tensorflow operations have the “axes” parameter that is crucial to perform reduction operation over the correct dimensions.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    As has been discussed in the approach part, the chosen graph to be used in single-turn interactions is the encoder-decoder joint approach by Liu et al.
   </span>
   <sup>
    <a href="#ftnt162" id="ftnt_ref162">
     [162]
    </a>
   </sup>
   <span>
    .
   </span>
  </p>
  <p>
   <span>
    The papers come with an implementation
   </span>
   <span>
    <a href="https://github.com/HadoopIt/rnn-nlu">
     https://github.com/HadoopIt/rnn-nlu
    </a>
   </span>
   <span>
    that is not complete: output label dependencies are not considered. Another negative property of the implementation is that it does not use the latest API available, that provides higher level functionalities and helper functions. Another implementation has been found
   </span>
   <span>
    <a href="https://github.com/applenob/RNN-for-Joint-NLU">
     https://github.com/applenob/RNN-for-Joint-NLU
    </a>
   </span>
   <span>
    that uses the latest API and is really a lot more understandable that the original one.
   </span>
  </p>
  <p>
   <span>
    TODO say something about the seq2seq package and how can be used
   </span>
  </p>
  <p>
   <span>
    Starting from this last implementation, that highly depend on the ATIS dataset
   </span>
   <sup>
    <a href="#ftnt163" id="ftnt_ref163">
     [163]
    </a>
   </sup>
   <span>
    in the preprocessing part, a slightly different implementation has been developed.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h4 id="h.jspx6kh9zcn4">
   <span>
    The differences
   </span>
  </h4>
  <p>
   <span>
    The main changes are not on the network structure, that is the one of the original paper, but in some details about the inputs.
   </span>
  </p>
  <p>
   <span>
    First of all, the inputs of the computational graph have been changed from word indexes to string tensors (that exist in TensorFlow although its math-preponderance). In this way the dictionaries are part of the graph. Translation from word to indexes and inverse is possible thanks to the package tf.contrib.lookup that can be used to implement lookup tables for both the directions (words to indexes and indexes to words).
   </span>
  </p>
  <p>
   <span>
    Then, having the lookup operations inside the computational graph and having the string tensors as inputs, it has been possible to define different ways to obtain the embedding values with the same interface. Both preceding implementations contain the embedding matrix as a trainable Variable with size
   </span>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5Bvocabulary%5C_size%5C+x%5C+embedding%5C_size%5D"/>
   <span>
    , and this embedding technique has been moved in a specific class. The other way to obtain the embedding values, that corresponds to pretrained fixed values, is to use an external library (SpaCy, see the details and implications below in word embeddings section [REF:implementationWV]) to get those values. The library requires words as parameters to get their representation, and it would not have possible to pass them the word ids as in the previous implementation.
   </span>
  </p>
  <p>
   <span>
    Having another class that wraps the external library call to get the embedding vectors, it is possible to test the two different approaches (trainable embeddings versus fixed pretrained embeddings) and some measures have been done showing a boost given by the second approach empowered by big corpuses.
   </span>
  </p>
  <p>
   <span>
    The last kind of modifications that have been done are relative to saving the models trained and being able to resume them for inference time. This is possible thanks to the naming of tensors, that allows to save the computational graph to the disk and resume it without re-defining all the variables that were present previously. To interact with the graph, it is possible to retrieve the input and output tensors by name and all the intermediate steps will be internally managed by tensorflow. As will be underlined in integrating spacy in the graph in [REF:implementationSpacyPyFunc], the only parts that cannot be serialized are the external functions, but a solution exists also for this problem.
   </span>
  </p>
  <h4 id="h.l1f0mzomvabv">
   <span>
    Multi-turn
   </span>
  </h4>
  <p>
   <span>
    Is there something to tell specifically to the implementation of multi-turn? Is it detailed enough in Approach? Maybe talk about why propagating the intent value and not the logits in top RNN
   </span>
  </p>
  <h3 id="h.gtjq3t21gyb3">
   <span>
    Word embeddings
   </span>
  </h3>
  <p>
   <span>
    [LABEL:implementationWV]
   </span>
  </p>
  <p>
   <span>
    TODO introduce why SpaCy (also because used in early experimentation with entities)
   </span>
  </p>
  <p>
   <span>
    Advantages of the spacy models (size, performance)
   </span>
  </p>
  <p>
   <span>
    A library for common NLP tasks, SpaCy, has been used for tokenization and for pretrained word embeddings on the english language. The corpus used comes from CommonCrawl, and the word embeddings have a size of 300. Different models come with the library, and they change how much space the word embeddings take. The biggest one, used in the implementation, contains 1.1m of words in the dictionary. Smaller models apply a reduction on the number of vectors and on the dictionary size; the mapping between those two dimensions is not always one-to-one: different words can be mapped on the same vector for further reduction in space.
   </span>
  </p>
  <h4 id="h.hicafqiwqr81">
   <span>
    The tokenization of spaCy
   </span>
  </h4>
  <p>
   <span>
    TODO
   </span>
  </p>
  <h4 id="h.veqzygdxclk">
   <span>
    Integrating SpaCy word vectors in the computational graph
   </span>
  </h4>
  <p>
   <span>
    [LABEL:implementationSpacyPyFunc]
   </span>
  </p>
  <p>
   <span>
    To integrate the embeddings values provided by this library in the computational graph written in tensorflow, the `py_func` method has been used. In this way a function declared in definition time will be called in runtime: tensorflow manages this by making the py_func wrapper work externally on tensors and internally with numpy arrays.
   </span>
  </p>
  <p>
   <span>
    Issue of serialization and solution
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h4 id="h.rhub0ifx6or2">
   <span>
    Italian word embeddings
   </span>
  </h4>
  <p>
   <span>
    For the italian language, SpaCy has an incomplete model that does not contain word vectors. For this reason, a side work has been done, taking the wikipedia corpus and computing GloVe vectors on it. Some preprocessing has been done to extract the text only of the wikipedia dumps (available at
   </span>
   <span>
    <a href="https://dumps.wikimedia.org/itwiki/">
     https://dumps.wikimedia.org/itwiki/
    </a>
   </span>
   <span>
    ) with a tool written at Pisa University (
   </span>
   <span>
    <a href="http://medialab.di.unipi.it/wiki/Wikipedia_Extractor">
     http://medialab.di.unipi.it/wiki/Wikipedia_Extractor
    </a>
   </span>
   <span>
    ), and eventually changing the whitespace separation of words in order to align it to the tokenization done by SpaCy. In this way the dictionary used in the training of the word embeddings is the same that is used by the computational graph (tokenization: how the input sentences are separated in single words)
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Look at commoncrawl preprocessed corpus
   </span>
   <span>
    <a href="http://data.statmt.org/ngrams/deduped/">
     http://data.statmt.org/ngrams/deduped/
    </a>
   </span>
   <span>
    76GB
   </span>
  </p>
  <p>
   <span>
    <a href="http://slides.com/smerity/common-crawl-for-nlp%23/">
     http://slides.com/smerity/common-crawl-for-nlp#/
    </a>
   </span>
   <span>
   </span>
  </p>
  <h3 id="h.x106sk22d6zg">
   <span>
    Datasets collection
   </span>
  </h3>
  <p>
   <span>
    [LABEL:implementationDatasets]
   </span>
  </p>
  <p>
   <span>
    From wit.ai exploitation for single turn, collection of BotCycle singleturn.
   </span>
  </p>
  <p>
   <span>
    Talk about the sessions collection process and the resulting dataset
   </span>
  </p>
  <p>
   <span>
    See also validation
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <hr style="page-break-before:always;display:none;"/>
  <p>
   <span>
   </span>
  </p>
  <h1 id="h.ugfb055fre9a">
   <span>
    Validation
   </span>
  </h1>
  <p>
   <span>
    [LABEL:validation]
   </span>
  </p>
  <p>
   <span>
    “How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation”
   </span>
  </p>
  <p>
   <span>
    This paper analyzes metrics for
   </span>
   <span>
    unsupervised
   </span>
   <span>
    dialogue systems -&gt; not task focused. Correlation between automatic metrics and human ratings
   </span>
  </p>
  <h2 id="h.fv6027fmilty">
   <span>
    NLU evaluation
   </span>
  </h2>
  <p>
   <span>
    [LABEL:validationNLU]
   </span>
  </p>
  <p>
   <span>
    The Natural Language Understanding, because of its central role in this work because of the deep exploration of literature performed and also because of the proposed approach for multi-turn, requires a detailed and dedicated evaluation for its performance. This subsection therefore will describe the datasets used (TODO add refs), the evaluation protocol and the measures analysis.
   </span>
  </p>
  <h3 id="h.x01t27gnrlp8">
   <span>
    Datasets
   </span>
  </h3>
  <p>
   <span>
    [LABEL:validationDatasets]
   </span>
  </p>
  <p>
   <span>
    In the search of datasets available for the Natural Language Understanding, two main types have been found, respectively for single-turn interaction and for multi-turn interactions.
   </span>
   <sup>
    <a href="#cmnt6" id="cmnt_ref6">
     [f]
    </a>
   </sup>
  </p>
  <h4 id="h.8wekmvpcm5ph">
   <span>
    Single turn
   </span>
  </h4>
  <h5 id="h.1wcjylw42o8c">
   <span>
    ATIS
   </span>
  </h5>
  <p>
   <span>
    The first dataset, used historically for the tasks of intent detection and slot filling, is Air Travel Information System (ATIS)
   </span>
   <sup>
    <a href="#ftnt164" id="ftnt_ref164">
     [164]
    </a>
   </sup>
   <span>
    . This dataset contains transcriptions of questions obtained from
   </span>
  </p>
  <p>
   <span>
    the Official Airline Guide (OAG, 1990): each question is annotated with its intent, over 18 different intent types, and with the relative slots, over 127 slot types. The slot types correspond to entity types with a specific role: for example the slot type “fromloc.city_name” corresponds to the entity type “city_name” with the role “fromloc”. The dataset has 4978 training samples and 893 validation samples: the original division is only in two splits, no “development” set is available. However some works have additionally divided the the train set into two parts to leave the validation set on its own for the final evaluation and not for the hyperparameters optimization.
   </span>
  </p>
  <h5 id="h.ky245rjhml6x">
   <span>
    NLU Benchmark
   </span>
  </h5>
  <p>
   <span>
    Another dataset, that have been used always in the specificity of single turned interactions, is the nlu-benchmark that was published by Snips
   </span>
   <sup>
    <a href="#ftnt165" id="ftnt_ref165">
     [165]
    </a>
   </sup>
   <span>
    as an effort to compare and evaluate different NLU commercial providers like Amazon’s Alexa
   </span>
   <sup>
    <a href="#ftnt166" id="ftnt_ref166">
     [166]
    </a>
   </sup>
   <span>
    , Google’s Api.ai
   </span>
   <sup>
    <a href="#ftnt167" id="ftnt_ref167">
     [167]
    </a>
   </sup>
   <span>
    , Microsoft’s Luis
   </span>
   <sup>
    <a href="#ftnt168" id="ftnt_ref168">
     [168]
    </a>
   </sup>
   <span>
    , Apple’s SiriKit
   </span>
   <sup>
    <a href="#ftnt169" id="ftnt_ref169">
     [169]
    </a>
   </sup>
   <span>
    , IBM’s Watson
   </span>
   <sup>
    <a href="#ftnt170" id="ftnt_ref170">
     [170]
    </a>
   </sup>
   <span>
    , Facebook’s Wit
   </span>
   <sup>
    <a href="#ftnt171" id="ftnt_ref171">
     [171]
    </a>
   </sup>
   <span>
    , and Snips
   </span>
   <sup>
    <a href="#ftnt172" id="ftnt_ref172">
     [172]
    </a>
   </sup>
   <span>
    itself. The dataset used for their benchmark
   </span>
   <sup>
    <a href="#ftnt173" id="ftnt_ref173">
     [173]
    </a>
   </sup>
   <span>
    is composed of 7 different intents
   </span>
  </p>
  <p>
   <span>
    TODO look at
   </span>
   <span>
    <a href="https://github.com/sebischair/NLU-Evaluation-Corpora">
     https://github.com/sebischair/NLU-Evaluation-Corpora
    </a>
   </span>
   <span>
    and integrate it in the datasets? There exists a paper for this dataset, see on the repo github.
   </span>
  </p>
  <h5 id="h.n6d2unjva2y0">
   <span>
    Wit.ai
   </span>
  </h5>
  <p>
   <span>
    More specific to the prototype built for this project, BotCycle, two single-turn datasets have been collected with the support of wit.ai platform: one in english and one in italian. The collection of samples begun with the early stages of the prototype, that was relying on wit.ai for the understanding part. This platform provides a friendly GUI where the intents and slots can be configured, and also some training samples can be provided. Moreover it keeps track of the “inbox” messages used in operational setup, that can be later reviewed and added to the gold standard. Even when the understanding part switched to local deployment, we kept sending the requests for processing to wit.ai in order to continue to increase the dataset. WIth this technique we collected 517
   </span>
   <span>
    (TODO update with final stats)
   </span>
   <span>
    samples for the english language and 91 samples for the italian one.
   </span>
  </p>
  <h4 id="h.xa7exm60qpn">
   <span>
    Multi turn
   </span>
  </h4>
  <p>
   <span>
    Instead for the multi turn NLU, that is described in (TODO add reference to the approach for multi turn proposed in HQA2018), there have been used less datasets.
   </span>
  </p>
  <h5 id="h.gxt0s3umpmaj">
   <span>
    Key-Value Retrieval
   </span>
  </h5>
  <p>
   <span>
    The only dataset found in literature is the Key-Value Retrieval
   </span>
   <sup>
    <a href="#ftnt174" id="ftnt_ref174">
     [174]
    </a>
   </sup>
   <span>
    , that contains sessions of interaction between a driver and his smart car. Each session begins with an utterance of the driver that clearly states his intent (out of 3 possible values: Calendar Scheduling, Weather Information Retrieval, POI Navigation), and the subsequent turns of both parties continue the dialogue to refine the search and to reach the desired final response. About the annotation of the slots, they are available but not annotated in a straightforward way: each slot (15 types available) is stored with its value, but there are some problems in identifying their displacement in the sentences, because a normalization step (entity linking is missing).
   </span>
  </p>
  <ul>
   <li>
    <span>
     trailing spaces in the slot annotation: solved by trimming strings;
    </span>
   </li>
   <li>
    <span>
     capitalization differences between slot annotation and text of the sentence: solved by applying match insensitive;
    </span>
   </li>
   <li>
    <span>
     slots annotated on later turns referring to previous turns: solved by reversing the look-up processing order on the sentences and annotations, in order to enable backward references to text;
    </span>
   </li>
   <li>
    <span>
     multiple matches for the same word in a sentence: by keeping the forward slots to be referenced, sometimes multiple entities overlap partially in the sentences, that is not allowed by the IOB annotation scheme;
    </span>
   </li>
   <li>
    <span>
     incomplete slot value: for example “take my pills” in the sentence is annotated as “take pills”, and this is not solvable by simple look-up strategies;
    </span>
   </li>
   <li>
    <span>
     entity resolution problems:  for example the reference “there” is annotated with its resolution (the place mentioned before), or “today” in the text is annotated as “wednesday” in the slots;
    </span>
   </li>
   <li>
    <span>
     other entity resolution problems that could be solved by looking at the attached Knowledge Base: those are not resolved since it is not the only limitation;
    </span>
   </li>
  </ul>
  <p>
   <span>
    The selected approach, while providing also outputs for the slot labels being trained jointly, is here analyzed mainly under the point of view of the intents, so this is actually not a problem. The main goal of the multi-turn approach is to be able to automatically recognize when a certain session ends or continue, where time-based session split can be misleading and a working solution should rely on the contents only.
   </span>
  </p>
  <p>
   <span>
    This dataset, therefore, satisfies our needs: each sentence is annotated with its speaker and the intent values are available. The preprocessing is composed of three steps: 1 annotation of the intent from session-level to sentence-level by copying the values; 2 concatenation of all the sentences, removing the concept of session that remains only on the intent values; 3 consider as samples only the driver sentences, each one stored together with the current and previous intent value and with the previous sentence of the agent.
   </span>
  </p>
  <h5 id="h.ftgmlmugryqn">
   <span>
    BotCycle multi turn
   </span>
  </h5>
  <p>
   <span>
    As for the single turn, a targeted dataset is needed for the specific field of application relatively for the bike sharing domain. For this reason, all the conversations between the user and the bot have been recorded, and from them some sessions have been cleaned up, eventually correcting the predicted outputs. The conversations have been generated from a restricted team of testers that knew the limitations of the system and focused the interaction on the actual types of things the system is able to do. Through this technique it has been possible to collect XXX training sessions with an average length of XXX (TODO complete those values).
   </span>
  </p>
  <h4 id="h.2xcyfcesv60j">
   <span>
    Italian Word Embeddings
   </span>
  </h4>
  <p>
   <span>
    Wikipedia it
   </span>
  </p>
  <p>
   <span>
    TODO
   </span>
  </p>
  <h3 id="h.cf6oldpt7ig8">
   <span>
    Evaluation Protocol
   </span>
  </h3>
  <p>
   <span>
    [LABEL:validationMeasures]
   </span>
  </p>
  <p>
   <span>
    To evaluate the performance of the computational graph, a search of commonly used measures has been done. For the ATIS dataset usually
   </span>
   <sup>
    <a href="#ftnt175" id="ftnt_ref175">
     [175]
    </a>
   </sup>
   <span>
    the performance is evaluated by computing the intent error rate and the F1 measure for the slot filling. It is important to notice that in classification tasks for which every test case is guaranteed to be assigned to exactly one class, F-measure on micro level (globally counting TP,FN,FP) is equivalent to accuracy.
   </span>
  </p>
  <p>
   <span>
    So for both settings (single turn and multi-turn) the measures chosen to evaluate the results are the following:
   </span>
  </p>
  <ul>
   <li>
    <span>
     F1 measure on the intents
    </span>
   </li>
   <li>
    <span>
     F1 measure for the slots (a slot is correctly detected if both type and start and end are correct)
    </span>
   </li>
   <li>
    <span>
     F1 measure on the IOB annotation level
    </span>
    <span>
     highly unbalanced because ‘O’ annotation is prevalent
    </span>
   </li>
  </ul>
  <p>
   <span>
    For the comparison different approaches have been considered in order to evaluate several choices:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Word embeddings
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Pretrained
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     EN model size (wins large model)
    </span>
   </li>
   <li>
    <span>
     IT cnr vs spacy tokenization (wins spacy)
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Trained from scratch vs pretrained spacy large (wins pretrained spacy large)
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Single turn variations
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     LSTM vs GRU
    </span>
   </li>
   <li>
    <span>
     Attention vs non attention
    </span>
   </li>
   <li>
    <span>
     Intent decoder rnn vs last state of bidirectional encoder
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Multi-turn variations
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     LSTM vs GRU vs CRF on top
    </span>
   </li>
   <li>
    <span>
     Consider bot turn vs no bot turn
    </span>
   </li>
   <li>
    <span>
     Consider previous intent vs not consider
    </span>
   </li>
   <li>
    <span>
     Word-level CRF
    </span>
   </li>
  </ul>
  <h3 id="h.iytfenmdhzr5">
   <span>
    Results
   </span>
  </h3>
  <p>
   <span>
    [LABEL:validationResults]
   </span>
  </p>
  <p>
   <span>
    The results of the evaluation
   </span>
  </p>
  <h4 id="h.ystjzzsyuhmx">
   <span>
    Word embeddings
   </span>
  </h4>
  <p>
   <span>
    TODO Evaluation of word2vec: “italian Word Embeddings Go to Italy: a Comparison of Models and Training Datasets”
   </span>
   <sup>
    <a href="#ftnt176" id="ftnt_ref176">
     [176]
    </a>
   </sup>
   <span>
    see for understanding if those wv computed are better than  39.91% → YES: 58.14%
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Total sentences: 306, Correct: 96.41%, Incorrect: 3.59%, Set: capital-common-countries
   </span>
  </p>
  <p>
   <span>
    Total sentences: 915, Correct: 84.70%, Incorrect: 15.30%, Set: capital-world
   </span>
  </p>
  <p>
   <span>
    Total sentences: 106, Correct: 1.89%, Incorrect: 98.11%, Set: currency
   </span>
  </p>
  <p>
   <span>
    Total sentences: 782, Correct: 28.64%, Incorrect: 71.36%, Set: city-in-state
   </span>
  </p>
  <p>
   <span>
    Total sentences: 342, Correct: 40.06%, Incorrect: 59.94%, Set: regione capoluogo
   </span>
  </p>
  <p>
   <span>
    Total sentences: 272, Correct: 71.69%, Incorrect: 28.31%, Set: family
   </span>
  </p>
  <p>
   <span>
    Total sentences: 506, Correct: 10.28%, Incorrect: 89.72%, Set: gram1-adjective-to-adverb
   </span>
  </p>
  <p>
   <span>
    Total sentences: 30, Correct: 16.67%, Incorrect: 83.33%, Set: gram2-opposite
   </span>
  </p>
  <p>
   <span>
    Total sentences: 12, Correct: 8.33%, Incorrect: 91.67%, Set: gram3-comparative
   </span>
  </p>
  <p>
   <span>
    Total sentences: 57, Correct: 42.11%, Incorrect: 57.89%, Set: gram4-superlative (assoluto)
   </span>
  </p>
  <p>
   <span>
    Total sentences: 342, Correct: 61.11%, Incorrect: 38.89%, Set: gram5-present-participle (gerundio)
   </span>
  </p>
  <p>
   <span>
    Total sentences: 1371, Correct: 82.57%, Incorrect: 17.43%, Set: gram6-nationality-adjective
   </span>
  </p>
  <p>
   <span>
    Total sentences: 420, Correct: 42.14%, Incorrect: 57.86%, Set: gram7-past-tense
   </span>
  </p>
  <p>
   <span>
    Total sentences: 756, Correct: 43.39%, Incorrect: 56.61%, Set: gram8-plural
   </span>
  </p>
  <p>
   <span>
    Total sentences: 273, Correct: 79.85%, Incorrect: 20.15%, Set: gram9-plural-verbs (3rd person)
   </span>
  </p>
  <p>
   <span>
    Error for Set gram10-plural-verbs (1st person)
   </span>
  </p>
  <p>
   <span>
    Error for Set gram11-remote-past-verbs (1st person)
   </span>
  </p>
  <p>
   <span>
    Total sentences: 132, Correct: 58.33%, Incorrect: 41.67%, Set: gram12-noun-masculine-feminine-singular
   </span>
  </p>
  <p>
   <span>
    Total sentences: 2, Correct: 0.00%, Incorrect: 100.00%, Set: gram13-noun-masculine-feminine-plural
   </span>
  </p>
  <p>
   <span>
    Total sentences: 6624, Correct: 58.14%, Incorrect: 41.86%, Set: total
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Keeping the same network, show here boost on different datasets given by:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Pretrained word vectors and effect of spacy model size with respect to embedding layer
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Instead of trainable from scratch, pretrained on bigger corpus
    </span>
   </li>
   <li>
    <span>
     Improvement of results with pre-trained on ATIS/+others dataset
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Effect of better tokenization in italian language and comparison with
    </span>
    <span>
     <a href="http://hlt.isti.cnr.it/wordembeddings/">
      http://hlt.isti.cnr.it/wordembeddings/
     </a>
    </span>
    <span>
    </span>
   </li>
  </ul>
  <h4 id="h.vybi8bw7eizb">
   <span>
    The multi-turn
   </span>
  </h4>
  <ul>
   <li>
    <span>
     Intent tracking capabilities (HQA2018)
    </span>
   </li>
   <li>
    <span>
     Entities?
    </span>
   </li>
  </ul>
  <h2 id="h.rjc0z5sumcgp">
   <span>
    System as a whole
   </span>
  </h2>
  <p>
   <span>
    [LABEL:validationWhole]
   </span>
  </p>
  <p>
   <span>
    Ground-truth based (comparing with a predetermined output. Using some already available datasets, but domain problems)
   </span>
  </p>
  <ul>
   <li>
    <span>
     Continuous sequences of correct actions from the beginning of the dialog
    </span>
   </li>
   <li>
    <span>
     Dialogue success rate
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Objective measures:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Duration of conversation
    </span>
   </li>
   <li>
    <span>
     Length of sentences
    </span>
   </li>
   <li>
    <span>
     Uptime of the bot
    </span>
   </li>
   <li>
    <span>
     Errors in time unit
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Detecting from chat:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Sentiment detection: not always applicable (“cold sentences”)
    </span>
   </li>
   <li>
    <span>
     User feedback: ok/thanks
    </span>
   </li>
   <li>
    <span>
     Thumbs up/down at the end of conversation → must be shown to user
    </span>
   </li>
   <li>
    <span>
     User repeats question → means the system didn’t catch it, but dangerous, maybe user wanted again updated data (can distinguish on time passed: short time could show that system didn’t catch)
    </span>
   </li>
   <li>
    <span>
     Not recognised intent→ could be
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Out of domain
    </span>
   </li>
   <li>
    <span>
     Into the domain but unforeseen
    </span>
   </li>
   <li>
    <span>
     Number of error messages
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     System turn duration (to generate a response)
    </span>
   </li>
   <li>
    <span>
     Task completion time (could compare against existing app)
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Survey-based:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Overall evaluation:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Usefulness
    </span>
   </li>
   <li>
    <span>
     Usability
    </span>
   </li>
   <li>
    <span>
     Relevance of results
    </span>
   </li>
   <li>
    <span>
     Missing features
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Per-interaction feedback (C. Chakrabarti, G.F. Luger / Expert Systems with Applications 42 (2015) 6878–6897):
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Grice’s maxims:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Quality: information is good
    </span>
   </li>
   <li>
    <span>
     Quantity: appropriate quantity of information
    </span>
   </li>
   <li>
    <span>
     Relation: relevant to context and to the topic of conversation
    </span>
   </li>
   <li>
    <span>
     Manner: direct and straightforward
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Solving problems
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     The bot asked the required information
    </span>
   </li>
   <li>
    <span>
     The bot kept conversation on-topic (coherence)
    </span>
   </li>
   <li>
    <span>
     The bot solved issue
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Evaluation of personalization: when giving a personalized response, check if user prefers the basic one or the customized one
    </span>
    <hr style="page-break-before:always;display:none;"/>
   </li>
  </ul>
  <h1 id="h.ibkj9td9ecq0">
   <span>
    Conclusion
   </span>
  </h1>
  <p>
   <span>
    [LABEL:conclusion]
   </span>
  </p>
  <ul>
   <li>
    <span>
     Point out what has been achieved: competences, multi-turn approach?
    </span>
   </li>
   <li>
    <span>
     Next works that could be done:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Integrating with stationless systems?
    </span>
   </li>
   <li>
    <span>
     NLU:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Stronger entity resolution
    </span>
   </li>
   <li>
    <span>
     More structured dialogues
    </span>
   </li>
   <li>
    <span>
     Combination with a generative approach for chit-chat?
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Personalization:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Actual implementation
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <div>
   <p>
    <span>
    </span>
   </p>
  </div>
  <hr/>
  <div>
   <p>
    <a href="#ftnt_ref1" id="ftnt1">
     [1]
    </a>
    <span>
     Key:pennington2014glove Pennington, J., Socher, R., &amp; Manning, C. (2014). Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) (pp. 1532-1543).
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref2" id="ftnt2">
     [2]
    </a>
    <span>
     Key:goldberg1993structure Goldberg, L. R. (1993). The structure of phenotypic personality traits. American Psychologist. 48: 26–34. doi:10.1037/0003-066x.48.1.26
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref3" id="ftnt3">
     [3]
    </a>
    <span>
     Key:liu2016attention Liu, B. and Lane, I. (2016). Attention-based recurrent neural network models for joint intent detection and slot filling. Proceedings of The 17th Annual Meeting of the International Speech Communication Association
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref4" id="ftnt4">
     [4]
    </a>
    <span>
    </span>
    <span>
     <a href="https://dumps.wikimedia.org/">
      https://dumps.wikimedia.org/
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref5" id="ftnt5">
     [5]
    </a>
    <span>
     https://www.polito.it/
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref6" id="ftnt6">
     [6]
    </a>
    <span>
     http://www.ismb.it/
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref7" id="ftnt7">
     [7]
    </a>
    <span>
     Key:walsh2016turing Walsh, T. (2016).
    </span>
    <span>
     Turing’s red flag
    </span>
    <span>
     . Communications of the ACM
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref8" id="ftnt8">
     [8]
    </a>
    <span>
     Key:hibbard2014ethical Hibbard, B. (2014). Ethical artificial intelligence. arXiv preprint arXiv:1411.1373.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref9" id="ftnt9">
     [9]
    </a>
    <span>
     Key:moor2009four Moor, J. (2009). Four kinds of ethical robots. Philosophy Now, 72, 12-14.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref10" id="ftnt10">
     [10]
    </a>
    <span>
     Key:clarke2011asimov Clarke, R. (2011). Asimov’s Laws of Robotics: Implications for information technology. Machine Ethics, 254-84.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref11" id="ftnt11">
     [11]
    </a>
    <span>
     Key:minsky1952neural Minsky, M. (1952). A neural-analogue calculator based upon a probability model of reinforcement. Harvard University Psychological Laboratories, Cambridge, Massachusetts.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref12" id="ftnt12">
     [12]
    </a>
    <span>
     Key:mcculloch1943logical McCulloch, W. S., &amp; Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity. The bulletin of mathematical biophysics, 5(4), 115-133.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref13" id="ftnt13">
     [13]
    </a>
    <span>
     Key:rosenblatt1958perceptron Rosenblatt, F. (1958). The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review, 65(6), 386.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref14" id="ftnt14">
     [14]
    </a>
    <span>
     Key:turing1950computing Turing, A. (1950).
    </span>
    <span>
     Computing Machinery and Intelligence
    </span>
    <span>
     . Mind
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref15" id="ftnt15">
     [15]
    </a>
    <span>
     Key:ekbia2014heteromation Ekbia, H., &amp; Nardi, B. (2014). Heteromation and its (dis) contents: The invisible division of labor between humans and machines. First Monday, 19(6).
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref16" id="ftnt16">
     [16]
    </a>
    <span>
     Key:lecun2015deep LeCun, Y., Bengio, Y., &amp; Hinton, G. (2015). Deep learning. nature, 521(7553), 436.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref17" id="ftnt17">
     [17]
    </a>
    <span>
     Key:tian2005facial Tian, Y. L., Kanade, T., &amp; Cohn, J. F. (2005). Facial expression analysis. In Handbook of face recognition (pp. 247-275). Springer, New York, NY.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref18" id="ftnt18">
     [18]
    </a>
    <span>
    </span>
    <span>
     <a href="https://www.wired.com/story/new-kepler-exoplanet-90i-discovery-fueled-by-ai/">
      https://www.wired.com/story/new-kepler-exoplanet-90i-discovery-fueled-by-ai/
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref19" id="ftnt19">
     [19]
    </a>
    <span>
    </span>
    <span>
     <a href="https://blog.google/products/translate/found-translation-more-accurate-fluent-sentences-google-translate/">
      https://blog.google/products/translate/found-translation-more-accurate-fluent-sentences-google-translate/
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref20" id="ftnt20">
     [20]
    </a>
    <span>
     Key:bengio2003neural Bengio, Y., Ducharme, R., Vincent, P., &amp; Jauvin, C. (2003). A neural probabilistic language model. Journal of machine learning research, 3(Feb), 1137-1155.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref21" id="ftnt21">
     [21]
    </a>
    <span>
     Key:sahlgren2008distributional Sahlgren, M. (2008). The distributional hypothesis. Italian Journal of Disability Studies, 20, 33-53.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref22" id="ftnt22">
     [22]
    </a>
    <span>
     Key:chouard2016go Chouard, T. (2016). The Go files: AI computer wraps up 4–1 victory against human champion. Nature, doi, 10.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref23" id="ftnt23">
     [23]
    </a>
    <span>
     Key:brown2017superhuman Brown, N., &amp; Sandholm, T. (2017). Superhuman AI for heads-up no-limit poker: Libratus beats top professionals. Science, eaao1733.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref24" id="ftnt24">
     [24]
    </a>
    <span>
     Key:mnih2015human Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... &amp; Petersen, S. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref25" id="ftnt25">
     [25]
    </a>
    <span>
     Key:xu2015show Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., ... &amp; Bengio, Y. (2015, June). Show, attend and tell: Neural image caption generation with visual attention. In International Conference on Machine Learning (pp. 2048-2057).
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref26" id="ftnt26">
     [26]
    </a>
    <span>
     Key:rumelhart1986learning Rumelhart, D. E., Hinton, G. E., &amp; Williams, R. J. (1986). Learning representations by back-propagating errors. nature, 323(6088), 533.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref27" id="ftnt27">
     [27]
    </a>
    <span>
     Key:bottou2010large Bottou, L. (2010). Large-scale machine learning with stochastic gradient descent. In Proceedings of COMPSTAT'2010 (pp. 177-186). Physica-Verlag HD.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref28" id="ftnt28">
     [28]
    </a>
    <span>
     Key:chouard2016go Chouard, T. (2016). The Go files: AI computer wraps up 4–1 victory against human champion. Nature, doi, 10.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref29" id="ftnt29">
     [29]
    </a>
    <span>
     Key:athalye2018obfuscated Athalye, A., Carlini, N., &amp; Wagner, D. (2018). Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref30" id="ftnt30">
     [30]
    </a>
    <span>
    </span>
    <span>
     <a href="https://www.wired.com/story/ai-has-a-hallucination-problem-thats-proving-tough-to-fix/">
      https://www.wired.com/story/ai-has-a-hallucination-problem-thats-proving-tough-to-fix/
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref31" id="ftnt31">
     [31]
    </a>
    <span>
     Key:pearl2018theoretical Pearl, J. (2017).
    </span>
    <span>
     Theoretical Impediments to Machine Learning
    </span>
    <span>
     . cse-lab.ethz.ch
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref32" id="ftnt32">
     [32]
    </a>
    <span>
     Key:gottfredson1997mainstream Gottfredson, L. S. (1997).
    </span>
    <span>
     Mainstream science on intelligence: An editorial with 52 signatories, history, and bibliography.
    </span>
    <span>
     Intelligence, 24, 13–23.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref33" id="ftnt33">
     [33]
    </a>
    <span>
     Key:lake2017building Lake, Brenden M and Ullman, Tomer D and Tenenbaum, Joshua B and Gershman, Samuel J. (2017). Building machines that learn and think like people. Behavioral and Brain Sciences
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref34" id="ftnt34">
     [34]
    </a>
    <span>
     Key:pearl2018theoretical Pearl, J. (2017). Theoretical Impediments to Machine Learning. cse-lab.ethz.ch
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref35" id="ftnt35">
     [35]
    </a>
    <span>
     Key:hawkins2017theory “A Theory of How Columns in the Neocortex Enable Learning the Structure of the World” J Hawkins, S Ahmad, Y Cui - Frontiers in Neural Circuits, 2017
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref36" id="ftnt36">
     [36]
    </a>
    <span>
     Key:lerer2016learning Lerer, Adam and Gross, Sam and Fergus, Rob. (2016). Learning physical intuition of block towers by example. arXiv preprint arXiv:1603.01312
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref37" id="ftnt37">
     [37]
    </a>
    <span>
     Key:hawkins2016neurons Hawkins, J. and Ahmad, S. (2016).
    </span>
    <span>
     Why Neurons Have Thousands of Synapses, A Theory of Sequence Memory in Neocortex
    </span>
    <span>
     . Frontiers in neural circuits
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref38" id="ftnt38">
     [38]
    </a>
    <span>
     Key:pearl2018theoretical Pearl, J. (2017). Theoretical Impediments to Machine Learning. cse-lab.ethz.ch
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref39" id="ftnt39">
     [39]
    </a>
    <span>
    </span>
    <span>
     <a href="http://www.fields.utoronto.ca/video-archive/static/2018/01/2509-17997/mergedvideo.ogv">
      http://www.fields.utoronto.ca/video-archive/static/2018/01/2509-17997/mergedvideo.ogv
     </a>
    </span>
    <span>
    </span>
    <span>
     <a href="https://openreview.net/pdf?id%3DS1Euwz-Rb">
      https://openreview.net/pdf?id=S1Euwz-Rb
     </a>
    </span>
    <span>
     currently under review in ICLR2018 “Compositional attention networks for machine reasoning”. See
    </span>
    <span>
     <a href="https://docs.google.com/document/d/1ls6sMXtnYstoRav6heKS17otIG_bKPVteShvTL-_7A8/edit">
      https://docs.google.com/document/d/1ls6sMXtnYstoRav6heKS17otIG_bKPVteShvTL-_7A8/edit
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref40" id="ftnt40">
     [40]
    </a>
    <span>
     Key:johnson2017clevr CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref41" id="ftnt41">
     [41]
    </a>
    <span>
     Key:muller2016future Müller, V., Bostrom, N. (2016).
    </span>
    <span>
     Future progress in artificial intelligence: A survey of expert opinion
    </span>
    <span>
     . Fundamental Issues of Artificial Intelligence (Synthese Library; Berlin: Springer).
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref42" id="ftnt42">
     [42]
    </a>
    <span>
     Human Brain Project,
    </span>
    <span>
     <a href="https://www.humanbrainproject.eu/en/">
      https://www.humanbrainproject.eu/en/
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref43" id="ftnt43">
     [43]
    </a>
    <span>
    </span>
    <span>
     <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID%3D0134732">
      https://www.nsf.gov/awardsearch/showAward?AWD_ID=0134732
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref44" id="ftnt44">
     [44]
    </a>
    <span>
    </span>
    <span>
     <a href="https://www.forbes.com/sites/zarastone/2017/11/07/everything-you-need-to-know-about-sophia-the-worlds-first-robot-citizen">
      https://www.forbes.com/sites/zarastone/2017/11/07/everything-you-need-to-know-about-sophia-the-worlds-first-robot-citizen
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref45" id="ftnt45">
     [45]
    </a>
    <span>
    </span>
    <span>
     <a href="https://www.theverge.com/2017/10/30/16552006/robot-rights-citizenship-saudi-arabia-sophia">
      https://www.theverge.com/2017/10/30/16552006/robot-rights-citizenship-saudi-arabia-sophia
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref46" id="ftnt46">
     [46]
    </a>
    <span>
    </span>
    <span>
     <a href="http://www.telegraph.co.uk/science/2016/03/12/meet-nadine-the-worlds-most-human-like-robot/">
      http://www.telegraph.co.uk/science/2016/03/12/meet-nadine-the-worlds-most-human-like-robot/
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref47" id="ftnt47">
     [47]
    </a>
    <span>
    </span>
    <span>
     <a href="https://www.theverge.com/2017/11/10/16617092/sophia-the-robot-citizen-ai-hanson-robotics-ben-goertzel">
      https://www.theverge.com/2017/11/10/16617092/sophia-the-robot-citizen-ai-hanson-robotics-ben-goertzel
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref48" id="ftnt48">
     [48]
    </a>
    <span>
     Key:pearl2018theoretical Pearl, J. (2017). Theoretical Impediments to Machine Learning. cse-lab.ethz.ch
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref49" id="ftnt49">
     [49]
    </a>
    <span>
     Key:hibbard2014ethical Hibbard, B. (2014). Ethical artificial intelligence. arXiv preprint arXiv:1411.1373.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref50" id="ftnt50">
     [50]
    </a>
    <span>
     Key:moor2009four Moor, J. (2009). Four kinds of ethical robots. Philosophy Now, 72, 12-14.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref51" id="ftnt51">
     [51]
    </a>
    <span>
     Key:clarke2011asimov Clarke, R. (2011). Asimov’s Laws of Robotics: Implications for information technology. Machine Ethics, 254-84.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref52" id="ftnt52">
     [52]
    </a>
    <span>
     Key:asimov1942runaround Asimov, I. (1942). Runaround. Astounding Science Fiction, 29(1), 94-103.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref53" id="ftnt53">
     [53]
    </a>
    <span>
    </span>
    <span>
     <a href="https://futureoflife.org/ai-principles/">
      https://futureoflife.org/ai-principles/
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref54" id="ftnt54">
     [54]
    </a>
    <span>
     Key:moor2009four Moor, J. (2009). Four kinds of ethical robots. Philosophy Now, 72, 12-14.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref55" id="ftnt55">
     [55]
    </a>
    <span>
     Key:hibbard2014ethical Hibbard, B. (2014). Ethical artificial intelligence. arXiv preprint arXiv:1411.1373.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref56" id="ftnt56">
     [56]
    </a>
    <span>
     Key:turing1950computing Turing, A. (1950). Computing Machinery and Intelligence. Mind
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref57" id="ftnt57">
     [57]
    </a>
    <span>
    </span>
    <span>
     <a href="http://www.aisb.org.uk/events/loebner-prize">
      http://www.aisb.org.uk/events/loebner-prize
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref58" id="ftnt58">
     [58]
    </a>
    <span>
     Key:walsh2016turing Walsh, T. (2016).
    </span>
    <span>
     Turing’s red flag
    </span>
    <span>
     . Communications of the ACM
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref59" id="ftnt59">
     [59]
    </a>
    <span>
     Key:rickards1817statutes (1865). Locomotive Act. The Statutes of the United Kingdom of Great Britain and Ireland
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref60" id="ftnt60">
     [60]
    </a>
    <span>
    </span>
    <span>
     <a href="https://www.apple.com/ios/siri/">
      https://www.apple.com/ios/siri/
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref61" id="ftnt61">
     [61]
    </a>
    <span>
     Key:asimov1942runaround Asimov, I. (1942). Runaround. Astounding Science Fiction, 29(1), 94-103.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref62" id="ftnt62">
     [62]
    </a>
    <span>
     Key:zlotowski2017can Zlotowski, J., Yogeeswaran, K., &amp; Bartneck, C. (2017).
    </span>
    <span>
     Can we control it? Autonomous robots threaten human identity, uniqueness, safety, and resources
    </span>
    <span>
     . International Journal of Human-Computer Studies
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref63" id="ftnt63">
     [63]
    </a>
    <span>
     Key:zlotowski2017can Zlotowski, J., Yogeeswaran, K., &amp; Bartneck, C. (2017).
    </span>
    <span>
     Can we control it? Autonomous robots threaten human identity, uniqueness, safety, and resources
    </span>
    <span>
     . International Journal of Human-Computer Studies
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref64" id="ftnt64">
     [64]
    </a>
    <span>
    </span>
    <span>
     <a href="https://replika.ai/">
      https://replika.ai/
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref65" id="ftnt65">
     [65]
    </a>
    <span>
     Key:picard2000affective Picard, R. W. (2000). Affective Computing. MIT Press.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref66" id="ftnt66">
     [66]
    </a>
    <span>
    </span>
    <span>
     <a href="http://kernelmag.dailydot.com/issue-sections/features-issue-sections/15708/addicting-apps-mobile-technology-health/">
      http://kernelmag.dailydot.com/issue-sections/features-issue-sections/15708/addicting-apps-mobile-technology-health/
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref67" id="ftnt67">
     [67]
    </a>
    <span>
     Key:konrath2011changes Konrath, S. H., O'Brien, E. H., &amp; Hsing, C. (2011). Changes in dispositional empathy in American college students over time: A meta-analysis. Personality and Social Psychology Review, 15(2), 180-198.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref68" id="ftnt68">
     [68]
    </a>
    <span>
     Key:furlong2008japanese Furlong, A. (2008). The Japanese hikikomori phenomenon: acute social withdrawal among young people. The sociological review, 56(2), 309-325.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref69" id="ftnt69">
     [69]
    </a>
    <span>
     Key:liu2016attention Attention joint SoA
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref70" id="ftnt70">
     [70]
    </a>
    <span>
    </span>
    <span>
     <a href="https://developer.amazon.com/alexa-skills-kit">
      https://developer.amazon.com/alexa-skills-kit
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref71" id="ftnt71">
     [71]
    </a>
    <span>
    </span>
    <span>
     <a href="https://developer.microsoft.com/en-us/cortana">
      https://developer.microsoft.com/en-us/cortana
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref72" id="ftnt72">
     [72]
    </a>
    <span>
     Key:franklin1996agent Franklin, S., &amp; Graesser, A. (1996, August). Is it an Agent, or just a Program?: A Taxonomy for Autonomous Agents. In International Workshop on Agent Theories, Architectures, and Languages (pp. 21-35). Springer, Berlin, Heidelberg.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref73" id="ftnt73">
     [73]
    </a>
    <span>
     Key:franklin1996agent Franklin, S., &amp; Graesser, A. (1996, August). Is it an Agent, or just a Program?: A Taxonomy for Autonomous Agents. In International Workshop on Agent Theories, Architectures, and Languages (pp. 21-35). Springer, Berlin, Heidelberg.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref74" id="ftnt74">
     [74]
    </a>
    <span>
     Jurafsky, D. (2017). Conversational Agents [PDF]. Retrieved from Stanford University course cs124
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref75" id="ftnt75">
     [75]
    </a>
    <span>
     Key:isbister2002design Isbister, K., &amp; Doyle, P. (2002, July). Design and evaluation of embodied conversational agents: A proposed taxonomy. In The first international joint conference on autonomous agents &amp; multi-agent systems.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref76" id="ftnt76">
     [76]
    </a>
    <span>
     Key:franklin1996agent Franklin, S., &amp; Graesser, A. (1996, August). Is it an Agent, or just a Program?: A Taxonomy for Autonomous Agents. In International Workshop on Agent Theories, Architectures, and Languages (pp. 21-35). Springer, Berlin, Heidelberg.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref77" id="ftnt77">
     [77]
    </a>
    <span>
     Key:franklin1996agent Franklin, S., &amp; Graesser, A. (1996, August). Is it an Agent, or just a Program?: A Taxonomy for Autonomous Agents. In International Workshop on Agent Theories, Architectures, and Languages (pp. 21-35). Springer, Berlin, Heidelberg.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref78" id="ftnt78">
     [78]
    </a>
    <span>
     Key:weizenbaum1966eliza Weizenbaum, J. (1966). ELIZA—a computer program for the study of natural language communication between man and machine. Communications of the ACM, 9(1), 36-45.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref79" id="ftnt79">
     [79]
    </a>
    <span>
     ELIZA can be easily tested inside the text editor Emacs with the command doctor (meta-x doctor).
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref80" id="ftnt80">
     [80]
    </a>
    <span>
     Key:shieber1994lessons Shieber, S. (1992). Lessons from a Restricted Turing Test.
    </span>
    <span>
     Technical Report TR-19-92, Harvard University
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref81" id="ftnt81">
     [81]
    </a>
    <span>
     Key:mauldin1994chatterbots Mauldin, M. L. (1994). Chatterbots, TinyMUDs and the Turing Test: Entering the Loebner Prize Competition.
    </span>
    <span>
     Proc. AAAI-94
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref82" id="ftnt82">
     [82]
    </a>
    <span>
    </span>
    <span>
     <a href="https://replika.ai/">
      https://replika.ai/
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref83" id="ftnt83">
     [83]
    </a>
    <span>
    </span>
    <span>
     <a href="https://woebot.io/">
      https://woebot.io/
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref84" id="ftnt84">
     [84]
    </a>
    <span>
     Key:fitzpatrick2017delivering Fitzpatrick, K. K., Darcy, A., &amp; Vierhile, M. (2017). Delivering cognitive behavior therapy to young adults with symptoms of depression and anxiety using a fully automated conversational agent (Woebot): a randomized controlled trial. JMIR mental health, 4(2).
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref85" id="ftnt85">
     [85]
    </a>
    <span>
     Key:hoffner2017survey Höffner, K., Walter, S., Marx, E., Usbeck, R., Lehmann, J., &amp; Ngonga Ngomo, A. C. (2017). Survey on challenges of question answering in the semantic web. Semantic Web, 8(6), 895-920.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref86" id="ftnt86">
     [86]
    </a>
    <span>
    </span>
    <span>
     <a href="https://developer.amazon.com/alexaprize">
      https://developer.amazon.com/alexaprize
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref87" id="ftnt87">
     [87]
    </a>
    <span>
    </span>
    <span>
     <a href="https://www.wired.com/story/inside-amazon-alexa-prize/">
      https://www.wired.com/story/inside-amazon-alexa-prize/
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref88" id="ftnt88">
     [88]
    </a>
    <span>
     Key:weizenbaum1966eliza Weizenbaum, J. (1966). ELIZA—a computer program for the study of natural language communication between man and machine. Communications of the ACM, 9(1), 36-45.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref89" id="ftnt89">
     [89]
    </a>
    <span>
    </span>
    <span>
     <a href="https://home.pandorabots.com/en/">
      https://home.pandorabots.com/en/
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref90" id="ftnt90">
     [90]
    </a>
    <span>
     Key:ghose2013toward Ghose, S., Barua, J. (2013). Toward the Implementation of a Topic Specific Dialogue-Based Natural Language Chatbot as an Undergraduate Advisor.
    </span>
    <span>
     Proceeds of the International Conference on Informatics, Electronics, and Vision
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref91" id="ftnt91">
     [91]
    </a>
    <span>
    </span>
    <span>
     <a href="https://rasa.ai/products/rasa-core/">
      https://rasa.ai/products/rasa-core/
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref92" id="ftnt92">
     [92]
    </a>
    <span>
     Key:ritter2011data Ritter, A., Cherry, C., Dolan, W. (2011). Data-driven response generation in social media.
    </span>
    <span>
     EMNLP
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref93" id="ftnt93">
     [93]
    </a>
    <span>
     Key:lowe2015ubuntu Lowe, R., Pow, N., Serban, I. V., &amp; Pineau, J. (2015, September). The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems. In 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue (p. 285).
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref94" id="ftnt94">
     [94]
    </a>
    <span>
     Key:serban2016building Serban, I. V., Sordoni, A., Bengio, Y., Courville, A. C., &amp; Pineau, J. (2016, February). Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models. In AAAI (Vol. 16, pp. 3776-3784).
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref95" id="ftnt95">
     [95]
    </a>
    <span>
    </span>
    <span>
     <a href="https://www.washingtonpost.com/news/the-intersect/wp/2016/03/24/the-internet-turned-tay-microsofts-fun-millennial-ai-bot-into-a-genocidal-maniac/">
      https://www.washingtonpost.com/news/the-intersect/wp/2016/03/24/the-internet-turned-tay-microsofts-fun-millennial-ai-bot-into-a-genocidal-maniac/
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref96" id="ftnt96">
     [96]
    </a>
    <span>
     Key:li2016persona Li, J., Galley, M., Brockett, C., Gao, J., Bill, D. (2016). A persona-based neural conversation model.
    </span>
    <span>
     ACL, pages 994–1003
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref97" id="ftnt97">
     [97]
    </a>
    <span>
     Key:li2015diversity Li, J., Galley, M., Brockett, C., Gao, J., and Dolan, B. (2015). A diversity-promoting objective function for neural conversation models.
    </span>
    <span>
     CoRR abs/1510.03055.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref98" id="ftnt98">
     [98]
    </a>
    <span>
     Key:kannan2016smart Kannan, A., Kurach, K., Ravi, S., Kaufmann, T., Tomkins, A., Miklos, B., Corrado, G., Lukacs, L., Ganea, M., et al. (2016). Smart reply: Automated response suggestion for email.
    </span>
    <span>
     ACM SIGKDD
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref99" id="ftnt99">
     [99]
    </a>
    <span>
     Key:eric2017key Eric, M. and Manning, C. (2017). Key-value retrieval networks for task-oriented dialogue. SIGDIAL 2017: Session on Natural Language Generation for Dialog Systems
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref100" id="ftnt100">
     [100]
    </a>
    <span>
     Key:ballesteros2015improved Ballesteros, M., Dyer, C., &amp; Smith, N. A. (2015). Improved transition-based parsing by modeling characters instead of words with lstms. arXiv preprint arXiv:1508.00657.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref101" id="ftnt101">
     [101]
    </a>
    <span>
     Key:guo2014joint Guo, D., Tur, G., Yih, W., Zweig, G. (2014).
    </span>
    <span>
     Joint semantic utterance classification and slot filling with recursive neural networks
    </span>
    <span>
     . Proceedings of the IEEE SLT Workshop
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref102" id="ftnt102">
     [102]
    </a>
    <span>
     Key:liu2016attention Liu, B. and Lane, I. (2016).
    </span>
    <span>
     Attention-based recurrent neural network models for joint intent detection and slot filling
    </span>
    <span>
     . Proceedings of The 17th Annual Meeting of the International Speech Communication Association
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref103" id="ftnt103">
     [103]
    </a>
    <span>
    </span>
    <span>
     <a href="https://en.wikipedia.org/wiki/Loop_unrolling">
      https://en.wikipedia.org/wiki/Loop_unrolling
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref104" id="ftnt104">
     [104]
    </a>
    <span>
     Key:werbos1990backpropagation Werbos, P. J. (1990). Backpropagation through time: what it does and how to do it. Proceedings of the IEEE, 78(10), 1550-1560.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref105" id="ftnt105">
     [105]
    </a>
    <span>
    </span>
    <span>
     <a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/">
      http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref106" id="ftnt106">
     [106]
    </a>
    <span>
     Key:bengio1994learning Bengio, Y., Simard, P., Frasconi, P. (1994).
    </span>
    <span>
     Learning long-term dependencies with gradient descent is difficult
    </span>
    <span>
     . IEEE Transactions on Neural Networks and Learning Systems
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref107" id="ftnt107">
     [107]
    </a>
    <span>
     Key:hochreiter1998vanishing Hochreiter, S. (1998).
    </span>
    <span>
     The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions
    </span>
    <span>
     . International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref108" id="ftnt108">
     [108]
    </a>
    <span>
     Key:hochreiter1997long Hochreiter, S., Schmidhuber, J. (1997).
    </span>
    <span>
     Long short-term memory
    </span>
    <span>
     . Neural computation - MIT Press
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref109" id="ftnt109">
     [109]
    </a>
    <span>
    </span>
    <span>
     <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">
      http://colah.github.io/posts/2015-08-Understanding-LSTMs/
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref110" id="ftnt110">
     [110]
    </a>
    <span>
     Key:cho2014learning Cho, K. et al. (2014).
    </span>
    <span>
     Learning phrase representations using RNN encoder-decoder for statistical machine translation
    </span>
    <span>
     . Proc. Conference on Empirical Methods in Natural Language Processing 1724–1734
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref111" id="ftnt111">
     [111]
    </a>
    <span>
     Key:jozefowicz2015empirical Jozefowicz, R., Zaremba, W., Sutskever, I. (2015).
    </span>
    <span>
     An empirical exploration of recurrent network architectures
    </span>
    <span>
     . Proceedings of the 32nd International Conference on Machine Learning, pages 2342–2350
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref112" id="ftnt112">
     [112]
    </a>
    <span>
     Key:chung2014empirical Chung, J., Gulcehre, C., Cho, K., Bengio, Y. (2014).
    </span>
    <span>
     Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling
    </span>
    <span>
     . NIPS Deep Learning Workshop
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref113" id="ftnt113">
     [113]
    </a>
    <span>
     Key:sahlgren2008distributional Sahlgren, M. (2008). The distributional hypothesis. Italian Journal of Disability Studies, 20, 33-53.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref114" id="ftnt114">
     [114]
    </a>
    <span>
    </span>
    <span>
     <a href="https://dumps.wikimedia.org/">
      https://dumps.wikimedia.org/
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref115" id="ftnt115">
     [115]
    </a>
    <span>
    </span>
    <span>
     <a href="http://statmt.org/ngrams/">
      http://statmt.org/ngrams/
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref116" id="ftnt116">
     [116]
    </a>
    <span>
     Key:maaten2008visualizing Maaten, L. V. D., &amp; Hinton, G. (2008). Visualizing data using t-SNE. Journal of machine learning research, 9(Nov), 2579-2605.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref117" id="ftnt117">
     [117]
    </a>
    <span>
     Key:bengio2003neural Bengio, Y., Ducharme, R., Vincent, P., &amp; Jauvin, C. (2003). A neural probabilistic language model. Journal of machine learning research, 3(Feb), 1137-1155.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref118" id="ftnt118">
     [118]
    </a>
    <span>
     Key:TODO Golub, G. H., &amp; Reinsch, C. (1970). Singular value decomposition and least squares solutions. Numerische mathematik, 14(5), 403-420.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref119" id="ftnt119">
     [119]
    </a>
    <span>
     Key:mikolov2013efficient Mikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013). Efficient estimation of word representations in vector space. ICLR Workshop 2013
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref120" id="ftnt120">
     [120]
    </a>
    <span>
     Key:pennington2014glove Pennington, J., Socher, R., &amp; Manning, C. (2014). Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) (pp. 1532-1543).
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref121" id="ftnt121">
     [121]
    </a>
    <span>
     Key:bojanowski2016enriching Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas. (2016). Enriching Word Vectors with Subword Information. arXiv preprint arXiv:1607.04606
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref122" id="ftnt122">
     [122]
    </a>
    <span>
     Key:bojanowski2016enriching Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas. (2016). Enriching Word Vectors with Subword Information. arXiv preprint arXiv:1607.04606
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref123" id="ftnt123">
     [123]
    </a>
    <span>
     Key:pinter2017mimicking Pinter,Y., Guthrie, R., Eisenstein, J. (2017). Mimicking word embeddings using subword rnns.
    </span>
    <span>
     Proceedings of EMNLP
    </span>
    <span>
     .
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref124" id="ftnt124">
     [124]
    </a>
    <span>
     Key:bojanowski2016enriching Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas. (2016). Enriching Word Vectors with Subword Information.
    </span>
    <span>
     arXiv preprint arXiv:1607.04606
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref125" id="ftnt125">
     [125]
    </a>
    <span>
     Key:lita2003truecasing Lita, L. V., Ittycheriah, A., Roukos, S., &amp; Kambhatla, N. (2003, July). Truecasing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1 (pp. 152-159). Association for Computational Linguistics.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref126" id="ftnt126">
     [126]
    </a>
    <span>
    </span>
    <span>
     <a href="https://chatbotslife.com/text-classification-using-algorithms-e4d50dcba45">
      https://chatbotslife.com/text-classification-using-algorithms-e4d50dcba45
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref127" id="ftnt127">
     [127]
    </a>
    <span>
     Key:jordan1997serial Jordan, M. I. (1997). Serial order: A parallel distributed processing approach. In Advances in psychology (Vol. 121, pp. 471-495). North-Holland.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref128" id="ftnt128">
     [128]
    </a>
    <span>
     Key:cho2014learning Cho, K. et al. (2014).
    </span>
    <span>
     Learning phrase representations using RNN encoder-decoder for statistical machine translation
    </span>
    <span>
     . Proc. Conference on Empirical Methods in Natural Language Processing 1724–1734
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref129" id="ftnt129">
     [129]
    </a>
    <span>
     Key:liu2016attention Liu, B. and Lane, I. (2016).
    </span>
    <span>
     Attention-based recurrent neural network models for joint intent detection and slot filling
    </span>
    <span>
     . Proceedings of The 17th Annual Meeting of the International Speech Communication Association
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref130" id="ftnt130">
     [130]
    </a>
    <span>
     Key:bahdanau2014neural Bahdanau, D., Cho, K. &amp; Bengio, Y. (2015).
    </span>
    <span>
     Neural machine translation by jointly learning to align and translate
    </span>
    <span>
     . Proc. International Conference on Learning Representations
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref131" id="ftnt131">
     [131]
    </a>
    <span>
     Key:bahdanau2014neural Bahdanau, D., Cho, K. &amp; Bengio, Y. (2015).
    </span>
    <span>
     Neural machine translation by jointly learning to align and translate
    </span>
    <span>
     . Proc. International Conference on Learning Representations
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref132" id="ftnt132">
     [132]
    </a>
    <span>
     Key:liu2016attention Liu, B. and Lane, I. (2016).
    </span>
    <span>
     Attention-based recurrent neural network models for joint intent detection and slot filling
    </span>
    <span>
     . Proceedings of The 17th Annual Meeting of the International Speech Communication Association
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref133" id="ftnt133">
     [133]
    </a>
    <span>
     Key:liu2016attention Liu, B. and Lane, I. (2016).
    </span>
    <span>
     Attention-based recurrent neural network models for joint intent detection and slot filling
    </span>
    <span>
     . Proceedings of The 17th Annual Meeting of the International Speech Communication Association
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref134" id="ftnt134">
     [134]
    </a>
    <span>
     Key:chen2016end Chen, Y., Hakkani-Tur, D., Tur, G., Gao, J., Li, D. (2016). End-to-end memory networks with knowledge carryover for multi-turn spoken language understanding. Proceedings of Interspeech
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref135" id="ftnt135">
     [135]
    </a>
    <span>
     Key:chen2017dynamic Chen, P., Chi, T., Su, S., Chen, Y. (2017). Dynamic Time-Aware Attention to Speaker Roles and Contexts for Spoken Language Understanding. Proceedings of 2017 IEEE Workshop on Automatic Speech Recognition and Understanding
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref136" id="ftnt136">
     [136]
    </a>
    <span>
     Key:sukhbaatar2015end Sukhbaatar, S., Szlam, A., Weston, J., Fergus, R. (2015).
    </span>
    <span>
     End-to-end memory networks
    </span>
    <span>
     . Proceedings of NIPS
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref137" id="ftnt137">
     [137]
    </a>
    <span>
     Key:eric2017key Eric, M. and Manning, C. (2017).
    </span>
    <span>
     Key-value retrieval networks for task-oriented dialogue
    </span>
    <span>
     . SIGDIAL 2017: Session on Natural Language Generation for Dialog Systems
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref138" id="ftnt138">
     [138]
    </a>
    <span>
     Key:kosinski2015facebook Kosinski, M., Matz, S. C., &amp; Gosling, S. D. (2015). Facebook as a research tool for the social sciences. American Psychologist, 70(6), 543–556. doi:10.1037/a0039210
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref139" id="ftnt139">
     [139]
    </a>
    <span>
     https://www.psychometrics.cam.ac.uk/productsservices/mypersonality
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref140" id="ftnt140">
     [140]
    </a>
    <span>
     Key:costa2008revised Costa, P.T. Jr. &amp; McCrae, R.R. (1992). Revised NEO Personality Inventory (NEO-PI-R) and NEO Five-Factor Inventory (NEO-FFI) manual.
    </span>
    <span>
     Odessa, FL: Psychological Assessment Resources
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref141" id="ftnt141">
     [141]
    </a>
    <span>
     Key:goldberg1993structure Goldberg, L. R. (1993). The structure of phenotypic personality traits.
    </span>
    <span>
     American Psychologist. 48: 26–34. doi:10.1037/0003-066x.48.1.26
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref142" id="ftnt142">
     [142]
    </a>
    <span>
     Key:youyou2015computer Youyou, W., Kosinski, M., Stillwell, D. (2015). Computer-based personality judgments are more accurate than those made by humans.
    </span>
    <span>
     PNAS pp. 1–5
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref143" id="ftnt143">
     [143]
    </a>
    <span>
     Key:kosinski2013private Kosinski, M., Stillwell, D., Graepel, Y. (2013). Private traits and attributes are predictable from digital records of human behavior.
    </span>
    <span>
     Proceedings of the National Academy of Sciences (PNAS).
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref144" id="ftnt144">
     [144]
    </a>
    <span>
     Key:mairesse2007using F. Mairesse, M.A. Walker, M.R. Mehl, and R.K. Moore. (2007). Using linguistic cues for the automatic recognition of personality in conversation and text.
    </span>
    <span>
     Journal of Artificial Intelligence Research, 30(1):457–500
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref145" id="ftnt145">
     [145]
    </a>
    <span>
     Key:lops2011content Lops, P., De Gemmis, M., &amp; Semeraro, G. (2011). Content-based recommender systems: State of the art and trends. In Recommender systems handbook (pp. 73-105). Springer US.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref146" id="ftnt146">
     [146]
    </a>
    <span>
     Key:liu2016attention Liu, B. and Lane, I. (2016). Attention-based recurrent neural network models for joint intent detection and slot filling.
    </span>
    <span>
     Proceedings of The 17th Annual Meeting of the International Speech Communication Association
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref147" id="ftnt147">
     [147]
    </a>
    <span>
    </span>
    <span>
     <a href="https://github.com/HadoopIt/rnn-nlu">
      https://github.com/HadoopIt/rnn-nlu
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref148" id="ftnt148">
     [148]
    </a>
    <span>
    </span>
    <span>
     <a href="http://hlt.isti.cnr.it/wordembeddings/">
      http://hlt.isti.cnr.it/wordembeddings/
     </a>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref149" id="ftnt149">
     [149]
    </a>
    <span>
    </span>
    <span>
     <a href="https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md">
      https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md
     </a>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref150" id="ftnt150">
     [150]
    </a>
    <span>
    </span>
    <span>
     <a href="http://slides.com/smerity/common-crawl-for-nlp">
      http://slides.com/smerity/common-crawl-for-nlp
     </a>
    </span>
    <span>
     . Source, preprocessed data and models can be downloaded from
    </span>
    <span>
     <a href="http://statmt.org/ngrams">
      http://statmt.org/ngrams
     </a>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref151" id="ftnt151">
     [151]
    </a>
    <span>
     Key:liu2016attention Liu, B. and Lane, I. (2016).
    </span>
    <span>
     Attention-based recurrent neural network models for joint intent detection and slot filling
    </span>
    <span>
     . Proceedings of The 17th Annual Meeting of the International Speech Communication Association
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref152" id="ftnt152">
     [152]
    </a>
    <span>
     Key:liu2016attention Liu, B. and Lane, I. (2016).
    </span>
    <span>
     Attention-based recurrent neural network models for joint intent detection and slot filling
    </span>
    <span>
     . Proceedings of The 17th Annual Meeting of the International Speech Communication Association
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref153" id="ftnt153">
     [153]
    </a>
    <span>
     Zoey Chong. (2017, December 6). Obike becomes latest victim of global data breach. CNET. Retrieved from
    </span>
    <span>
     <a href="https://www.cnet.com">
      https://www.cnet.com
     </a>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref154" id="ftnt154">
     [154]
    </a>
    <span>
     Key:quercia2015smelly Daniele Quercia, Rossano Schifanella, Luca Maria Aiello, Kate McLean. Smelly Maps: The Digital Life of Urban Smellscapes. In Proc. of the 9th International AAAI Conference on Web and Social Media (ICWSM), 2015.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref155" id="ftnt155">
     [155]
    </a>
    <span>
     Key:quercia2016emotional Daniele Quercia, Luca Maria Aiello, Rossano Schifanella. The Emotional and Chromatic Layers of Urban Smells. In Proc. of the 10th International AAAI Conference on Web and Social Media (ICWSM), 2016.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref156" id="ftnt156">
     [156]
    </a>
    <span>
     Key:aiello2016chatty Luca Maria Aiello, Rossano Schifanella, Daniele Quercia, Francesco Aletta. Chatty maps: constructing sound maps of urban areas from social media data. Royal Society Open Science (RSOS), 2016.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref157" id="ftnt157">
     [157]
    </a>
    <span>
    </span>
    <span>
     <a href="https://www.heroku.com/">
      https://www.heroku.com/
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref158" id="ftnt158">
     [158]
    </a>
    <span>
    </span>
    <span>
     <a href="http://www.fast.ai/2017/01/03/keras/">
      http://www.fast.ai/2017/01/03/keras/
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref159" id="ftnt159">
     [159]
    </a>
    <span>
     Key:liu2016attention Liu, B. and Lane, I. (2016).
    </span>
    <span>
     Attention-based recurrent neural network models for joint intent detection and slot filling
    </span>
    <span>
     . Proceedings of The 17th Annual Meeting of the International Speech Communication Association
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref160" id="ftnt160">
     [160]
    </a>
    <span>
     Key:cho2014learning Cho, K. et al. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. Proc. Conference on Empirical Methods in Natural Language Processing 1724–1734
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref161" id="ftnt161">
     [161]
    </a>
    <span>
     Key:hochreiter1997long Hochreiter, S., Schmidhuber, J. (1997). Long short-term memory. Neural computation - MIT Press
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref162" id="ftnt162">
     [162]
    </a>
    <span>
     Key:liu2016attention Liu, B. and Lane, I. (2016).
    </span>
    <span>
     Attention-based recurrent neural network models for joint intent detection and slot filling
    </span>
    <span>
     . Proceedings of The 17th Annual Meeting of the International Speech Communication Association
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref163" id="ftnt163">
     [163]
    </a>
    <span>
     Key:hemphill1990atis Hemphill, C., Godfrey, J., Doddington, G. (1990). The ATIS spoken language systems pilot corpus. DARPA Speech and Natural Language Workshop
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref164" id="ftnt164">
     [164]
    </a>
    <span>
     Key:hemphill1990atis Hemphill, C., Godfrey, J., Doddington, G. (1990). The ATIS spoken language systems pilot corpus. DARPA Speech and Natural Language Workshop
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref165" id="ftnt165">
     [165]
    </a>
    <span>
    </span>
    <span>
     <a href="https://snips.ai/content/sdk-benchmark-visualisation/">
      https://snips.ai/content/sdk-benchmark-visualisation/
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref166" id="ftnt166">
     [166]
    </a>
    <span>
    </span>
    <span>
     <a href="https://developer.amazon.com/alexa">
      https://developer.amazon.com/alexa
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref167" id="ftnt167">
     [167]
    </a>
    <span>
    </span>
    <span>
     <a href="https://api.ai/">
      https://api.ai/
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref168" id="ftnt168">
     [168]
    </a>
    <span>
    </span>
    <span>
     <a href="https://www.luis.ai/">
      https://www.luis.ai/
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref169" id="ftnt169">
     [169]
    </a>
    <span>
    </span>
    <span>
     <a href="https://developer.apple.com/sirikit/">
      https://developer.apple.com/sirikit/
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref170" id="ftnt170">
     [170]
    </a>
    <span>
    </span>
    <span>
     <a href="https://www.ibm.com/watson/developercloud/nl-classifier.html">
      https://www.ibm.com/watson/developercloud/nl-classifier.html
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref171" id="ftnt171">
     [171]
    </a>
    <span>
    </span>
    <span>
     <a href="https://wit.ai/">
      https://wit.ai/
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref172" id="ftnt172">
     [172]
    </a>
    <span>
    </span>
    <span>
     <a href="https://snips.ai/">
      https://snips.ai/
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref173" id="ftnt173">
     [173]
    </a>
    <span>
    </span>
    <span>
     <a href="https://github.com/snipsco/nlu-benchmark">
      https://github.com/snipsco/nlu-benchmark
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref174" id="ftnt174">
     [174]
    </a>
    <span>
     Key:eric2017key Eric, M. and Manning, C. (2017). Key-value retrieval networks for task-oriented dialogue. SIGDIAL 2017: Session on Natural Language Generation for Dialog Systems
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref175" id="ftnt175">
     [175]
    </a>
    <span>
     Key:tur2010left Tur, G., Hakkani-Tür, D., &amp; Heck, L. (2010, December). What is left to be understood in ATIS?. In Spoken Language Technology Workshop (SLT), 2010 IEEE (pp. 19-24). IEEE.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref176" id="ftnt176">
     [176]
    </a>
    <span>
     Key:berardi2015word Berardi, G., Esuli, A., &amp; Marcheggiani, D. (2015, May). Word Embeddings Go to Italy: A Comparison of Models and Training Datasets. In IIR.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#cmnt_ref1" id="cmnt1">
     [a]
    </a>
    <span>
     Ho diviso il riassunto in 3 sottosezioni, come mi hanno consigliato altri ragazzi e come è fatto intendere anche dai pochi documenti disponibili sul sito del poli
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#cmnt_ref2" id="cmnt2">
     [b]
    </a>
    <span>
     forse ne parli dopo, c'e' un progetto EU molto grande https://www.humanbrainproject.eu/en/
    </span>
   </p>
   <p>
    <span>
    </span>
   </p>
   <p>
    <span>
     e ce n'e' anche uno americano analogo
    </span>
   </p>
   <p>
    <span>
     https://www.nsf.gov/awardsearch/showAward?AWD_ID=0134732
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#cmnt_ref3" id="cmnt3">
     [c]
    </a>
    <span>
     See https://web.stanford.edu/class/cs124/lec/chatbot17.pdf for detailed analysis
    </span>
   </p>
   <p>
    <span>
     For frame-based and another introduction see https://web.stanford.edu/class/cs224s/lectures/224s.17.lec10.pdf
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#cmnt_ref4" id="cmnt4">
     [d]
    </a>
    <span>
     vedere anche https://www.wired.com/story/inside-amazon-alexa-prize/
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#cmnt_ref5" id="cmnt5">
     [e]
    </a>
    <span>
     GoBee abbandona Torino: http://torino.repubblica.it/cronaca/2018/02/09/news/gobee_bike_colosso_asiatico_delle_bici_a_nollegio_dopo_la_francia_abbandona_pure_torino-188432125/
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#cmnt_ref6" id="cmnt6">
     [f]
    </a>
    <span>
     aggiungere l'asse della lingua
    </span>
   </p>
  </div>
 </body>
</html>