<html>
 <head>
  <meta content="text/html; charset=utf-8" http-equiv="content-type"/>
 </head>
 <body>
  <div>
   <p>
    <span>
    </span>
   </p>
  </div>
  <h1 id="h.k60jpmft2v71">
   <span>
    Introduction
   </span>
  </h1>
  <p>
   <span>
    Introduce the problem and motivation
   </span>
  </p>
  <p>
   <span>
    Brief summary of previous works
   </span>
  </p>
  <p>
   <span>
    Specific objectives
   </span>
  </p>
  <p>
   <span>
    Roadmap
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <hr style="page-break-before:always;display:none;"/>
  <p>
   <span>
   </span>
  </p>
  <h1 id="h.hohs5u6vo8mq">
   <span>
    State of the Art
   </span>
  </h1>
  <p>
   <span>
    In this section I am presenting the related works on the topic.
   </span>
  </p>
  <h2 id="h.8cfue3eizy9l">
   <span>
    Social bots
   </span>
  </h2>
  <p>
   <span>
    The term can be used for different purposes:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Fake user profiles
    </span>
   </li>
   <li>
    <span>
     Crawlers that analyze social profiles
    </span>
   </li>
   <li>
    <span>
     Autonomous agents that provide some kind of service
    </span>
   </li>
  </ul>
  <p>
   <span>
    AI trends: Expectation VS reality
   </span>
  </p>
  <ul>
   <li>
    <span>
     <a href="https://dl.acm.org/citation.cfm?id%3D2770869">
      https://dl.acm.org/citation.cfm?id=2770869
     </a>
    </span>
    <span>
    </span>
   </li>
   <li>
    <span>
     AI hype
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Bots and autonomous systems papers
   </span>
  </p>
  <p>
   <span>
    “Turing’s red flag”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Problem: AI mistaken for human
    </span>
   </li>
   <li>
    <span>
     Proposition:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     autonomous system should be designed in a way to make clear that is not a human
    </span>
   </li>
   <li>
    <span>
     should identify as autonomous system at the start of new interactions
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Cases:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Self-driving car: should be recognizable in order to avoid accidents and to allow other drivers (autonomous or not) to behave in proper way
    </span>
   </li>
   <li>
    <span>
     Virtual assistants: pretending to be human is a dangerous precedent. Now it is clear that they are AI, but with technological progress the difference could become unnoticed
    </span>
   </li>
   <li>
    <span>
     Online games: bots can have some advantages and disadvantages, but user should know what kind of player they belong to
    </span>
   </li>
   <li>
    <span>
     Computer-generated text: depending on the domain, can impact emotions of reader
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “The rise of social bots”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Problem: social bots can be harmful. Beyond the problem of veracity of information, they can gain influence and become popular
    </span>
   </li>
   <li>
    <span>
     Cases:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Political influence: audience is artificially enlarged
    </span>
   </li>
   <li>
    <span>
     Market influence: fake informations are amplified without fact-checking
    </span>
   </li>
   <li>
    <span>
     Exposing private informations: this makes people not to trust social media
    </span>
   </li>
   <li>
    <span>
     Manipulate emotions: on socials, they are contagious
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Proposition: detect bots
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Auto-reposting: easy to spot with posts/tweets without sense
    </span>
   </li>
   <li>
    <span>
     More advanced bots: emulating human behavior, filling profiles with data that seems legit, interacting actively with other users. In some cases they also clone profiles of real users
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Implementation types:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Based on social media information (graph-based): “sybil” accounts are prone to have more connections with other sybils -&gt; groups. Based on assumption that legitimate users refuse to interact with unknown accounts, but this is not so true
    </span>
   </li>
   <li>
    <span>
     Based on crowdsourcing: humans analyze profiles to detect bots, and majority voting is applied (same profiles shown to different workers)
    </span>
   </li>
   <li>
    <span>
     Machine-learning: find features that are significant for discrimination. The system analyzes a set of features (network, user, friends, timing, content, sentiment) to evaluate a score using cross validation. Not able to detect cyborgs (mixture of human and bot) and hacked accounts
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Is that a bot running the social media feed?”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: understand if users perceive differently a bot agent or a human agent
    </span>
   </li>
   <li>
    <span>
     Results: bots are perceived credible, attractive and competent as humans. The cause is that users use the same way of interacting with bots and with humans. However the attraction to the human agent is higher
    </span>
   </li>
   <li>
    <span>
     Limitation: very restricted context
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Towards the implementation of a Topic specific dialogue based Natural Language Chatbot as an Undergraduate Advisor”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: develop a chatbot to help students in University with admission and course information (FAQ) using AIML. Weights of state transitions are tuned to make the conversation stay on topic.
    </span>
   </li>
   <li>
    <span>
     Metrics for performance:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Satisfaction: user vote if the answer is appropriate in the context
    </span>
   </li>
   <li>
    <span>
     Topic switching rate: how much the dialogue is switched from on-topic to off-topic
    </span>
   </li>
   <li>
    <span>
     Correction rate: responses that are corrected by the user
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Can we control it? Autonomous robots threaten human identity, uniqueness, safety, and resources”
   </span>
   <sup>
    <a href="#ftnt1" id="ftnt_ref1">
     [1]
    </a>
   </sup>
  </p>
  <ul>
   <li>
    <span>
     Proposition: analyze the impact of autonomous systems on society, how they are perceived
    </span>
   </li>
   <li>
    <span>
     Problems of autonomous system:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Humans want to always have control and power. The system’s autonomy should always be bounded: only automatically perform specific tasks as requested. However in some cases there can be conflicts between what the user asks and the principle the system was designed with. This should be the only situation in which the system must disregard a command. If the system decides things on his own, also without conflicting with commands, this is seen as a threat to human superiority on machines. The hierarchy of society is threatened
    </span>
   </li>
   <li>
    <span>
     Realistic threats: menacing safety, wellbeing or material resources
    </span>
   </li>
   <li>
    <span>
     Symbolic threats: maintain the difference between human and machine. The concept of identity and distinctiveness are threatened
    </span>
   </li>
   <li>
    <span>
     Both kinds of threat lead to negative attitudes towards robots and robotics research
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “An Ergonomics Evaluation to Chatbot Equipped with Knowledge-Rich Mind”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: how to incorporate knowledge in chatbot systems
    </span>
   </li>
   <li>
    <span>
     Implementation:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Combining more approaches together. AIML (common dialog questions, containing a lot of stop words), Regular Matcher Component (for simple commands), FAQ (translating sentences into queries, selecting best match), NLP
    </span>
   </li>
  </ul>
  <h3 id="h.ntrzngmuo26j">
   <span>
    Artificial intelligence
   </span>
  </h3>
  <p>
   <span>
    General topic
   </span>
  </p>
  <p>
   <span>
    <a href="https://medium.com/@Numenta/the-secret-to-strong-ai-61d153e26273">
     https://medium.com/@Numenta/the-secret-to-strong-ai-61d153e26273
    </a>
   </span>
   <span>
   </span>
  </p>
  <h3 id="h.d7tjvkxrgg7m">
   <span>
    Guidelines
   </span>
  </h3>
  <p>
   <span>
    Define boundaries on:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Autonomy of the system: smart system should be able to decide things in its domain because a lot of information can be used to take decision, doing work for the users. Some limits need not to be overtaken, in order not to threat the hierarchy of society. Machines are tools for human well being
    </span>
   </li>
   <li>
    <span>
     Distinguishability: according to “red flag law”, system should clearly be recognizable as robot. However for usability the interaction with users should be similar to human-to-human
    </span>
   </li>
   <li>
    <span>
     Personality: having an artificial intelligence with some personality makes interactions more interesting and seamless for users. A boundary needs to be defined in order to avoid emotional attachment and other inappropriate feelings towards machines. From social point of view, people need to keep their life in real world with real people
    </span>
   </li>
  </ul>
  <hr style="page-break-before:always;display:none;"/>
  <p>
   <span>
   </span>
  </p>
  <h2 id="h.1t8dpluzlftb">
   <span>
    Types of bot
   </span>
  </h2>
  <p>
   <span>
    Dimensions to consider:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Long or short conversations
    </span>
   </li>
   <li>
    <span>
     Open or closed domain. Closed is the only feasible
    </span>
   </li>
  </ul>
  <p>
   <span>
    Important features / challenges:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Context
    </span>
    <span>
     : keep track of informations given by user
    </span>
   </li>
   <li>
    <span>
     Coherent personality: since training is done on multiple users data (see “A persona-based neural conversation model”)
    </span>
   </li>
   <li>
    <span>
     Evaluation (see “How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation”). May use recall@k
    </span>
   </li>
   <li>
    <span>
     Intention and diversity: easy to fall into generic responses (see “A Diversity-Promoting Objective Function for Neural Conversation Models”) especially for open-domain systems
    </span>
   </li>
  </ul>
  <p>
   <span>
    Scope of dialogue:
   </span>
  </p>
  <ul>
   <li>
    <span>
     General chat
    </span>
   </li>
   <li>
    <span>
     Task specific
    </span>
   </li>
   <li>
    <span>
     Semantic question answering
    </span>
   </li>
  </ul>
  <h3 id="h.13wjijx5nwux">
   <span>
    Retrieval-based models
   </span>
  </h3>
  <p>
   <span>
    This kind of bots use a set of predefined response templates and apply some heuristics in order to select the most suitable one. The selection criterion can be:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Simple rule-based (as in AIML)
    </span>
   </li>
   <li>
    <span>
     More complex with machine learning classifiers
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Advantages:
   </span>
  </p>
  <ul>
   <li>
    <span>
     No grammatical mistakes
    </span>
   </li>
  </ul>
  <p>
   <span>
    Disadvantages:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Unseen cases cannot be handled
    </span>
   </li>
  </ul>
  <h4 id="h.lxbz31wv66qa">
   <span>
    AIML (rule based)
   </span>
  </h4>
  <p>
   <span>
    Is a language that describes how the bot replies to certain inputs. Inputs are evaluated against a set of patterns and the best match (category) is chosen. The actions performed by the category can be a simple response, or can also set variables and call other categories.
   </span>
  </p>
  <p>
   <span>
    Bot creation:
   </span>
  </p>
  <p>
   <span>
    Cyclical process called
   </span>
   <span>
    targeting
   </span>
   <span>
    : client inputs that find no match are collected using logs and the botmaster creates suitable responses. Targeting interface can also be exposed to clients, but with risks: not controlled by the botmaster
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    AIML can be used to build bots that simply do a pattern-matching job, it is only a stimulus-response system.
   </span>
  </p>
  <p>
   <span>
    Main disadvantages:
   </span>
  </p>
  <ul>
   <li>
    <span>
     A big set of AIML rules need to be built: time consuming. A lot of rules are also needed to perform reduction
    </span>
   </li>
   <li>
    <span>
     Difficult to reply to complex queries
    </span>
   </li>
   <li>
    <span>
     Not really understanding the language.
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Where it is used:
   </span>
  </p>
  <ul>
   <li>
    <span>
     ALICE based bots: Pandorabots is a service to host an AIML processor. Users can create files that contain the rules and test the bot
    </span>
   </li>
   <li>
    <span>
     <a href="https://github.com/keiffster/program-y">
      https://github.com/keiffster/program-y
     </a>
    </span>
    <span>
     is an implementation of AIML 2.0 using python 3
    </span>
   </li>
  </ul>
  <h4 id="h.3r2mfopy3uiq">
   <span>
    Machine learning (for the understanding part)
   </span>
  </h4>
  <p>
   <span>
    Better explained in NLU. This is the best approach for task-specific bots. The problem is understanding the user. Once the understanding is done, the system can use the domain rules and provide back a response. The part that gives back the response is better done programmatically
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Examples:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Google smart reply
    </span>
    <span>
     <a href="http://arxiv.org/abs/1606.04870">
      http://arxiv.org/abs/1606.04870
     </a>
    </span>
    <span>
    </span>
   </li>
  </ul>
  <h3 id="h.4yanc26b251y">
   <span>
    Generative models
   </span>
  </h3>
  <p>
   <span>
    Approach: give a huge corpus of dialogue. The system will learn how to generate next sentence
   </span>
  </p>
  <p>
   <span>
    Data sets:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Ubuntu dialogue corpus:
    </span>
    <span>
     <a href="https://github.com/rkadlec/ubuntu-ranking-dataset-creator">
      https://github.com/rkadlec/ubuntu-ranking-dataset-creator
     </a>
    </span>
    <span>
    </span>
   </li>
   <li>
    <span>
     Twitter
    </span>
   </li>
  </ul>
  <p>
   <span>
    Predictor:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Random. Very poor performance
    </span>
   </li>
   <li>
    <span>
     Using tf-idf: how important a word is important in a document. Documents with similar content have similar tf-idf vectors. Responses don’t need to be similar to the context to be correct
    </span>
   </li>
   <li>
    <span>
     Deep learning: dual-encoder LSTM network / seq2seq. Both are recurrent neural networks
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Don’t use pre-defined responses, but generate them from scratch. The technology is based on translation techniques.
   </span>
  </p>
  <p>
   <span>
    Advantages:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Can refer back to entities mentioned in the conversation
    </span>
   </li>
  </ul>
  <p>
   <span>
    Disadvantages:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Grammatical mistakes can occur
    </span>
   </li>
   <li>
    <span>
     Require huge amounts of training data
    </span>
   </li>
   <li>
    <span>
     Early stage of development
    </span>
   </li>
   <li>
    <span>
     Can go off the rails (example of Microsoft Tay)
    </span>
   </li>
  </ul>
  <p>
   <span>
    This approach has very good results for non-specific task dialogues, because the goal is simply to continue the dialogue and there is no need to introduce some task-specific informations (KBs).
   </span>
  </p>
  <p>
   <span>
    The results are quite promising in this field, but we decided not to use such end2end systems but instead to apply the NLU strategy to map the user sentences to a pre-designed fixed set of intents.
   </span>
  </p>
  <h2 id="h.34p3lqtcm9yp">
   <span>
    Main topics of the research phase
   </span>
  </h2>
  <p>
   <span>
    NLU:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Understanding the user queries: intent + slot
    </span>
   </li>
   <li>
    <span>
     Tracking the state of the dialogue
    </span>
   </li>
   <li>
    <span>
     Provide relatively good answers to non-task specific interactions
    </span>
   </li>
  </ul>
  <p>
   <span>
    Personalization:
   </span>
  </p>
  <ul>
   <li>
    <span>
     TODO
    </span>
   </li>
  </ul>
  <h2 id="h.n06dhtaupqp">
   <span>
    NLP and NLU
   </span>
  </h2>
  <p>
   <span>
    The most important part is to understand the user message. Instead to do a simple rule-matching, there are libraries that can process sentences and extract their structure, together with named entities.
   </span>
  </p>
  <p>
   <span>
    The classic NLP pipeline includes:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Spell check
    </span>
   </li>
   <li>
    <span>
     Tokenize (sentences, words)
    </span>
   </li>
   <li>
    <span>
     POS recognition
    </span>
   </li>
   <li>
    <span>
     Lemmatize (reconduct to base form, requires lexica) and reduce synonyms / stemming (more rude, simply removing suffixes, not context-aware, rule-based)
    </span>
   </li>
   <li>
    <span>
     Entity recognition
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Once the input is processed, there should be a logic that based on the informations extracted (entities and relationships) understands the
   </span>
   <span>
    root-level intent
   </span>
   <span>
    of the person and decides what actions need to be performed:
   </span>
  </p>
  <ol start="1">
   <li>
    <span>
     Question classification and entity extraction
    </span>
   </li>
   <li>
    <span>
     Information retrieval
    </span>
   </li>
   <li>
    <span>
     Answer extraction
    </span>
   </li>
  </ol>
  <p>
   <span>
    NLP frameworks:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Stanford CoreNLP
    </span>
    <span>
     <a href="http://stanfordnlp.github.io/CoreNLP/">
      http://stanfordnlp.github.io/CoreNLP/
     </a>
    </span>
    <span>
     : java
    </span>
   </li>
   <li>
    <span>
     NLTK: wide variety of algorithms and stemmers. Can finely customize the model. String based
    </span>
   </li>
   <li>
    <span>
     <a href="https://github.com/sloria/TextBlob">
      https://github.com/sloria/TextBlob
     </a>
    </span>
    <span>
     : based on NLTK
    </span>
   </li>
   <li>
    <span>
     <a href="https://github.com/explosion/spaCy">
      https://github.com/explosion/spaCy
     </a>
    </span>
    <span>
     : faster, but a bit lower quality. A single stemmer. Object oriented
    </span>
   </li>
   <li>
    <span>
     Google syntaxnet
    </span>
    <span>
     <a href="https://github.com/tensorflow/models/tree/master/syntaxnet">
      https://github.com/tensorflow/models/tree/master/syntaxnet
     </a>
    </span>
    <span>
     . A tensorflow model that is the most accurate parser. Also able to parse correctly some garden-paths
    </span>
   </li>
  </ul>
  <p>
   <span>
    Comparison between NLTK and spaCy
   </span>
   <span>
    <a href="https://gist.github.com/rschroll/61b20c41e984a963df2870cfc9e628ed">
     https://gist.github.com/rschroll/61b20c41e984a963df2870cfc9e628ed
    </a>
   </span>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Those NLP framework can be used for the specific task of NLU (that corresponds to turning the sentences into intent+entities). The other NLP tasks (syntactical analysis, POS tagging) are not main interests in the field of bots.
   </span>
  </p>
  <h3 id="h.xsg4ur5pp4av">
   <span>
    Goal
   </span>
  </h3>
  <p>
   <span>
    As said before, from the sentences we want to:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Classify the intent (what the user wants, against a set of predefined ones)
    </span>
   </li>
   <li>
    <span>
     Extract the entities mentioned (we may want to care about only a specific set of entity types that are relevant to the abilities of the bot). This task is somewhere called slot filling, because the words that we want to extract may also not be entities. “The slot filling task is to search a document collection to fill in values for predefined slots (attributes) for a given entity.”
    </span>
   </li>
  </ul>
  <p>
   <span>
    For example, for the sentence “is Turin a supported city?” we may want to categorize it as a question that asks if a city is supported (this is the intent) and extract the word “Torino” that is the city the user is looking for (this is a slot that corresponds to an entity).
   </span>
  </p>
  <p>
   <span>
    The first task corresponds to a classification of the user sentences to decide what is the intention of the user. Since a bot is usually designed to answer to different types of questions, this stage is responsible for finding the type of question. The slot filling task instead (also called NER) is a process that annotates some parts of the input sentences with the name of the corresponding slot. A slot can be thought as a field in an online form. While the intent represents the type of question, the slots of a sentence are values that the bot must be able to extract from the sentences because they are used as parameters for the application logic.
   </span>
  </p>
  <p>
   <span>
    For intent classification different approaches can be used:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Key words
    </span>
   </li>
   <li>
    <span>
     Syntactic based: the input features are hand-crafted (for example verb + object are taken into account)
    </span>
   </li>
   <li>
    <span>
     Sequence modeling (see RNN)
    </span>
   </li>
  </ul>
  <p>
   <span>
    There are different approaches for entity extraction:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Syntactic-based: from the structure of the sentence (usually as a tree), a model is built to observe and learn where usually a certain type of entity is
    </span>
   </li>
   <li>
    <span>
     Statistical-based: instead of syntactically parsing the sentence, a model is built without hand-crafted features, only providing input sequences and output label sequences
    </span>
   </li>
   <li>
    <span>
     Semantic-based: the values of the entities are used to detect the entities. The entity lookup and linking is a key point in this approach
    </span>
   </li>
  </ul>
  <p>
   <span>
    Once the intent and the slots have been analyzed on the current sentence, we can have some rules at the application level that can put some constraints. For example, for a certain type of question, one or more slots can be compulsory. In this case, the conversation should make the user provide the values by prompting him some questions. These types of constraint make the interaction model more complex and require an analysis over multiple user and machine turns, as will be seen in this section (reference to “Beyond the single sentence”)
   </span>
  </p>
  <p>
   <span>
    The two tasks can be done independently, but some recent studies [this
   </span>
   <sup>
    <a href="#ftnt2" id="ftnt_ref2">
     [2]
    </a>
   </sup>
   <span>
    and this
   </span>
   <sup>
    <a href="#ftnt3" id="ftnt_ref3">
     [3]
    </a>
   </sup>
   <span>
    ] have shown that there can be benefits if they are performed together.
   </span>
  </p>
  <h3 id="h.iqkeiq1d8ogq">
   <span>
    NLU as a service
   </span>
  </h3>
  <p>
   <span>
    As of today, there are a lot of platforms that provide this as a service. Big companies have decided to invest on it, to be the ones that hold the technology. They made those topics extremely easy for the developers, that through a web interface can create dialogue flows and annotate manually the data. There is no need to know about the technology that is inside. All you need is to understand the jargon used by the platform and configure your black box.
   </span>
  </p>
  <p>
   <span>
    There have been some efforts to compare the performances of all those systems by SNIPS:
   </span>
   <span>
    <a href="https://snips.ai/content/sdk-benchmark-visualisation/">
     https://snips.ai/content/sdk-benchmark-visualisation/
    </a>
   </span>
   <span>
   </span>
   <span>
    <a href="https://medium.com/snips-ai/benchmarking-natural-language-understanding-systems-google-facebook-microsoft-and-snips-2b8ddcf9fb19">
     https://medium.com/snips-ai/benchmarking-natural-language-understanding-systems-google-facebook-microsoft-and-snips-2b8ddcf9fb19
    </a>
   </span>
   <span>
   </span>
   <span>
    <a href="https://www.slideshare.net/KonstantinSavenkov/nlu-intent-detection-benchmark-by-intento-august-2017">
     https://www.slideshare.net/KonstantinSavenkov/nlu-intent-detection-benchmark-by-intento-august-2017
    </a>
   </span>
   <span>
    that tested a lot of those platforms to compare the results on intent and slots. Emphasis also on the detection of out-of-domain samples (things that the agent was not trained for), learning curve, language support, response time. Dataset used:
   </span>
   <span>
    <a href="https://github.com/snipsco/nlu-benchmark">
     https://github.com/snipsco/nlu-benchmark
    </a>
   </span>
   <span>
   </span>
  </p>
  <p>
   <span>
    EXTEND THIS SECTION
   </span>
  </p>
  <h3 id="h.jot16v735fof">
   <span>
    Joint slot filling and intent classification
   </span>
  </h3>
  <p>
   <span>
    This section contains a description of the different structures of neural networks that have reached the State of the Art condition and have been described in literature.
   </span>
  </p>
  <p>
   <span>
    A neural-network approach can be used for the tasks of slot filling and intent detection. The first thing that needs to be done is to point out what are the inputs and outputs of the system. The inputs to the network are the words contained in the sentence and the outputs are the intent label (for the intent classification task) and the slot labels (for the slot filling task).
   </span>
  </p>
  <p>
   <span>
    As slot labels, the slot names can be used directly but, in order to better handle multi-word slots, a commonly used format is the IOB, where the O indicates “other”, the B is the beginning of a slot and the I is the label associated with a word that continues the slot of the previous word. The IOB labels are prepended to the slot name.
   </span>
  </p>
  <p>
   <span>
    For the previous example, the output annotations could be:
   </span>
  </p>
  <p>
   <span>
    “Is(O) Turin(B-city) a(O) supported(O) city(O)?” intent=supported_city
   </span>
  </p>
  <p>
   <span>
    Once defined the inputs and outputs, we can think of a component that implements the desired functionalities.
   </span>
  </p>
  <h4 id="h.l6k85hs6qcqb">
   <span>
    Why RNN
   </span>
  </h4>
  <p>
   <span>
    Instead of using simple feed-forward networks, a lot of studies make use of networks with recurrent components. The core idea behind RNN is to make use of sequential informations. In feed-forward networks the assumption is that each input-output pair is independent from the others. For RNN, the recurrence stays in performing the same task for every element of the sequence, making the output depend on the previous steps. This idea can be seen as the RNN having memory that keeps informations from the past iterations. Specifically on the ability of remember and use in the correct way their memory, a lot of studies have been done to develop particular cells (a cell is the basic building block for RNNs).
   </span>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 241.33px;">
    <img alt="" src="https://lh5.googleusercontent.com/HvnduF8wYNcp3zfwOqKoIaPhbvuY5XfOJp194XSBL6XAEHWrbAd5vpW_sRZNQ2AfURDFBLFs1dKuPyWiWxpxT-Awqq-5VJXo4Ssh-KeuQhoIOkgRJMis15RyfAQ7B4nikoAOTfJs" style="width: 602.00px; height: 241.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    A recurrent cell is a type of cell that takes as input also its previous output. Those types of neural networks have been designed for problems where the order of inputs matters, and the length of input sequence can vary. For example, they are commonly used with sequence of words, characters, frames in a video and their goodness stands in modeling features that belong to the sequence. Unlike feed-forward nets, that consider the fixed set of inputs to generate the outputs, the recurrent nets are applied in different timesteps to elements belonging to a sequence and, thanks to the loops of their cells, keep informations from previous timesteps and the output depends on them too.
   </span>
  </p>
  <p>
   <span>
    Since the same network (and cells) are used in different timesteps, the analysis of RNN is usually performed on the unfolded version of the network: the single elements are repeated different times (one for each timestep) and the looping links are now going from the element in the previous time to the next time. In this way a recurrent network is transformed into a multi-layer feedforward network, but keeping the same weights on the unfolded elements that generate from the same recurrent cell.
   </span>
  </p>
  <h4 id="h.2p7ylki847vn">
   <span>
    Backpropagation
   </span>
  </h4>
  <p>
   <span>
    Backpropagation is the algorithm used in neural networks to update the weights, that is used also for the training of recurrent networks, with a slight modification.
   </span>
  </p>
  <p>
   <span>
    The goal of the backpropagation training algorithm is to modify the weights of a neural network in order to minimize the error of the network outputs compared to some expected output in response to corresponding inputs.
   </span>
  </p>
  <p>
   <span>
    It is a supervised learning algorithm that allows the network to be corrected with regard to the specific errors made.
   </span>
  </p>
  <p>
   <span>
    The general algorithm is as follows:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Present a training input pattern and propagate it through the network to get an output
    </span>
   </li>
   <li>
    <span>
     Compare the predicted outputs to the expected outputs and calculate the error
    </span>
   </li>
   <li>
    <span>
     Calculate the derivatives of the error with respect to the network weights
    </span>
   </li>
   <li>
    <span>
     Adjust the weights to minimize the error
    </span>
   </li>
   <li>
    <span>
     Repeat
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    BPTT
   </span>
  </p>
  <p>
   <span>
    The name of the modified algorithm is BackPropagation Through Time (BPTT) and is basically the standard algorithm applied on the unrolled version of the network. The only difference is that, since the layers correspond to different timesteps of the same cell, the weight updates in each instance are summed together.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 201.33px;">
    <img alt="" src="https://lh6.googleusercontent.com/uF6S4ni9jiDhGj956-BgzLeW95wmEnPJ5Ob7IwWOhZNiNz_KvDybtef4dYCibFQ9k80qKe7ATBu-kFx7aI3oBLHdSMbIxVa-IXkmHAmATGD8ZYO02boF8FkUG_fkxRmllFtxN_OV" style="width: 602.00px; height: 201.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <h4 id="h.wp9j82dyzgo5">
   <span>
    Cell types (simple RNN, GRU, LSTM)
   </span>
  </h4>
  <p>
   <span>
    In this paragraph the mainly used cell/unit types are presented. Those cells are the basic unit used for the following architecture. As a layer in feedforward NN is composed of neurons, a layer in a RNN is composed of a recurrent cell.
   </span>
  </p>
  <h5 id="h.1oe97uu0zy2z">
   <span>
    Simple RNN
   </span>
  </h5>
  <p>
   <span>
    The simplest recurrent cell type is a block with two inputs and two outputs. One input is the actual input at the current timestep while the other one comes from the previous timestep (or from initialization on the first time). The two outputs of the cell are equivalent, and is simply to put emphasis on the fact that one of them will go as input to the next timestep (this corresponds to the loop in the not-unfolded representation) and the other one can be passed to the next layer or used as output after applying some other functions (usually a softmax) and/or other layers (recurrent or not).
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 386.00px; height: 276.00px;">
    <img alt="rnn.png" src="https://lh6.googleusercontent.com/AZkM5BzOFANxoiegDMrhW79EXbl11EPFL08VkfsYqqU5cT2t3ZP2Kp156ps6-6kU0vjZ2sbbxBNtgw1F2-JkAzN9hBkYgmFMFBctYGdfGVVPkc8IxAbYWdSRTXe8mmzcnfralGp-" style="width: 386.00px; height: 276.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    The two inputs are concatenated and passed through a single feed-forward layer, that corresponds to a linear transformation plus a non-linear function (e.g. tanh, sigma).
   </span>
  </p>
  <p>
   <span>
    Since the same cell is applied many times in time, and the recurrence loop feeds back the output as inputs, there can easily be two kinds of problem due to the fact that the weight matrix coefficients are multiplied at each timestep:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Exploding gradient: if some coefficients are greater than 1, the output values can become soon very big, making the network insensible to new inputs because is in some way saturated. The solution to this problem is using some non-linear function that limits the values not to be over the value of 1
    </span>
   </li>
   <li>
    <span>
     Vanishing gradient: if some coefficients are near to 0, the network will quickly forget previous inputs and the output won’t depend on them
    </span>
   </li>
  </ul>
  <p>
   <span>
    There are two factors that can affect the magnitude of gradients - the weights and the activation functions (or more precisely, their derivatives) that the gradient passes through.
   </span>
  </p>
  <p>
   <span>
    If either of these factors is smaller than 1, then the gradients may vanish in time; if larger than 1, then exploding might happen. For example, the tanh derivative is smaller than 1 for all inputs except 0; sigmoid is even worse and is always ≤0.25.
   </span>
  </p>
  <p>
   <span>
    Those problem have the same origin: the simple RNN is not able to manage long-term dependencies. This problem has been analyzed in detail by Bengio, et al. (1994)
   </span>
   <sup>
    <a href="#ftnt4" id="ftnt_ref4">
     [4]
    </a>
   </sup>
   <span>
    , Hochreiter et al. (1998)
   </span>
   <sup>
    <a href="#ftnt5" id="ftnt_ref5">
     [5]
    </a>
   </sup>
   <span>
    and other types of cells have been proposed.
   </span>
  </p>
  <h5 id="h.blhs0tl2bxox">
   <span>
    LSTM
   </span>
  </h5>
  <p>
   <span>
    LSTM
   </span>
   <sup>
    <a href="#ftnt6" id="ftnt_ref6">
     [6]
    </a>
   </sup>
   <span>
    is a solution that came out in 1997 in which a more complex cell is considered. The main idea is to have some gates that decide how much of the previous cell state to keep, and how much of the current input to consider for the calculation of the current state and current output.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 556.00px; height: 286.00px;">
    <img alt="rnn (2).png" src="https://lh4.googleusercontent.com/VmRxc0aSR7AZOeJulPfTqmsqy4bm3SZLc1AMIBH8sxs6bxEIbgiFa3w0CSoWJnF9i4mSjSaplDp_60BRsJmOoHxooaMNtjqEkGSFdCJlXRMMvG5wfbvcY1BDY0HC-M927lvi9NTL" style="width: 556.00px; height: 286.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    The gates are ft it and ot, that are:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Forget gate: decides how much of the previous hidden state to keep
    </span>
   </li>
   <li>
    <span>
     Input gate: decides how much of the current input to consider
    </span>
   </li>
   <li>
    <span>
     Output gate: decides how much of the hidden state is exposed to the output
    </span>
   </li>
  </ul>
  <p>
   <span>
    All those gates are implemented with single layer feedforward networks. This type of RNN is able to manage better the long-term dependencies, at the expenses of having four times the parameters. But with sufficient training examples, the network is able to learn how to output the correct values and how to mix the different inputs.
   </span>
  </p>
  <p>
   <span>
    Of this cell exist many implementation, the most common is the basic LSTM that is shown in the picture. Many variations exists (with peephole connections, or other variations on the gates).
   </span>
  </p>
  <p>
   <span>
    MATH FORMULATION
   </span>
  </p>
  <p>
   <img src="https://www.google.com/chart?cht=tx&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=it%3D%CF%83%28Wi%5Ccdot%7B%7D%5Bht-1%2Cxt%5D%2Bbi%29%5C+..."/>
  </p>
  <p>
   <span>
    How LSTM solve vanishing gradient problem:
   </span>
  </p>
  <p>
   <span>
    In the recurrency of the LSTM the activation function is the identity function (the + from c
   </span>
   <span>
    t-1
   </span>
   <span>
    to c
   </span>
   <span>
    t
   </span>
   <span>
    ) with a derivative of 1.0. So, the backpropagated gradient neither vanishes or explodes when passing through, but remains constant.
   </span>
  </p>
  <p>
   <span>
    The effective weight of the recurrency is equal to the forget gate activation. So, if the forget gate is on (activation close to 1.0), then the gradient does not vanish. Since the forget gate activation is never &gt;1.0, the gradient can't explode either.
   </span>
  </p>
  <h5 id="h.32ny97nh7agm">
   <span>
    GRU
   </span>
  </h5>
  <p>
   <span>
    Later studies
   </span>
   <sup>
    <a href="#ftnt7" id="ftnt_ref7">
     [7]
    </a>
   </sup>
   <span>
    have proposed a new type of cell/unit GRU that has only two gates: reset gate and update gate that adaptively control how much each hidden unit remembers or forgets while reading/generating a sequence. The hidden state and the state cell are merged together and therefore the output gate is no more required.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 557.00px; height: 287.00px;">
    <img alt="rnn (3).png" src="https://lh5.googleusercontent.com/0R2ILdqVuioopoMyzGhthdaELUESNy2-JBatSNcENB-3xKBr0o2kfUe9xAQHy8xK3XUqyE4lhK4dl1EEYh1PTXtxlZTSdJwosNY49D-67Bjv4PU-Vb-C1fzxzbgd64aydv-vH7fM" style="width: 557.00px; height: 287.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    The advantage of this type of element with respect to LSTM is that less parameters are used. Being more recent, less studies have used them, but from the performance point of view, they seem to be of the same order of LSTM [this
   </span>
   <sup>
    <a href="#ftnt8" id="ftnt_ref8">
     [8]
    </a>
   </sup>
   <span>
    and this
   </span>
   <sup>
    <a href="#ftnt9" id="ftnt_ref9">
     [9]
    </a>
   </sup>
   <span>
    ].
   </span>
  </p>
  <h4 id="h.enwb9pvnygyi">
   <span>
    Word embeddings
   </span>
  </h4>
  <p>
   <span>
    Since all neural networks only work with numbers, there must be a layer in the network that, given the input words, transforms them in numerical form.
   </span>
  </p>
  <p>
   <span>
    The most simple and naive approach is to consider the “one-hot” vector of the words. This representation is an array with length corresponding to the length of the input dictionary and contains values that are all zeros except for the one whose index corresponds to the index of the word in the vocabulary. This is straightforward to implement, but has some problems because highly depends on the input dictionary and can’t work with words that are not contained. Also it does not consider the similarity between words.
   </span>
  </p>
  <p>
   <span>
    A better approach is to consider a representation of words that consider semantics and syntactic informations. The hypothesis behind this method is the distributional semantics: words that appear in the same context (the context is there defined as the surrounding words) are considered similar, because somehow they can be exchanged the one for the other since they appear in similar contexts. Those representation of the words are called word embeddings and are made up of dense real vectors with a fixed dimension. This dimension is a lot smaller than the size of the input dictionary. The word embeddings are usually pre-trained on large corpuses of unlabeled data (for example the whole wikipedia). From those vectors it’s possible also to compute the similarity of two words and visualizing the word distribution on reduced dimensionality (for example plotting on a 2d plane).
   </span>
  </p>
  <p>
   <span>
    Using word embeddings as inputs to the neural network has the advantages:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Reduced size of input arrays
    </span>
   </li>
   <li>
    <span>
     Semantic and syntactic similarities of words are considered
    </span>
   </li>
  </ul>
  <p>
   <span>
    The embeddings can be part of the model (in this case the weights of the embedding layer are trainable) or can be pre-computed on external bigger corpus. The first option is preferred when the size of the used corpus is big enough, and is thought to be comprehensive enough in terms of word coverage (no unexpected new words in prediction time). Instead when the corpus of the considered problem is not big enough to model the word distribution in terms of syntax and semantics, it’s better to use pre-trained word embeddings.
   </span>
  </p>
  <p>
   <span>
    Pre-trained word embeddings can also be fine-tuned. Fine-tuning has as main advantage to reduce the loss on the desired task, but can also have some disadvantages: if two words are “near” in the pre-trained embedding space, and only one of them is used in a neural network and fine-tuning is applied, the position in the embedding space can change and move far from the other word, not trained in the model. If the other word occurs in model testing (inference), the network will have some problems because the two words are no more similar.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    There are different algorithms of producing those vectors: word2vec, glove.
   </span>
  </p>
  <p>
   <span>
    Some have also derived word embeddings by looking at the single characters (look at fasttext).
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Out Of Word (with respect to pretrained word vectors) Mimicking Word Embeddings using Subword RNNs:
   </span>
   <span>
    <a href="http://aclweb.org/anthology/D/D17/D17-1010.pdf">
     http://aclweb.org/anthology/D/D17/D17-1010.pdf
    </a>
   </span>
   <span>
    (only one citation??) found on
   </span>
   <span>
    <a href="http://blog.aylien.com/highlights-emnlp-2017-exciting-datasets-return-clusters/">
     http://blog.aylien.com/highlights-emnlp-2017-exciting-datasets-return-clusters/
    </a>
   </span>
   <span>
   </span>
   <span>
   </span>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 417.14px; height: 467.50px;">
    <img alt="" src="https://lh3.googleusercontent.com/90HtZION8cUpWoIgOVzcAyiUNNZqNmZ9S-wyS7Vaakky9ogLrbJIY0dyz_j96aE14SlQ0NmCUNuE6alNT-mfVGfpM2gxsdj0ZXhuugLxhwcsrSv8YRpjHjPvHTXjD9uxDanZDrc0" style="width: 417.14px; height: 467.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <h4 id="h.6cgwo7zeuk1q">
   <span>
    Intent classification
   </span>
  </h4>
  <p>
   <span>
    The intent classification task must take as input the sentence and provide on output a label that corresponds to the intent type of the sentence. It is a multi-class classification.
   </span>
  </p>
  <h5 id="h.4hmj3bjv7gq3">
   <span>
    Keyword based
   </span>
  </h5>
  <p>
   <span>
    The first approach that can be considered is keyword-based. In this approach, for each intent type we determine a set of keywords that, if present in the current input, give a score to the selected intent type. For example …
   </span>
  </p>
  <p>
   <span>
    This approach is not good enough because it looks only at some words in the current input sentence.
   </span>
  </p>
  <p>
   <span>
    Also it is difficult to establish which are the keywords for each class in a way that the conflicts are minimized.
   </span>
  </p>
  <p>
   <span>
    A better idea would be to compute a sentence-level representation that summarizes all the words and the meaning of the sentence, and from this vector do a classification on the output labels (intent types). For this sentence vector a lot of different approaches can be applied.
   </span>
  </p>
  <h5 id="h.dazxotng1x74">
   <span>
    Average of word vectors
   </span>
  </h5>
  <p>
   <span>
    First of all, given the word vectors for each word contained in the sentence, an average can be done. This strategy however is not good because it does not consider the order and the relationships between the words. The order matters a lot in natural language.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 316.00px; height: 381.00px;">
    <img alt="rnn (4).png" src="https://lh5.googleusercontent.com/d23uIOBMF_uSPRxH6pfMp-jPIManjpqTRCNnUverdtYPgrWXG8dF72rl-D4uQGYoktXYAQKddFj--NM1i-0dQaSbfn6sY_F-uQ7j2fb8X-Fc55VrYXI-w4bgtSfHCsOyqLIn5J3A" style="width: 316.00px; height: 381.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <h5 id="h.6dcfrbpjww3p">
   <span>
    RNN approach
   </span>
  </h5>
  <p>
   <span>
    To consider the order, a RNN can be used to summarize the sentence and produce a sentence representation that can be used in another layer to classify on the intent types. The output of the RNN is taken only at the end of the sequence. This approach, applied with bidirectional RNN, achieves good results as can be seen in the evaluation section.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 341.00px; height: 461.00px;">
    <img alt="rnn (5).png" src="https://lh3.googleusercontent.com/4tSkMuhbLUAh53q779byGpesW0cEdLAhMiIFIEs8kTuWWP_zgRqWe0MQhZEWFiD5QSHVeO27Rj8dYQKAhkItvxcCEj48hGDJeeca50RhKviF_0V1Eg09YYtxjDA_r6HK0E15Kd9R" style="width: 341.00px; height: 461.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <h5 id="h.6q4caqfmx7jf">
   <span>
    RNN + attention + other stuff
   </span>
  </h5>
  <p>
   <span>
    An attention mechanism can be used to learn a distribution over time of words that tells how much relevant is this word for the task.
   </span>
  </p>
  <p>
   <span>
    Attention used for sentiment analysis: see there
   </span>
   <span>
    <a href="https://arxiv.org/pdf/1703.03130.pdf">
     https://arxiv.org/pdf/1703.03130.pdf
    </a>
   </span>
   <span>
   </span>
  </p>
  <p>
   <span>
    Universal Sentence Representations:
   </span>
   <span>
    <a href="http://aclweb.org/anthology/D/D17/D17-1071.pdf">
     http://aclweb.org/anthology/D/D17/D17-1071.pdf
    </a>
   </span>
   <span>
   </span>
  </p>
  <h4 id="h.e7wkj7v9f1ma">
   <span>
    Sequence to sequence models for slot tagging
   </span>
  </h4>
  <p>
   <span>
    The task of slot tagging / named entity recognition instead consists of generating an output sequence in which each element corresponds to a tag for the corresponding input word. The tag can be directly the entity type or the IOB format can be used. IOB can be useful when there is need to deal with multi-word slots.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    A lot of the architectures listed below have been created for the task of Neural Machine Translation. This task takes as input the sequence of words in the source language and outputs a sequence of words in another language. This is somehow similar to the task of sequence labeling, because it maps an input sequence to an output sequence, but has some differences that make it a lot different and require a different approach. The differences will be explained in details during the analysis.
   </span>
  </p>
  <p>
   <span>
    A common characteristic of them is the presence of two key elements: an encoder and a decoder.
   </span>
  </p>
  <p>
   <span>
    The encoder is responsible of collecting all the useful features on the input sequence. The decoder instead must generate the output sequence. The differences between the different models are on the way that the encoder provides input to the decoding stage.
   </span>
  </p>
  <h6 id="h.1zr6peoxh1xl">
   <span>
    Output dependencies
   </span>
  </h6>
  <p>
   <span>
    Output labels of the words are correlated with the neighbors. For example a “I-ent” is always preceded by a “B-ent” or another “I-ent”. Also there can be some patterns that highlight that after a certain entity it is more probable to find another one. This is called output dependency, that occur at the decoding stage, just before the production of the outputs.
   </span>
  </p>
  <p>
   <span>
    Modeling output dependencies:
   </span>
  </p>
  <ul>
   <li>
    <span>
     local choice: using this approach, each decoding step produces an output that is then projected to the labels. The decision about which label to assign is done locally, performing a simple softmax operation followed by a sampling
    </span>
   </li>
   <li>
    <span>
     feed the previous output together with the current input to decoding timestep (Jordan Network??): in this approach, the output is fed back as input to the next timestep for the decoder network, and the network in training learns the output dependencies
    </span>
   </li>
   <li>
    <span>
     linear-chain CRF: an alternative to RNN for modeling the output dependencies is using a linear-chain CRF. This can substitute the decoder network, and use the encoder to produce its input features. CRF finds the paths with higher energy in a very similar way to RNN. In other words, it finds the output sequence that is mostly probable
    </span>
   </li>
  </ul>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 311.00px; height: 366.00px;">
    <img alt="rnn (6).png" src="https://lh5.googleusercontent.com/FlzKOjyPCM7CcZBkt2CI0zAwVu8_nS2P4vd3DGJSiccNRHsw9KmTAj13yx7KO1j4qvcyASg2oZfZsdSTfaxx7zbIb1vlJix1AsG4aEKHoDiwjR_EttAcJSqjFrllVlOIvyNartOo" style="width: 311.00px; height: 366.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <h5 id="h.ahu5xsb0fuvt">
   <span>
    Simple Encoder-Decoder
   </span>
  </h5>
  <p>
   <span>
    The most simple approach is the one where the encoder collects all the word vectors and using a RNN computes a final representation of the sentence (as in the intent classification task). This representation is passed to the decoder RNN that for each timestep produces an output word.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 402.00px; height: 349.00px;">
    <img alt="rnn (8).png" src="https://lh5.googleusercontent.com/jRpx2GsVDJtyIgPL859hsPfg_FAGyK70u9kAHf-1zOkFkgKgzsn_deX75Ep1yZ_ezqFZoTnIERuVCm-qAi-RlYdW1zeFuaE5aNvaaxvLPkfFprg6gMxMjHO5W_lGKBEPf_yvhZIq" style="width: 402.00px; height: 349.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    This model has some problems. First of all, the decoding part depends only on the sentence vector c, that must be able to keep all the informations on the input sequence, whose length can vary, in a fixed size. Then, the decoding steps may easily loose the relevant information from this vector, since the decoding can take a lot of decoding timesteps. But a much greater problem comes from the lack of constraint on the output sequence: in translation between different languages, this can be a good feature, but in our task of sequence labeling we want an output sequence with fixed length. The last observation we can make is that there is no alignment model between the input and output sequences. All the output depend on all the inputs, without having different encoded informations for the decoding of the output sequence.
   </span>
  </p>
  <h5 id="h.w9da5qrt3zos">
   <span>
    Encoder-decoder keeping sentence vector
   </span>
  </h5>
  <p>
   <span>
    This model comes from a study done in 2014
   </span>
   <sup>
    <a href="#ftnt10" id="ftnt_ref10">
     [10]
    </a>
   </sup>
   <span>
    on the task of neural machine translation. It is a enhanced version of the previously considered model because the sentence vector coming from the encoding stage is passed to all the decoding stages.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 401.00px; height: 350.00px;">
    <img alt="rnn (9).png" src="https://lh5.googleusercontent.com/pZ6U9kWLWB6n7-fDkWukQt9uR9rdfWSJr-9oMvor_uNEVVtxEZOs03vY0NU3pntggFzemBVyhZmqXFlDGyCdF-7UC63vqfNlvqD_Wc80D5Z7ZptvSffEiEYvYVA5AjJSlZEJhs-Q" style="width: 401.00px; height: 350.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    This approach helps the decoding stages that, receiving the sentence vector directly, can perform better in their task. However some problems are not solved: the output sequence length has no constraints and there is no alignment model.
   </span>
  </p>
  <h5 id="h.u4i2e8o9kcra">
   <span>
    Encoder-decoder with aligned inputs
   </span>
  </h5>
  <p>
   <span>
    This model was not born for the task of translation between languages, but has been proposed [3 B. Liu and I. Lane, 2016] specifically for the sequence labeling problem (that can be applied to NER/slot filling, POS tagging). In this model the encoder sends some information to the decoder for each input word instead of sending a single vector at the end.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 476.00px; height: 399.00px;">
    <img alt="rnn (10).png" src="https://lh4.googleusercontent.com/wZWau4rhltIDzvtE--fZx65Jjxp3m6b8A6Aj-JW68_u2CI8Dfvhn9zcX5hAye5ln0C1u70Jgx7QaOoOPIo-mmcceIZnFvQ-42xiUAggCvjkktrfZsD-2EpXJ8K2dMn6QzVEG0L3H" style="width: 476.00px; height: 399.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    This model fixes the output sequence length to the length of the input sequence. The alignment model is fixed: the decisions in decoding are done looking at current input word in the current left+right context.
   </span>
  </p>
  <h5 id="h.g5uc72zc01b8">
   <span>
    Encoder-decoder with attention
   </span>
  </h5>
  <p>
   <span>
    The idea of attention empowers recent studies on translation. The purpose is to decide which output of the encoder are more relevant for the current decoding step dynamically. On the previous model, always the aligned encoded input is used, but for language translation this can be a limitation.
   </span>
  </p>
  <p>
   <span>
    Using the attention, that provides a dynamic alignment model, we can:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Determine which are the encoded inputs that are more relevant
    </span>
   </li>
   <li>
    <span>
     Use them to provide a better translation
    </span>
   </li>
  </ul>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 351.50px; height: 352.24px;">
    <img alt="rnn (11).png" src="https://lh4.googleusercontent.com/bJjT-RS8B0wuc0-dWScWANiigB5p_D1All_tS9o4AESrcZyaCSna21LSzk-leuBrpzj8FdQths7bkQaBP383YHMAfvMy4ABHmHeNHZTe2RG7GsY36w4tbFGK_MCksiaXjk1lg1GZ" style="width: 351.50px; height: 352.24px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 402.00px; height: 446.00px;">
    <img alt="rnn (12).png" src="https://lh5.googleusercontent.com/oLrVgiGmnoDt5yLp_6PU6F_c0GR23T6hFz9eMyHE02txtOSvafvrxenjgaaeoAef8ySyBfLmhkErn9EK3MByiJ9nifVWoMPr5avJSqATAvN8g0jAqmBCnlEyhOlMK2-bPCxWFYyu" style="width: 402.00px; height: 446.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    With this network, used for translations, it is useful to show how the attention maps the words in the two languages. Using a matrix representation that says which input words were used to provide the corresponding output words.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 646.67px;">
    <img alt="Screen-Shot-2015-12-30-at-1.23.48-PM.png" src="https://lh4.googleusercontent.com/4unbZt4D8t_7wToG1hZX7rAWjSgW5s28WptmDgQXCRznBdibjM1oWCsn1PnYVNkhsryroQ2GzmmLA01H6wGmbPI86tdpRkUO9Aj2Xnv2gdmFqgc5csPzjEOYcO31DfbD2SVUx_PR" style="width: 602.00px; height: 646.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
   <sup>
    <a href="#ftnt11" id="ftnt_ref11">
     [11]
    </a>
   </sup>
  </p>
  <p>
   <span>
    The attention model is quite advanced, and since a lot of other parameters are there in the network, a lot more of training samples must be used.
   </span>
  </p>
  <p>
   <span>
    For the task of sequence tagging it seems to be too much. The output labels depend only on the current word context, that is already given by the bidirectional encoder RNN, so the expected values for the attention distribution is that it will keep the inputs and outputs aligned. It can be kept together with the aligned model just to provide more features
   </span>
  </p>
  <h4 id="h.2ijwzgcp6f7s">
   <span>
    Joint SLU
   </span>
  </h4>
  <p>
   <span>
    The two tasks can be combined together in one single network in different ways and with a wide variety of additional things that can be added (for example attention mechanism, output dependencies). The approaches found in literature try to use a common encoding stage and then differentiate on the decoding, one for each task. The network has to fork at some point because the shape of the outputs is different: one single label for the intent and many slot labels, considering a single input sentence.
   </span>
  </p>
  <p>
   <span>
    In [3] there are two different proposed architectures: one is based on the encoder-decoder adding the intent output, while the other collapses all in one single compact structure.
   </span>
  </p>
  <h5 id="h.b8fu65frdqly">
   <span>
    Encoder-decoder model with aligned inputs and attention
   </span>
  </h5>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 204.00px;">
    <img alt="rnn (16).png" src="https://lh3.googleusercontent.com/1MYU7QqZRUMWkROuJiquZP6tyZ13qqULM3D2nCsOQR6PActHzHsq7LLlGZRWHwT-fvspfStScOqsQg2NM_Fz0Vacqqb9pEEouwGzwFUbRyjBDPlFlVEn9Xwi8rHJE-EQye_dGfsj" style="width: 602.00px; height: 204.27px; margin-left: 0.00px; margin-top: -0.14px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    The intent classifier is added as a branch that takes the last state of the encoder and then projects to the intent space with a single layer feedforward.
   </span>
  </p>
  <h5 id="h.m8ph9y69kpg2">
   <span>
    Attention-based RNN model
   </span>
  </h5>
  <p>
   <span>
    The other network proposed in the paper is a bit different. Instead of having a separate RNN for encoder and decoder, it has a single bidirectional RNN, that in the forward direction also has the modeling of slot label dependencies. The intent classification is done on top of the bidirectional RNN output, doing a mean pooling on the states at each timestep or, if attention is enabled, by using a weighted average.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 458.00px; height: 492.00px;">
    <img alt="rnn (15).png" src="https://lh6.googleusercontent.com/YB8miahDVN3fKIuWwNHowwbCGuQaswaAdV8iAUIaDXW8MvTlQFSYEc7GNJEGgYmflnBnHsZ0hH4kJe7AYLcMKSa4sr2laDq3BnqccIazWugKDUATsSDtI3BbXMtF7_8O_tltz9UT" style="width: 458.00px; height: 492.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h3 id="h.1lih8pa124nk">
   <span>
    Beyond the single sentence
   </span>
  </h3>
  <p>
   <span>
    Considering context (previous interactions) and multi-turn interactions
   </span>
  </p>
  <p>
   <span>
    All the previously described networks only care about the current sentence, but human natural language understanding does not limit itself on this strict form. What happens when we meet some people that are already talking about something? It may take some time to understand the topic of the discussion, we need to collect some informations as the interaction goes on to get the point of the discussion. This happens because most of the sentences are not standalone, but belong to a context that goes beyond the single sentence. This feature can be seen as some kind of memory that actors in a conversation need to keep. The context is necessary to understand the dialogue at different levels:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Understand the role of some words in a sentence given some information contained in other sentences (e.g. “To the beach” is the name of a restaurant, because Mary asked John what was the name of it)
    </span>
   </li>
   <li>
    <span>
     Understand the meaning of the current sentence (e.g. in this sentence John told Mary that the restaurant was not as good as he expected)
    </span>
   </li>
   <li>
    <span>
     Understand the topic of the conversation (a synthesized attribute, e.g. John and Mary are talking about their last experience in a restaurant)
    </span>
   </li>
   <li>
    <span>
     Linking the meanings of all the things that were discussed, and being able to answer to some questions (test)
    </span>
   </li>
  </ul>
  <p>
   <span>
    All those levels of understanding are challenges that are felt by the NLP community and have not a general established solution. They fall under different names:
   </span>
  </p>
  <ul>
   <li>
    <span>
     “multi-turn interactions”: specific to virtual assistants, is a name for the first level of understanding: the goal is to identify the intent and entities in a dialogue where the user is not using a single sentence to ask for informations. It is the case where after the initial question of the user the
    </span>
    <span>
     agent asks back for some clarifications / parameters
    </span>
    <span>
     to refine the search. Example: “Send an email to Bob” “What would you like to write?” “Hi Bob, How are you?”. In this case the assistant should put together those sentences in a structure that says: send_email(to=”Bob”, body=”Hi Bob, How are You?”). Other common cases are
    </span>
    <span>
     user follow-up questions
    </span>
    <span>
     . Receiving some results from the agent, the user could ask for more details or to change some parameters. Example: “Find me a chinese restaurant” “Those are the results for chinese restaurants” “What about italian ones?”. In this case, the user refers to the previous query and is only changing the type of restaurant.
    </span>
   </li>
   <li>
    <span>
     “Dialogue state tracking”: mostly known because of the Dialogue State Tracking Challenge. This refers to the last level of understanding a dialogue: represent the dialogue state and updating it as the conversation keeps going on. MORE ON DIALOG STATE TRACKING
    </span>
   </li>
  </ul>
  <p>
   <span>
    Different solutions exist and depend on where we want the neural network to come in contact with the application logic (what to establish as manual rules and what is inferred by the neural-network approach). Literature shows case studies where there is a strict separation between the understanding module and the management of the dialog state, but also cases where everything is put together in an end-to-end fashion in a way more independent from domain rules.
   </span>
  </p>
  <p>
   <span>
    In the first type of systems, the NLU module contains the recurrent neural network stuff and produces in output intent+entities. Once they are extracted from the dialogue, another module “dialogue state tracker” keeps trace of the conversation and applying some handwritten rules decides the flow of the conversation and provides responses back to the user.
   </span>
  </p>
  <p>
   <span>
    Instead in the end-to-end architectures, all the components are trained by dialog examples. The positive points is that there is no need to handwritten rules for the dialogue state tracker. The rules are inferred by the dialog corpus and the system learns what is the most appropriate answer to provide. Some parts are still not part of the trainable model: the module accessing the data exposes some operations via some API. The trainable model learns when to issue API calls.
   </span>
  </p>
  <h4 id="h.wttb2qhy3rtz">
   <span>
    Multi-turn SLU
   </span>
  </h4>
  <p>
   <span>
    As said before, the goal of multi-turn SLU is simply to extract the intent and the entities when the user is not providing a single sentence with all the required parameters. In a second moment, could be after the agent asks back for some parameters, other sentences complete the initial one with more entities that are used to refine the search. It can be seen as an iterative filling of a fixed-structure form. For this reason, a naive approach could be simply to classify the intent on the first sentence and then collecting the parameters in a key-value fashion (the key is the name of the slot, that is asked by the agent, and the value is the answer taken “as is”).
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 326.00px; height: 136.00px;">
    <img alt="" src="https://lh4.googleusercontent.com/pdSLTBgzqcYwLkOBPsLY2t1jtzclpNaoeXCgcqmAd0Kt7sKL_InEE_qGYUBIaPL_W9o3Fw_2ULN-1wT-F8VCds2fgwq5IiMSFeUc8Y-JvHLadfvyklsobBqz_urhzYVTWXIs7fb2" style="width: 326.00px; height: 136.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    This approach is quite simple but has some problems: this is not natural language. Natural language dialogue has not this fixed structure, and the system must be able to receive a sentence that contains more than a single entity. Another issue is that the user may change his mind in the middle of the interaction and start a new intent. For those reasons the SLU task should be expanded to the multi-turn environment by a more complete approach.
   </span>
  </p>
  <h5 id="h.h5tesva7dlyr">
   <span>
    Contextual SLU [Chen et al. 2016
   </span>
   <sup>
    <a href="#ftnt12" id="ftnt_ref12">
     [12]
    </a>
   </sup>
   <span>
    ]
   </span>
  </h5>
  <p>
   <span>
    This is an approach that has been studied for the multi-turn SLU. The idea is to additionally incorporate contextual knowledge during slot tagging.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 324.00px;">
    <img alt="" src="https://lh3.googleusercontent.com/3ae-Y63logbUiiUI-dpTT5F-hNHhWR1mrt9UDxG0yGv3KMXf4sYqxSvu-vW99N6exGBRRT2jA3Q75x1rKs8r1KiVjAZzknJIFLr9u_f5y2DIcZlwZtTzuv4LLje9IJoCvcNzeMrU" style="width: 602.00px; height: 324.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    The main difference with the encoder-decoder structures that were previously explained is that also informations from the previous sentences are provided to the decoding stage. Those information are computed by doing a weighted sum of all the previous encoded sentences. To compute the weights, that represent the level of attention that has to be kept to the relative sentences, an inner product is performed to obtain a similarity score between the current sentence and the old ones. The hypothesis behind this scoring is that sentences that have similar encoded representation should be considered more than the ones that are different.
   </span>
  </p>
  <p>
   <span>
    The results of this approach have been evaluated on a proprietary dataset and therefore the results cannot be compared with other studies on the field. Furthermore in the paper the intent network is not described at all. However, this study is interesting in the perspective of how the previous sentences are considered together with the current one for the slot-tagging task.
   </span>
  </p>
  <p>
   <span>
    A problem of this architecture is that the agent sentences are not considered. But the agent sentences could contain some keywords that may help to identify the slot in the decoding stage. For example if a trip requires a source and a destination, the agent asked for the source and the user answered with that, it is very relevant for the task of slot filling to know that the agent asked for source and not for the destination.
   </span>
  </p>
  <h5 id="h.hywob0csablx">
   <span>
    Considering time and roles [Chen et al. 2017
   </span>
   <sup>
    <a href="#ftnt13" id="ftnt_ref13">
     [13]
    </a>
   </sup>
   <span>
    ]
   </span>
  </h5>
  <p>
   <span>
    Another study by the same group has been done on the value of time and roles in conversation. About the time, the idea is that most recent sentences count more, and an attention score is given with values that fade out as the time is more remote. Instead for the roles, two similar networks are used, one for each role.
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 258.67px;">
    <img alt="" src="https://lh4.googleusercontent.com/EW59TRmjAztp0pSFyaGysYbl0iJx0jbQuSU8095V6vqWn4lmV9Uafbllus6LLRnlBMmpPTp7FW-t9__cswTsN-kYRk39KRY0wGzH1A9-cDPXQ4dcMh-BzI2mlRrIPvflL8cMssbi" style="width: 602.00px; height: 258.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    We can see from this picture that the history of sentences is encoded with different networks, one for the tourist and one for the agent. Two different mechanism of attention are used: one is the content-aware, that is computed in the same way as in the previous mentioned paper with a dot product in the embedding space; the second one is the time-aware, that gives higher importance to recent sentences.
   </span>
  </p>
  <p>
   <span>
    In this case the work has been done on the DSTC4 dataset that contains human-to-human dialogues. One of them is a tourist and the other one is a guide.
   </span>
  </p>
  <h5 id="h.exv78bh8rv9v">
   <span>
    Other contextual SLU
   </span>
  </h5>
  <ul>
   <li>
    <span>
     Contextual domain classification in SLU using RNN
    </span>
    <span>
     <a href="https://www.clsp.jhu.edu/~puyang/papers/rnn_dom.pdf">
      https://www.clsp.jhu.edu/~puyang/papers/rnn_dom.pdf
     </a>
    </span>
    <span>
    </span>
    <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 526.00px; height: 338.00px;">
     <img alt="" src="https://lh6.googleusercontent.com/eloQnhOcbiuZD4YhNqG-ZQCBbyYPpU4oTldCI_mV_cLSqBjlvPCaN7kEQ6trxUZug6fCEgUE-AWcerLK8tZKvb0ra-6ItoRCmmVHwmxG8UNn53UebG5tOpTG_pHxG74FE7lV3Nob" style="width: 526.00px; height: 338.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
    </span>
   </li>
   <li>
    <span>
     Easy contextual intent prediction and slot detection
    </span>
    <span>
     <a href="https://pdfs.semanticscholar.org/cd2f/0c6aac6cdf98f96984ac37d106afc599d99d.pdf">
      https://pdfs.semanticscholar.org/cd2f/0c6aac6cdf98f96984ac37d106afc599d99d.pdf
     </a>
    </span>
    <span>
    </span>
    <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 519.00px; height: 418.00px;">
     <img alt="" src="https://lh3.googleusercontent.com/4yGWJAgkDoSB0Z9WGTBAQHq03HT6Zhk6-fj9HZ_kb_B2gpCbEgtc-PljtmEjTRRKcOCxVPMB-BnyCnncjG9lgjanV1yNCXq1g3oa8BJTD1atUXcrEP1VQTCgflTw3Dns4_8XlZ3R" style="width: 519.00px; height: 418.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
    </span>
    <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 527.00px; height: 458.00px;">
     <img alt="" src="https://lh5.googleusercontent.com/M16aOfNZkvZB3swYsEkvEdc-XQpxveBoV5gex0aJmKsjxfvtjzwqezLfw34Qg5mV9DC6H9TpW5zrKgwv8g3gHrBN8Cw0GqkHtCwcMOVKWqQvtzmVJMGPFgxmvu9s6DXpWve0Jcmq" style="width: 527.00px; height: 458.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
    </span>
   </li>
  </ul>
  <h4 id="h.13sbtqu88z5j">
   <span>
    Dialogue tracking systems
   </span>
  </h4>
  <p>
   <span>
    Explain something about DSTC.
   </span>
  </p>
  <p>
   <span>
    The main goal for those systems is to keep track of a dialogue in terms of what is being said. The architectures always make use of some memory, usually RNN. Is this really different from end-to-end dialog? The answer should be yes, because now only tracking the state of the dialogue, in end-to-end dialog the goal is to predict next sentence/action.
   </span>
  </p>
  <h4 id="h.kuqp0e77ccoi">
   <span>
    Semantic Question Answering
   </span>
  </h4>
  <p>
   <span>
    Instead of doing a classification of sentences on some pre-defined intents, the goal of those systems is to transform the user sentence in a database query. This requires a strong relation between the NL module and the Information Retrieval module. Informations are in the form of RDF (Resource description framework).
   </span>
  </p>
  <p>
   <span>
    The sentences are analyzed using some parsers and the sentence is mapped to a set of linguistic patterns.
   </span>
  </p>
  <p>
   <span>
    Main problem:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Lexical gap: the same meaning can be expressed in different ways
    </span>
   </li>
   <li>
    <span>
     Manually designed features: the linguistic patterns are created by a domain expert
    </span>
   </li>
  </ul>
  <p>
   <span>
    The systems represented are good for question answering systems where the questions are related to known facts and actors that are publicly known and retrievable using URIs.
   </span>
  </p>
  <p>
   <span>
    <a href="http://www.semantic-web-journal.net/content/survey-challenges-question-answering-semantic-web">
     http://www.semantic-web-journal.net/content/survey-challenges-question-answering-semantic-web
    </a>
   </span>
   <span>
   </span>
  </p>
  <p>
   <span>
    TODO move this in the types of bots:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Chit chat
    </span>
   </li>
   <li>
    <span>
     Task bot
    </span>
   </li>
   <li>
    <span>
     Question-answering about the world (this one)
    </span>
   </li>
  </ul>
  <h4 id="h.5h19s2dnfiyp">
   <span>
    End to End Goal-oriented dialog (generative)
   </span>
  </h4>
  <p>
   <span>
    As said before, this approach wants to overcome the need of domain-specific handcrafting in favour of a more generic system that can fastly be used for new domains. This approach has its origins in another type of setting: general chit-chat dialogs. From corpuses of discussions (forum threads / movie conversations) the system is trained in a end-to-end fashion to predict the next sentence. The porting from this scenario to goal-oriented ones has some issues, that generate from the fact that the conversation is intrinsically different in the purpose: the system is interrogated by the user and has to provide some information back. Beyond keeping the conversation smooth, the agent has to ask questions to the user to have a better formulation of the request, query the Knowledge Bases, interpreting the results and provide them to the user in the correct shape in order to complete a transaction.
   </span>
  </p>
  <p>
   <span>
    In this type of system the prediction are both the agent responses and the API calls.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Learning end-to-end goal-oriented dialog
   </span>
  </p>
  <p>
   <span>
    Paper
   </span>
   <span>
    <a href="https://arxiv.org/pdf/1605.07683.pdf">
     https://arxiv.org/pdf/1605.07683.pdf
    </a>
   </span>
   <span>
   </span>
  </p>
  <p>
   <span>
    Implementation
   </span>
   <span>
    <a href="https://github.com/vyraun/chatbot-MemN2N-tensorflow">
     https://github.com/vyraun/chatbot-MemN2N-tensorflow
    </a>
   </span>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <hr style="page-break-before:always;display:none;"/>
  <p>
   <span>
   </span>
  </p>
  <h2 id="h.bhhmsohw8kph">
   <span>
    Personalization/Recommendation
   </span>
  </h2>
  <p>
   <span>
    “Computer-based personality judgments are more accurate than those made by humans”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: compare accuracy of personality judgment done by computers against those made by humans. Using likes to judge five traits: openness, agreeableness, extraversion, conscientiousness and neuroticism
    </span>
   </li>
   <li>
    <span>
     Criterions:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Self-other agreement: how much external judger (computer or human) agrees with self-rating
    </span>
   </li>
   <li>
    <span>
     Interjudge agreement: similarity of rating given by two external judger (human or computer-&gt;training set divided in two parts)
    </span>
   </li>
   <li>
    <span>
     External validity: measuring the prediction on life outcomes (facts that can be verified) based on the traits of personality computed (computer) or self-assigned (human)
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Results: computer are more accurate. But there are some differences:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Computer-based judgments can take into account a very big quantity of data and use statistical modeling
    </span>
   </li>
   <li>
    <span>
     Humans can capture more subtle cues
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Private traits and attributes are predictable from digital records of human behavior”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: using data of social network profiles (user likes) analyze the prediction rate of non-published personal data -&gt; 10-fold cross-validation
    </span>
   </li>
   <li>
    <span>
     Results: the majority of informations are accurately predicted. Especially the ethnicity, gender, age. The accuracy increases with the number of likes available, but even with a small data set, the prediction is accurate for some attributes
    </span>
   </li>
   <li>
    <span>
     Implications of being highly predictive:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Recommendation services can be improved, without explicitly asking some information but inferring them
    </span>
   </li>
   <li>
    <span>
     Unwilling use of details that user wanted to hide, considered as personal. This implies a decrease in trust of online services
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Share Like Recommend”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: analyze news as social activity
    </span>
   </li>
   <li>
    <span>
     Results:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     users receive news from friends on social networks, a lot more than directly from news agencies. Friends act like a filter, by sharing news.
    </span>
   </li>
   <li>
    <span>
     News as a shared social experience is appealing for customers. Main reason for using social networks is sharing content: events, news
    </span>
   </li>
   <li>
    <span>
     Journalists on social network seem to be important for users
    </span>
   </li>
   <li>
    <span>
     Using social media for retrieving news is not done at expenses of mainstream media outlets
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Tag-Based User Profiling for Social Media Recommendation”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: use tags to support user profiling. Tags connect entities directly, while collaborative filtering must search for relationships between user preferences and item attribute
    </span>
   </li>
   <li>
    <span>
     Implementation:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Types of tags (have different weights):
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Personal view: tags specified by the user
    </span>
   </li>
   <li>
    <span>
     Social view: tags specified by the friends on the user contents
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Tag-to-Tag Matrix: correlations between person-attribute set (tags on user) and content-attribute set (tags on contents)
    </span>
   </li>
   <li>
    <span>
     User-feature vector: for each idea mentioned in a specific content, the probability that the user has to like it. If it is above a threshold, it is recommended
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Experiment (using social bookmarking site del.icio.us):
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     A long-tail model applies (URLs are bookmarked by few people). Very low precision (2.77%) because there are not central topics to be defined. Also caused by dependence on order of visit (if user visits a better site before, decision about bookmarking a site on the same topic is lower)
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Cross Social Media Recommendation”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: merge data from different social networks (Twitter and Weibo) to remove biasing of models based on a single social network. Recommend twitter hashtag to weibo
    </span>
   </li>
   <li>
    <span>
     Implementation:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     “Pseudo Global Social Media Network (PGSMN) model to interconnect users (with similar interests, not the same user because the sets are separate) and topics
    </span>
   </li>
   <li>
    <span>
     Three layers: Twitter, Weibo and Wikipedia. The last one is a bridge. Each layer contains intra-layer links (users to tags or page to category) and inter-layer links (tags to pages/category)
    </span>
   </li>
   <li>
    <span>
     Explicit Semantic Path Mining (ESPM), derived from Explicit Semantic Analysis (ESA), identifies semantic paths from Wikipedia hierarchical relationships
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Measurements of ranking performance:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Precision
    </span>
   </li>
   <li>
    <span>
     Mean average precision
    </span>
   </li>
   <li>
    <span>
     Normalized discounted cumulative gain
    </span>
   </li>
  </ul>
  <h3 id="h.tm5ee3a05f8l">
   <span>
    Accessing user data on facebook (discarding it for the moment)
   </span>
  </h3>
  <p>
   <span>
    The data that is available directly from the messenger API is the following: first_name,last_name,profile_pic,locale,timezone,gender,is_payment_enabled
   </span>
  </p>
  <p>
   <span>
    In order to be able to do personalization, there exists a way to link the messenger id (page-scoped) to other ids (also facebook id indirectly):
   </span>
   <span>
    <a href="https://developers.facebook.com/docs/messenger-platform/account-linking">
     https://developers.facebook.com/docs/messenger-platform/account-linking
    </a>
   </span>
  </p>
  <p>
   <span>
    Once the facebook id is retrieved, it is possible to use the facebook graph API.
   </span>
  </p>
  <p>
   <span>
    To the external account the messenger platform sends a request to an url specified by the developer, adding account_linking_token and redirect_uri (page to redirect user after login). The external site, if login successful, redirects to redirect_uri with authentication_code custom (maybe put there the id in order to allow the bot to do the join)
   </span>
  </p>
  <p>
   <span>
    From the other side, the oauth flow:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Redirect user to facebook oauth with client_id (the app id) and redirect_uri (where facebook will send the user)
    </span>
   </li>
   <li>
    <span>
     After that user gives permissions, fb will redirect to redirect_uri with other parameters
    </span>
   </li>
  </ul>
  <p>
   <span>
    The procedure seems quite complicated and requires a web server component that interacts on one side with messenger platform and on the other side handles the facebook login.
   </span>
  </p>
  <h3 id="h.gjp0hmp20g33">
   <span>
    Cold start
   </span>
  </h3>
  <p>
   <span>
    Another possible approach, without using the facebook data, would be to use a bootstrap in the
   </span>
   <span>
    cold start
   </span>
   <span>
    phase: a few questions to build a first model of the user. Then more informations can be
   </span>
   <span>
    extracted as the conversation flows.
   </span>
  </p>
  <p>
   <span>
    An explicit preference elicitation at the beginning, not too long, that then becomes implicit.
   </span>
  </p>
  <p>
   <span>
    Other systems apply this procedure (e.g. netflix)
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Facing the cold start problem in recommender systems”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: find a good solution for the user-side cold start
    </span>
   </li>
   <li>
    <span>
     Other approaches to the problem:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Content-based: requires ratings by user. Cold start problem
    </span>
   </li>
   <li>
    <span>
     Collaborative-filtering: requires other users ratings
    </span>
   </li>
   <li>
    <span>
     Explicit interview to new user about items (adapting new questions to the answers provided)
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Implementation: three phases (Collaborative-filtering)
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Classification of the new user in a specific group (based on demographic data): using C4.5 algorithm (decision tree) and Naive Bayes
    </span>
   </li>
   <li>
    <span>
     Find “neighbours” of the new user inside the group: weighted average of demographic data
    </span>
   </li>
   <li>
    <span>
     Calculation of outcome: prediction techniques
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Dealing with the new user cold-start problem in recommender systems: A comparative review”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: compare existing algorithms (collaborative filtering) on the cold-start problem. Types of systems:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Uses additional data sources (e.g. demographic data): a limitation of these systems is that sometimes data is not available (because user did not associate social profile)
    </span>
   </li>
   <li>
    <span>
     Selects a group of analogous users (without additional data sources): construct a decision tree where nodes are questions. NHSM also takes into account the global preference of user behaviors, using three factors of similarity: Proximity (how much two ratings are near), Significance (how much distant from the median) and Singularity (how the two ratings are different from others). Limitations: how to choose the optimal number of groups and splitting criteria. Must have some rating from new user (bootstrap)
    </span>
   </li>
   <li>
    <span>
     Hybrid methods:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     FARAMS: using multiple approaches (fuzzy sets and association rules).
    </span>
   </li>
   <li>
    <span>
     HU-FCF: combines analysis on demographic data (fuzzy similarity matrix) and on rating data (hard similarity matrix)
    </span>
   </li>
   <li>
    <span>
     Limitations: irrelevant users are still included in the computation of similarities
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Results: NHSM is the best one, both for accuracy and for computational time
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Learning multiple-question decision trees for cold-start recommendation”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Proposition: find a solution to the cold-start recommendation problem that maximizes accuracy and minimizes user efforts. Problem of classic bootstrap is that user usually does not know items in the first interactions. The idea is to build a tree with multiple questions at each node.
    </span>
   </li>
   <li>
    <span>
     Results:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Accuracy is better than single-question based system (because for each node/page more informations are extracted) and also better than linear-combination of multiple trees
    </span>
   </li>
   <li>
    <span>
     Time increment to answer more questions per screen is sublinear
    </span>
   </li>
   <li>
    <span>
     Time difference between rating scale and binary answer doesn’t have strong dependency on the number of questions
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    “Matrix Factorization Techniques for Recommender Systems”
   </span>
  </p>
  <ul>
   <li>
    <span>
     Collaborative filtering Approaches:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Neighborhood methods:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     User-oriented: find similar users to the target user, and from them find the items that they like
    </span>
   </li>
   <li>
    <span>
     Product-oriented: find items similar to the ones liked by the target user
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Latent factor models: characterize items and users on some factors (user/item vector) inferred from rating patterns (explicit or implicit feedback). Predicted rating is the dot product between the two vectors
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Learning algorithms for extracting the factor vectors:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Stochastic gradient descent: easier and faster
    </span>
   </li>
   <li>
    <span>
     Alternating least squares: can be parallelized, better on sparse data
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Temporal dynamics: the model should be dynamic because some terms vary over time (item biases, user biases and user preferences)
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <h3 id="h.27kmyxe8tkxx">
   <span>
    Proactive experience
   </span>
  </h3>
  <p>
   <span>
    <a href="http://www.kdd.org/kdd2016/papers/files/adf0295-sunA.pdf">
     http://www.kdd.org/kdd2016/papers/files/adf0295-sunA.pdf
    </a>
   </span>
   <span>
    get things done before you ask. “utilizes collaborative capabilities among users, and learns, for each user, a personalized dynamic system that effectively models the sequential correlation among contextual signals and intent”
   </span>
  </p>
  <hr style="page-break-before:always;display:none;"/>
  <p>
   <span>
   </span>
  </p>
  <h1 id="h.ral1n0tuv49">
   <span>
    Approach
   </span>
  </h1>
  <h2 id="h.p35bk62b32kr">
   <span>
    Multichannel Support
   </span>
  </h2>
  <p>
   <span>
    Conversational agent separate from the messaging platform. Using a generic message-proxy (see botkit) that will handle all the specific features of the platforms. The conversational agent should be independent.
   </span>
  </p>
  <h2 id="h.dzssnlm4kvyj">
   <span>
    NLU
   </span>
  </h2>
  <ul>
   <li>
    <span>
     NLU: extraction of intent and entities (transforming language into actionable data)
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     User intents: (
    </span>
    <span>
     <a href="https://github.com/ryankiros/skip-thoughts">
      https://github.com/ryankiros/skip-thoughts
     </a>
    </span>
    <span>
     )
    </span>
   </li>
  </ul>
  <ol start="1">
   <li>
    <span>
     Search a bike
    </span>
   </li>
   <li>
    <span>
     Search a free slot
    </span>
   </li>
   <li>
    <span>
     Plan a trip (1+2)
    </span>
   </li>
   <li>
    <span>
     Communicate location
    </span>
   </li>
   <li>
    <span>
     Get informations about a station
    </span>
   </li>
   <li>
    <span>
     Find closest station to a certain point
    </span>
   </li>
   <li>
    <span>
     Search other information (restaurant, cafe, …)
    </span>
   </li>
   <li>
    <span>
     Basic interactions (greet/thank)
    </span>
   </li>
   <li>
    <span>
     Feedback (on recommendation/place/system)
    </span>
   </li>
   <li>
    <span>
     Informations on the bot (who are you? What can you do?)
    </span>
   </li>
   <li>
    <span>
     Setting preferences:
    </span>
   </li>
  </ol>
  <ol start="1">
   <li>
    <span>
     User favorite places (role → place)
    </span>
   </li>
   <li>
    <span>
     Enable/disable unsolicited messages
    </span>
   </li>
   <li>
    <span>
     Customization
    </span>
   </li>
  </ol>
  <ul>
   <li>
    <span>
     Entities:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     location to/from
    </span>
   </li>
   <li>
    <span>
     time
    </span>
   </li>
   <li>
    <span>
     Recognise some values (home/work)
    </span>
   </li>
   <li>
    <span>
     Disambiguation: retrieve the right entity if it is not uniquely identified (contextualize with current city)
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <h3 id="h.bc96kppcy8e3">
   <span>
    Where the NN leaves the space to handwritten rules
   </span>
  </h3>
  <p>
   <span>
    Before describing the NLU module, it is important to understand one thing.
   </span>
  </p>
  <p>
   <span>
    Using NN for everything could be good only with lots of data. Something need to be handwritten. For example the fact that “to search for a bike you should provide a search criterion or the last known position should be known and recent (2hr or ask for confirmation)” is not manageable from a data-only focused approach. Inside the domain it is better to provide static rules that are handwritten in some configuration files. Although there are several studies on end-to-end goal-oriented dialog, for the implementation of this prototype a rule-based logic has been chosen. The neural network are only applied in the NLU module that is responsible to extract intent and entities from the current dialog (multi-turn)
   </span>
  </p>
  <h3 id="h.4xv0kvc2etve">
   <span>
    Statefulness of architecture for multi-turn SLU
   </span>
  </h3>
  <p>
   <span>
    For solving the problem of multi-turn SLU, after analyzing all the ideas about keeping the context, the decision has been to change the single-turn architecture to make it stateful. Making it stateful means that the RNN cells involved will have the initial hidden state equal to the last hidden state. The hope is that initializing the RNN cells will make the work on the current sentence depend also on previous sentences, both for the intent classification and for the slot tagging. The current sentence may not contain indicators of the intent that is contained in previous user sentence. And specifically for the slot filling, we decided to consider also the agent sentence in order to provide the decoding stage. Since the model is a single one, the agent turn words are used in both the intent classification and in the slot filling networks.
   </span>
  </p>
  <p>
   <span>
    Additional work on the dataset:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Identifying the sessions
    </span>
   </li>
   <li>
    <span>
     Aligning (initializing with the previous state of the same session)
    </span>
   </li>
  </ul>
  <p>
   <span>
    Architecture:
   </span>
  </p>
  <p>
   <span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 388.00px;">
    <img alt="" src="https://lh3.googleusercontent.com/hPOJ7glI3EQi0OVAOQYS6u0WgDA_CgeWEG4uUHanTlqJQ7NKYuFMEmihEI61dts-1q3Kw_A3zrRAvWzm9pfgAcKb-Qv7ZcViaCUmznmY10oTTZEcKyyP29eZhIdGYecTFX6hBJDQ" style="width: 602.00px; height: 391.23px; margin-left: 0.00px; margin-top: -1.62px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""/>
   </span>
  </p>
  <p>
   <span>
    The architecture chosen is basically the same of the encoder-decoder with aligned inputs for joint SLU. The part of the network for the intent, instead of taking simply the last state of the encoder, takes all the values of the encoder and summarizes them using a RNN. The paper [3] only used the last encoder state for the first proposed architecture, and in the second architecture used a mean pooling over the encoded states, additionally using an attention mechanism. An RNN was chosen because it can learn how to put together all the encoded states, and also because it is able to keep memory, that is what we want in a multi-turn environment.
   </span>
  </p>
  <p>
   <span>
    For the memory between turns, the hidden states of the RNN cells are saved for each user, and are restored back in the cells when a new sentence is received from the same user. To also consider the agent turn, the network receives as inputs not only the current user turn words, but also the agent turn words. Those won’t produce in output slot labels, because is not our interest, but will affect the RNN cells in the forward direction.
   </span>
  </p>
  <p>
   <span>
    Looking at the complete structure, it can be noticed that the memory of the states only initializes the forward cells. This is wanted, because in this way the new turn is affected by the old one, and the backward links are not connected through different turns because the decisions that have been taken in the past cannot be modified by next turns.
   </span>
  </p>
  <p>
   <span>
    A critical point where this model should prove the goodness of the chosen cell, is when two completely uncorrelated sentence follow each others (e.g. two different intents). In this case the LSTMs/GRUs should learn to forget their past state in order to provide the new values. In inference time we can never know whether a certain sentence is a follow-up or is a new intent without any logical linking with the previous one, so the only good option is to train the network to learn when this happens. For this motivation, instead of training on single independent sessions, the batch size has been reduced to 1 and all the sessions have been concatenated. In this way the model will be able to work better both on sentences that belong to the same session (for example answering back to a missing slot) and on other independent questions (for example an independent intent).
   </span>
  </p>
  <p>
   <span>
    Batch_size=1
   </span>
  </p>
  <ul>
   <li>
    <span>
     The order of samples is chosen in the program flow
    </span>
   </li>
   <li>
    <span>
     Can avoid padding:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     No mess with masks
    </span>
   </li>
   <li>
    <span>
     In stateful RNN padding is likely to deteriorate performances
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Performance decreases (only at training/evaluation time, not in usage because sentences to the bot are likely not to arrive all together)
    </span>
   </li>
  </ul>
  <h2 id="h.9vi2nf26bxcs">
   <span>
    Personalization
   </span>
  </h2>
  <h3 id="h.2t419x4gurpa">
   <span>
    Content
   </span>
  </h3>
  <ul>
   <li>
    <span>
     Manifest variables: gender, age, profession, … &lt;- may be irrelevant (bootstrap/social)
    </span>
   </li>
   <li>
    <span>
     Habits variables: recurrent locations with roles &lt;- interaction
    </span>
   </li>
   <li>
    <span>
     Latent/hidden variables:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     health/sportive
    </span>
   </li>
   <li>
    <span>
     Arts
    </span>
   </li>
   <li>
    <span>
     Big five personality traits (feasible?)
    </span>
   </li>
   <li>
    <span>
     Pre-defined or internally created by the model
    </span>
   </li>
   <li>
    <span>
     <a href="http://goodcitylife.org/happymaps/">
      http://goodcitylife.org/happymaps/
     </a>
    </span>
    <span>
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Extract data to build user model (manifest variables):
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Bootstrap questions (explicit or by social media) to extract relevant informations in cold start phase:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Age
    </span>
   </li>
   <li>
    <span>
     Gender
    </span>
   </li>
   <li>
    <span>
     Profession
    </span>
   </li>
   <li>
    <span>
     What the user wants to be recommended on
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Habits (can be derived from interactions):
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Home location
    </span>
   </li>
   <li>
    <span>
     Work/school location
    </span>
   </li>
   <li>
    <span>
     Other recurring places
    </span>
   </li>
   <li>
    <span>
     Timing informations
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Feedback from suggestions:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Attitude to recommendations (important)
    </span>
   </li>
   <li>
    <span>
     Interests
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Suggestion engine:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Trip suggestion and reminders (must not be invasive):
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Previously used stations
    </span>
   </li>
   <li>
    <span>
     Previous schemes/timetables
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Provide informations on the city context (not strictly related to domain)
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     User relevant places on the way
    </span>
   </li>
   <li>
    <span>
     Time relevant (suggested place should be open)
    </span>
   </li>
  </ul>
  <h3 id="h.h3t6ly4gdrse">
   <span>
    Interaction
   </span>
  </h3>
  <p>
   <span>
    Operative modes, ...
   </span>
  </p>
  <h2 id="h.jvb587u3mpo8">
   <span>
    Data sources
   </span>
  </h2>
  <ul>
   <li>
    <span>
     bike sharing (easy part)
    </span>
   </li>
   <li>
    <span>
     Context of the city: events, places. Possible source of data:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Foursquare
    </span>
    <span>
     <a href="https://developer.foursquare.com/docs/venues/explore">
      https://developer.foursquare.com/docs/venues/explore
     </a>
    </span>
    <span>
     can easily ask for suggestion on venues with specific category/property (120.000 requests per day, bigger coverage)
    </span>
   </li>
   <li>
    <span>
     Google places API
    </span>
    <span>
     <a href="https://developers.google.com/places/web-service/search">
      https://developers.google.com/places/web-service/search
     </a>
    </span>
    <span>
     provides similar features (1.000 requests per day)
    </span>
   </li>
   <li>
    <span>
     <a href="https://www.quora.com/What-are-the-pros-and-cons-of-each-Places-API">
      https://www.quora.com/What-are-the-pros-and-cons-of-each-Places-API
     </a>
    </span>
    <span>
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Weather forecasts: if bad weather is expected, tell user
    </span>
    <span>
     <a href="https://www.wunderground.com/weather/api/">
      https://www.wunderground.com/weather/api/
     </a>
    </span>
    <span>
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <ul>
   <li>
    <span>
     External profile data interface (consider technical difficulties) for more personalization and less explicit interview. If present, should be optional (not every user are on facebook, and also if they are, they may not want to share their profile)
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <h3 id="h.j4qm78dpama7">
   <span>
    Language understanding process
   </span>
  </h3>
  <ol start="1">
   <li>
    <span>
     Reduction to patterns: NLP with spaCy
    </span>
   </li>
  </ol>
  <ol start="1">
   <li>
    <span>
     get the grammatical relevant structure
    </span>
   </li>
   <li>
    <span>
     Reduce synonyms, reducing separately the Parts Of Speech
    </span>
   </li>
  </ol>
  <ol start="2">
   <li>
    <span>
     From patterns to intents
    </span>
   </li>
  </ol>
  <ol start="1">
   <li>
    <span>
     Applying a classifier (RNN LSTM or simple NN)
    </span>
   </li>
  </ol>
  <ol start="3">
   <li>
    <span>
     Intent processing
    </span>
   </li>
  </ol>
  <ol start="1">
   <li>
    <span>
     Get entities
    </span>
   </li>
  </ol>
  <ol start="4">
   <li>
    <span>
     Contextualization
    </span>
   </li>
  </ol>
  <ol start="1">
   <li>
    <span>
     Disambiguate (non-global entities contextualized)
    </span>
   </li>
  </ol>
  <ol start="1">
   <li>
    <span>
     Incomplete names (e.g. a store name, where multiple stores with this name exist)
    </span>
   </li>
   <li>
    <span>
     Variables (e.g. home, work)
    </span>
   </li>
  </ol>
  <ol start="5">
   <li>
    <span>
     Checking required informations (do some questions if missing) and pass them to the modules that use them (bike-sharing, meteo, places, …)
    </span>
   </li>
  </ol>
  <p>
   <span>
   </span>
  </p>
  <h3 id="h.m202m0o42x2b">
   <span>
    Recommendation phases
   </span>
  </h3>
  <ol start="1">
   <li>
    <span>
     SSH (social sciences and humanities):
    </span>
   </li>
  </ol>
  <ol start="1">
   <li>
    <span>
     Input: bootstrap/social data, episodic KB
    </span>
   </li>
   <li>
    <span>
     Output: user features (can be described and are kind of explicit dimensions)
    </span>
   </li>
  </ol>
  <ol start="2">
   <li>
    <span>
     Recommender system:
    </span>
   </li>
  </ol>
  <ol start="1">
   <li>
    <span>
     Input: user features + stimulus (what type of recommendation)
    </span>
   </li>
   <li>
    <span>
     Output: recommendation
    </span>
   </li>
   <li>
    <span>
     Internally using user profile vector that contains model-generated dimensions (can match or not the user features)
    </span>
   </li>
  </ol>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Finding recurrent actions:
   </span>
  </p>
  <p>
   <span>
    The EpisodicKB of actions describes events like: “on day DD/MM/YYYY time HH:MM user XXX searched for a bike at place PPP”. The place is not a simple position but is an external key to a Place row. Those places contain info that come from the contextual knowledge derived from some PlacesAPI (externally stored), or are stations or other specific positions with a role (home/work/school) (internally stored)
   </span>
  </p>
  <p>
   <span>
    This table is monitored to find patterns that are then stored in the RecurrentAction table
   </span>
  </p>
  <h2 id="h.6tunsp8x3u1l">
   <span>
    Data
   </span>
  </h2>
  <p>
   <span>
    Request: {utterance: {userID, time, content}, NLP: {intent, entities:[type,value]}, context, info: {stations:[], places:[], meteo:[]}, recommendations: [type,value,confidence], decision: {}}
   </span>
  </p>
  <p>
   <span>
    Is filled along the pipeline. Each stage is adding an element to it
   </span>
  </p>
  <p>
   <span>
    NLP
   </span>
  </p>
  <p>
   <span>
    From utterance:
   </span>
   <span>
    utteranceID
   </span>
   <span>
    , userID, time, content
   </span>
  </p>
  <p>
   <span>
    To: {intent,entities[type,value]}
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Entity resolver
   </span>
  </p>
  <p>
   <span>
    From user model:
   </span>
  </p>
  <ul>
   <li>
    <span>
     User city
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     User position
    </span>
   </li>
   <li>
    <span>
     User places: home, work/school
    </span>
   </li>
  </ul>
  <p>
   <span>
    The location entities become references to places in the DB (using places API):
   </span>
  </p>
  <ul>
   <li>
    <span>
     From name to place (the name in the utterance is used as keyword to do a search in the city proximity)
    </span>
   </li>
   <li>
    <span>
     From position to place (when user sends position, find a relevant place in the proximity)
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Core
   </span>
  </p>
  <p>
   <span>
    Internal state:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Previous questions / topic (field of dialogue): to understand messages without explicit intent and to resume interaction (stack of interactions: “i want a bike” PUSH “where are you?” -&gt; “i am at XXX” -&gt; “ok got it” POP “you can find 3 bikes at YYY”)
    </span>
   </li>
   <li>
    <span>
     Special topic of bootstrap: activated at the beginning
    </span>
   </li>
  </ul>
  <p>
   <span>
    Core steps:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Intent
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     If the intent is not specified (or generic intent “approve”/”disapprove”), look at the entity provided and if a previous state is saved. Based on these, derive the intent
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Preparation of parameters for functions (take from entities) and check requirements
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     If requirements are ok, proceed with steps
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     If previous state, pop it
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     If some requirements are missing, save current state (push) and ask for requirement
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Bike sharing:
   </span>
  </p>
  <p>
   <span>
    Station:
   </span>
   <span>
    stationID
   </span>
   <span>
    , name, description
   </span>
   <span>
    , (lat,long), free_bikes, free_slots
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Personalization:
   </span>
  </p>
  <p>
   <span>
    User:
   </span>
   <span>
    userID
   </span>
   <span>
    , Name, Surname, sex, age
   </span>
  </p>
  <p>
   <span>
    Episodic KB:
   </span>
   <span>
    userID, location, time
   </span>
   <span>
    , action (was at/took bike/left bike)
   </span>
  </p>
  <p>
   <span>
    Explicit rating: collecting user feedback after recommendation / bootstrap / unsolicited rating → needs to be investigated
   </span>
  </p>
  <p>
   <span>
    User features (built by the model): favorite/recurrent places, sportiveness, art interest
   </span>
  </p>
  <ul>
   <li>
    <span>
     RecurrentAction:
    </span>
    <span>
     ID
    </span>
    <span>
     , userID, type, frequency (days in week pattern), time of day, userPlaceID (external key to UserPlace)
    </span>
   </li>
   <li>
    <span>
     UserPlaces:
    </span>
    <span>
     userID, placeID
    </span>
    <span>
     , role (home/work/school/other)
    </span>
   </li>
  </ul>
  <p>
   <span>
    The stimulus link from the result provider to the recommender contains: trip information / direct question to the recommending system
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    User_feature:
   </span>
   <span>
    userID, featureID
   </span>
   <span>
    , value ← output of user modeler, input to recommender
   </span>
  </p>
  <p>
   <span>
    Place:
   </span>
   <span>
    placeID
   </span>
   <span>
    , name, description, (lat,long), category, subcategory
   </span>
  </p>
  <p>
   <span>
    Place_feature:
   </span>
   <span>
    placeID, featureID
   </span>
   <span>
    , value
   </span>
  </p>
  <h2 id="h.kqqgd8as7rlg">
   <span>
    Scenarios
   </span>
  </h2>
  <p>
   <span>
    Search for informations:
   </span>
  </p>
  <p>
   <span>
    The utterance is processed by the NLU module. The query understanding engine activates the contextual knowledge engine to get data that is related to the question, and in parallel records the event into the episodic knowledge. The contextual knowledge is composed of informations belonging to bike sharing, meteo, placesAPI. The results from the different sources are combined by the appropriate module. At this point the recommender is interrogated, in order to provide some personalized suggestions, if they have some metrics (telling that the suggestion is appropriate and relevant). The structured information of the result that is going to be provided needs some elaboration to provide a natural-language feeling: this is the role of the sentence generation engine that, based on templates, puts the response in human-like format.
   </span>
  </p>
  <p>
   <span>
    Expressing availability to talk (familiarization):
   </span>
  </p>
  <p>
   <span>
    The user may be sending some messages that don’t have some needs inside. Also the first interaction, in which the user sends some “/start” or “hi“ messages can activate this use case. The NLU processes them as usual, and the query understanding engine detects the situation. Instead of using contextual knowledge, it interacts with the personalization module to find some questions for the user: what he expects from the bot, or asking some features (interests/…).
   </span>
  </p>
  <p>
   <span>
    Sending messages without being asked:
   </span>
  </p>
  <p>
   <span>
    Warning: this breaks the principle that bots should only interact when explicitly asked. For this reason this feature should be kept under full control of the user (opt-in proposition vs one-time test + asking if this is liked), that in any moment can decide to turn it off. The reminder/suggestion, based on explicit request to do that or because a pattern has been detected, is generated and sent. The feedback must always be measured. The activation may be defined by the user (e.g. “tomorrow at 5 p.m. give me informations about bikes to go home”) or when the expected time, derived from the detected pattern, is coming (e.g: user always asks for a bike between 8:10 and 8:30 to go to university, now it is 8:00, could be the right time to send him the suggestion).
   </span>
  </p>
  <p>
   <span>
    Another useful activation is user asks for station X→ give results now on X and activate watcher on station X → estimate user-arrival-time to X → if station X is becoming useless (no/few bikes/slots depending on user need) send message to warn user
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Location can be inside the utterance as entity or can be the user position. In this case it must be up-to-date (define a timeout of validity and a way to ask user: “are you still there?”). If the dependency is not fulfilled, a way of skip
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <a id="t.2c97474fc09cd2758e0b91a19c5cfdca72f43631">
  </a>
  <a id="t.0">
  </a>
  <table>
   <tbody>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        intent
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        dependencies
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        steps
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        decision
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Search bike/free slot
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Location / updated position
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Find station informations, check meteo. Collect event. Activate watcher on station. Interrogate recommender
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Station informations + meteo warning* + recommendation*
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Plan trip
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Location A+B
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Same as above, but twice because both station must be watched / both events need to be collected
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        As above
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        If i will go home in 30 minutes, will I get wet?
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        OMG
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
       </span>
      </p>
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <hr style="page-break-before:always;display:none;"/>
  <p>
   <span>
   </span>
  </p>
  <h1 id="h.sa57zp440c1i">
   <span>
    Implementation
   </span>
  </h1>
  <h2 id="h.8zj60ofw3oog">
   <span>
    Interaction with chat platforms
   </span>
  </h2>
  <p>
   <span>
    Decoupling botkit from core
   </span>
  </p>
  <p>
   <span>
    Which ones
   </span>
  </p>
  <p>
   <span>
    How
   </span>
  </p>
  <h2 id="h.4sq87epw7a2r">
   <span>
    Data sources
   </span>
  </h2>
  <p>
   <span>
    Getting the bike sharing informations
   </span>
  </p>
  <p>
   <span>
    Getting places informations
   </span>
  </p>
  <p>
   <span>
    Other data sources
   </span>
  </p>
  <p>
   <span>
    Meteo
   </span>
  </p>
  <h2 id="h.b7qyoqg579vm">
   <span>
    Wit.ai prototype
   </span>
  </h2>
  <p>
   <span>
    A concept of the bot has been developed using wit.ai for doing intent detection and entity extraction. This can be used as a reference. The training data can be downloaded from the platform, useful for our model training.
   </span>
  </p>
  <p>
   <span>
    Objectives:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Experimenting
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Collect sentences and interaction sessions
    </span>
   </li>
   <li>
    <span>
     Quick prototype to understand the user needs
    </span>
   </li>
  </ul>
  <h2 id="h.rdt81o3mb6tf">
   <span>
    Neural Networks
   </span>
  </h2>
  <h3 id="h.5gr30bgy3moi">
   <span>
    Framework
   </span>
  </h3>
  <p>
   <span>
    Keras vs Tensorflow
   </span>
  </p>
  <p>
   <span>
    In this section there is a comparison between the two most popular neural network libraries.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <a id="t.522a20e01ac7b3f467e4103ac499479fb019deb7">
  </a>
  <a id="t.1">
  </a>
  <table>
   <tbody>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Keras
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Tensorflow
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        High level
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Low level access, but also high level API
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        easy
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        More difficult
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Less cells/layers available
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Lot of already defined classes, contrib package
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Less customizable
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Highly customizable and flexible
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Rapid prototyping
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Advanced operations
       </span>
      </p>
     </td>
    </tr>
    <tr>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Documentation good for what’s implemented
       </span>
      </p>
     </td>
     <td colspan="1" rowspan="1">
      <p>
       <span>
        Documentation gap between trivial and advanced
       </span>
      </p>
     </td>
    </tr>
   </tbody>
  </table>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Keras is more like LEGO, build from blocks. On tensorflow can customize operations.
   </span>
  </p>
  <h4 id="h.30pe1175q3g4">
   <span>
    Keras Recurrent API
   </span>
  </h4>
  <p>
   <span>
    Keras Recurrent API is described extensively there (
   </span>
   <span>
    <a href="https://keras.io/layers/recurrent/">
     https://keras.io/layers/recurrent/
    </a>
   </span>
   <span>
    ). Here a quick overview is performed.
   </span>
  </p>
  <p>
   <span>
    To define a recurrent
   </span>
   <span>
    layer
   </span>
   <span>
    in Keras the class keras.layer.RNN takes as argument a RNN cell. Some parameters can be used to define how the layer will behave.
   </span>
  </p>
  <p>
   <span>
    A
   </span>
   <span>
    RNN cell
   </span>
   <span>
    is a class that has a call method that takes as input the actual input and the current state and produces the output and the next state. Different implemented
   </span>
   <span>
    cells
   </span>
   <span>
    exist.
   </span>
  </p>
  <ul>
   <li>
    <span>
     SimpleRNNCell: a fully connected RNN
    </span>
   </li>
   <li>
    <span>
     GRUCell: Gated Recurrent Unit - Cho et al. 2014
    </span>
   </li>
   <li>
    <span>
     LSTMCell: Long-Short Term Memory layer - Hochreiter 1997
    </span>
   </li>
  </ul>
  <p>
   <span>
    Layers can be easily stacked and the creation of deep recurrent network is very easy. Networks that are a simple stack of different layers (Recurrent, Dropout, Dense) won’t find any problem in implementation. But when the networks are not so linear in the flow or when some advanced layers are required, there might not be an already implemented solution. For example there is no native implementation for seq2seq models and layers, no Attention mechanism and is not possible to have a ready to use help for doing output dependency modeling (feeding back the outputs in a decoding layer). They can be implemented with the existing API, but they are not part of the library itself (some implementations can be found online as
   </span>
   <span>
    <a href="https://github.com/farizrahman4u/recurrentshop">
     https://github.com/farizrahman4u/recurrentshop
    </a>
   </span>
   <span>
    , but the number of people working on it is order of magnitude smaller than the tensorflow community
   </span>
   <span>
    ).
   </span>
  </p>
  <h4 id="h.3rp3zhi54s79">
   <span>
    Tensorflow Recurrent API
   </span>
  </h4>
  <p>
   <span>
    Also tensorflow has classes for defining RNN layers and RNN cells. But there is more:
   </span>
  </p>
  <ul>
   <li>
    <span>
     More cells: instead of a single implementation of LSTM and GRU, a wide variety of cells can be chosen
    </span>
   </li>
   <li>
    <span>
     More wrappers: wrappers can be used to encapsulate cells. DropoutWrapper, AttentionWrapper and a lot more can be added to the cells
    </span>
   </li>
   <li>
    <span>
     Seq2Seq native support: classes for defining encoders and decoders, with some helpers for modeling output dependencies can make a lot easier to build complex models that are not a linear chain of layers but contain links that move backwards (as in output dependencies, where the output label is fed back to the decoder RNN). For example to build a decoder that considers output dependencies, in tensorflow 3 components are needed: a RNN cell (e.g. LSTMCell), a CustomHelper (that is responsible to provide values to the cell and take the outputs. All this by simply declaring three custom functions) and a BasicDecoder that combines the cell with the helper and builds a layer. By dividing the roles of the cell and of the helper, a complex decoder can be built without a big effort.
    </span>
   </li>
  </ul>
  <p>
   <span>
    More details about the available classes can be found in the official documentation
   </span>
   <span>
    <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq">
     https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq
    </a>
   </span>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h4 id="h.8mdxf2pa39ns">
   <span>
    Performances
   </span>
  </h4>
  <p>
   <span>
    Since Keras can use as backed both Tensorflow and Theano, but Theano has many disadvantages (single GPU, non intuitive API, ceased development, ...), Keras can be seen as the official higher level API for tensorflow
   </span>
   <sup>
    <a href="#ftnt14" id="ftnt_ref14">
     [14]
    </a>
   </sup>
   <span>
    . For this reason, using tensorflow directly or inside Keras should make no big differences in terms of performance. There is also the possibility to use Keras API inside tensorflow, via the tensorflow.contrib.keras package.
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h4 id="h.a08x42jk1qdf">
   <span>
    Decision
   </span>
  </h4>
  <p>
   <span>
    The objectives of the decision are:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Ability to build the proposed network without too much headache. Especially for the decoding stage, some advanced wiring is needed and would like not to start from scratch (error-prone) but have some support from the library
    </span>
   </li>
   <li>
    <span>
     Code readability/maintainability: possibly using quite high level operations, no reinventing the wheel (in a buggy version)
    </span>
   </li>
   <li>
    <span>
     Quite good performances
    </span>
   </li>
  </ul>
  <p>
   <span>
    Considering those objectives, the choice has been to use the native tensorflow APIs for building the solution, using the seq2seq package for faster implementation
   </span>
  </p>
  <h3 id="h.z0ec9b4l4c3k">
   <span>
    Implementation details
   </span>
  </h3>
  <ul>
   <li>
    <span>
     Word tensors
    </span>
   </li>
   <li>
    <span>
     Seq2seq package
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <h2 id="h.txecykd802tw">
   <span>
    Personalization
   </span>
  </h2>
  <p>
   <span>
    Magicsauce
   </span>
  </p>
  <p>
   <span>
    Bootstrap
   </span>
  </p>
  <p>
   <span>
    Facebook login (why, how)
   </span>
  </p>
  <p>
   <span>
    content
   </span>
  </p>
  <p>
   <span>
    Foursquare (why, how)
   </span>
  </p>
  <p>
   <span>
    From the user to the places
   </span>
  </p>
  <p>
   <span>
    Interaction
   </span>
  </p>
  <p>
   <span>
    Operative modes
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
   </span>
  </p>
  <hr style="page-break-before:always;display:none;"/>
  <p>
   <span>
   </span>
  </p>
  <h1 id="h.9ldkmut5kq6f">
   <span>
    Validation
   </span>
  </h1>
  <p>
   <span>
    “How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation”
   </span>
  </p>
  <p>
   <span>
    This paper analyzes metrics for
   </span>
   <span>
    unsupervised
   </span>
   <span>
    dialogue systems -&gt; not task focused. Correlation between automatic metrics and human ratings
   </span>
  </p>
  <h2 id="h.9u2lsbvmk08z">
   <span>
    System as a whole
   </span>
  </h2>
  <p>
   <span>
    Ground-truth based (comparing with a predetermined output. Using some already available datasets, but domain problems)
   </span>
  </p>
  <ul>
   <li>
    <span>
     Continuous sequences of correct actions from the beginning of the dialog
    </span>
   </li>
   <li>
    <span>
     Dialogue success rate
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Objective measures:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Duration of conversation
    </span>
   </li>
   <li>
    <span>
     Length of sentences
    </span>
   </li>
   <li>
    <span>
     Uptime of the bot
    </span>
   </li>
   <li>
    <span>
     Errors in time unit
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Detecting from chat:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Sentiment detection: not always applicable (“cold sentences”)
    </span>
   </li>
   <li>
    <span>
     User feedback: ok/thanks
    </span>
   </li>
   <li>
    <span>
     Thumbs up/down at the end of conversation → must be shown to user
    </span>
   </li>
   <li>
    <span>
     User repeats question → means the system didn’t catch it, but dangerous, maybe user wanted again updated data (can distinguish on time passed: short time could show that system didn’t catch)
    </span>
   </li>
   <li>
    <span>
     Not recognised intent→ could be
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Out of domain
    </span>
   </li>
   <li>
    <span>
     Into the domain but unforeseen
    </span>
   </li>
   <li>
    <span>
     Number of error messages
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     System turn duration (to generate a response)
    </span>
   </li>
   <li>
    <span>
     Task completion time (could compare against existing app)
    </span>
   </li>
  </ul>
  <p>
   <span>
   </span>
  </p>
  <p>
   <span>
    Survey-based:
   </span>
  </p>
  <ul>
   <li>
    <span>
     Overall evaluation:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Usefulness
    </span>
   </li>
   <li>
    <span>
     Usability
    </span>
   </li>
   <li>
    <span>
     Relevance of results
    </span>
   </li>
   <li>
    <span>
     Missing features
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Per-interaction feedback (C. Chakrabarti, G.F. Luger / Expert Systems with Applications 42 (2015) 6878–6897):
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Grice’s maxims:
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Quality: informations are good
    </span>
   </li>
   <li>
    <span>
     Quantity: appropriate quantity of informations
    </span>
   </li>
   <li>
    <span>
     Relation: relevant to context and to the topic of conversation
    </span>
   </li>
   <li>
    <span>
     Manner: direct and straightforward
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Solving problems
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     The bot asked the required information
    </span>
   </li>
   <li>
    <span>
     The bot kept conversation on-topic (coherence)
    </span>
   </li>
   <li>
    <span>
     The bot solved issue
    </span>
   </li>
  </ul>
  <ul>
   <li>
    <span>
     Evaluation of personalization: when giving a personalized response, check if user prefers the basic one or the customized one
    </span>
    <hr style="page-break-before:always;display:none;"/>
   </li>
  </ul>
  <h1 id="h.b8i4k637cy5o">
   <span>
    Conclusion
   </span>
  </h1>
  <div>
   <p>
    <span>
    </span>
   </p>
  </div>
  <hr/>
  <div>
   <p>
    <a href="#ftnt_ref1" id="ftnt1">
     [1]
    </a>
    <span>
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref2" id="ftnt2">
     [2]
    </a>
    <span>
     D Guo, G Tur, W Yih, G Zweig, “Joint semantic utterance classification and slot filling with recursive neural networks” in Spoken Language Technology Workshop (SLT), 2014
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref3" id="ftnt3">
     [3]
    </a>
    <span>
     B. Liu and I. Lane, “Attention-based recurrent neural network models for joint intent detection and slot filling,” in Proceedings of The 17th Annual Meeting of the International Speech Communication Association, 2016
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref4" id="ftnt4">
     [4]
    </a>
    <span>
     Bengio, et al. (1994), IEEE Transactions on Neural Networks, “Learning long-term dependencies with gradient descent is difficult”
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref5" id="ftnt5">
     [5]
    </a>
    <span>
     S Hochreiter - International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 1998 - World Scientific, “The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions”
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref6" id="ftnt6">
     [6]
    </a>
    <span>
     S Hochreiter, J Schmidhuber, Neural computation, 1997, “LONG SHORT-TERM MEMORY”
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref7" id="ftnt7">
     [7]
    </a>
    <span>
     Cho, et al. arXiv preprint arXiv (2014), “Learning phrase representations using RNN encoder-decoder for statistical machine translation”
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref8" id="ftnt8">
     [8]
    </a>
    <span>
     R Jozefowicz, W Zaremba, I Sutskever - Proceedings of the 32nd …, 2015, “An Empirical Exploration of Recurrent Network Architectures”
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref9" id="ftnt9">
     [9]
    </a>
    <span>
     J Chung, C Gulcehre, KH Cho, Y Bengio - arXiv preprint arXiv:1412.3555, 2014, “Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling“
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref10" id="ftnt10">
     [10]
    </a>
    <span>
     K. Cho, 2014, Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref11" id="ftnt11">
     [11]
    </a>
    <span>
     “Neural Machine Translation By Jointly Learning To Align And Translate” Cho et Al, 2015
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref12" id="ftnt12">
     [12]
    </a>
    <span>
     Chen, et al., “End-to-End Memory Networks with Knowledge Carryover for Multi-Turn Spoken Language Understanding,” in Interspeech, 2016.
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref13" id="ftnt13">
     [13]
    </a>
    <span>
     Chen, et al., “Dynamic Time-Aware Attention to Speaker Roles and Contexts for Spoken Language Understanding”, 2017
    </span>
   </p>
  </div>
  <div>
   <p>
    <a href="#ftnt_ref14" id="ftnt14">
     [14]
    </a>
    <span>
    </span>
    <span>
     <a href="http://www.fast.ai/2017/01/03/keras/">
      http://www.fast.ai/2017/01/03/keras/
     </a>
    </span>
    <span>
    </span>
   </p>
  </div>
 </body>
</html>