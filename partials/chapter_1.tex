% !TEX encoding = utf8
% !TEX root = ../main.tex

\chapter{Introduction}
This chapter provides a general introduction to the work done in this thesis project. Starting with a brief introduction of the setting where this work has been conducted, an introduction to the topic of Conversational Agents (also known as Chatbots or Bots) is given. An overview of the field of Artificial Intelligence is done, by considering both its current achievements and some general purpose guidelines that should lead the research and development of such systems. The chapter ends with an outline of the next chapters.

\section{Istituto Superiore Mario Boella}
This work has been conducted as a joint collaboration between Politecnico di Torino\footnote{\url{https://www.polito.it/}} and Istituto Superiore Mario Boella.\footnote{\url{http://www.ismb.it/}} This institute, founded in 2000 as a research and innovation center by Compagnia di San Paolo and Politecnico di Torino, is organized in different research areas that focus on some core sectors of ICT such as mobile solutions, cloud computing wireless systems and sensors, and navigation technologies. The research area where the current work has been done is the Innovation Development area that investigates the future of data-driven innovations doing research on three different axes: data science, data experience and economics. This work, being on the topics of Natural Language and Personalization, has been supported by the Data Science Group through its studies and development.

\section{Autonomous Systems in the society}
This work about Conversational Agents belongs to the wider topic of Autonomous Systems. If we think about independent systems that live together with humans, a lot of science fiction stories may come to our minds. From this literature we can extract some major themes that may reflect how the society can see the development of autonomous systems.

One of them is surely the problem of dominance and control. There is a fight (real or implicit) between humans and machines. The dominance can be hold by both sides, and what emerges is the difference between the goals of the machines and the goals of the humans.

Another very important topic that emerges is the self-awareness and what distinguishes humans from machines. This is where \textit{sentient} AI seek for understanding of the world together with a purpose of existence, and always arises ethical questions, such as effects on our behaviours and interactions, how to keep control over the singularity, what are the rights of both sides.

All those stories, being part of science fiction, are mostly far from reality. But since technology is advancing faster and faster, an analysis should be done on the consequences it can have on the society, and how the existence of some principles could turn those advances into empowering tools for humans and not something to be afraid of.

These concepts have to be analyzed and some principles must be known and followed when designing autonomous systems. Keeping in mind that the systems analyzed in this work are far from generic artificial intelligence, because they are designed to solve very narrow problems, those principles have to be analyzed at the beginning.

For this reason, after giving an introduction to Artificial Intelligence that empowers those autonomous agents, looking at its roots and evolution (Subsection~\ref{aiEvolution}), the discussion will go on some guidelines (Subsection~\ref{aiGuidelines}) that have been derived from common sense and scientific literature~\cite{walsh2016turing,hibbard2014ethical,moor2009four,clarke2011asimov}.

\subsection{Artificial Intelligence evolution}
\label{aiEvolution}

Here it is provided a bit of background on Artificial Intelligence, starting from the historical roots that lead to development of systems that imitate a human being. Successively, an overview is given about what can be done by AI today in different fields. Then a brief exploration of the Machine Learning techniques opens the discourse of general Artificial Intelligence.

\subsubsection{Artificial Intelligence as Human imitation}
The development of artificial intelligence has its roots back in the 50s. The idea is to build an artificial brain, inspired by the human brain. Empowered by the studies carried in those years about neurons and synapses, imitating those elements and their connections became an active topic of research. The first machines trying to achieve this task did not use computers, but were completely controlled by analog circuitry. The first neural network machine was built by Marvin Minsky in 1951~\cite{minsky1952neural}, based on the idea of the artificial neuron~\cite{mcculloch1943logical,rosenblatt1958perceptron}.

With the goal of exploring how the human brain works, the neuroscience community has done a lot of progresses with the advance of years, and the studies are still open trying to find the biological basis of information processing (take a look at the ``human brain project''  in \ref{generalAI}).

While the first experiment were carried out in 50s, on the philosophical side Alan Turing published a paper~\cite{turing1950computing} in which he argues that a ``\textit{thinking machine}'' could be built. The criterion defined to distinguish the thinking process is based on human judgment: if the machine is not distinguishable from another human being in a conversation over a teleprinter, we can say that the machine is able to think. This setting is the so called \textit{Turing Test} . In this test, two players (a human and a machine) are tested against a human interrogator who tries to understand their nature. A machine that passes this test is said to be intelligent.

As we can see, all is based on imitation of humans: at a more detailed level, the artificial neural networks try to mimic the behaviour of the human brain. If the intelligence of a human is located in its brain, a good reproduction of it can potentially have the same intelligence of the original one. And the root of the abilities of the brain stands in its structure. At more higher level of abstraction, a system can be seen as intelligent if it can emulate a human characteristic (such as holding a conversation over a teleprinter) well enough to convince the interrogator.

This can be seen as a first definition of artificial intelligence: \textbf{the simulation of human intelligence by machines}. This imitation also is reflected in the shapes of robots, because a more human-like appearance is a faster way to immediately feel more human. Having a face that can emulate some expressions is an active field of work in the robotics. As we know, the interaction is not bound to what is said in communication, but has its strength in multimodality, combining visual and audio channels.

But is really the human intelligence the best an autonomous system can achieve?\\
In today's world, with the enormous quantity of data available, and with the increasing processing power of computers, an artificial system can be seen as smart because combines and extracts the information available very fast and in a way that is useful to the humans using it.

Having a good distinction between what is human intelligence and what artificial intelligence could be can help building a roadmap about how the society wants and will like to be empowered by technology. The concept used to express this collaboration between human and machine is \textbf{heteromation}~\cite{ekbia2014heteromation}: the labour is divided between humans and machines by using the automation for all the repetitive tasks while the humans are used for critical and decisional tasks.

\subsubsection{What AI can do today}
As of today, Artificial Intelligence has reached some good results in very narrow tasks. The key point that made this possible is how the intelligent behaviour is achieved: from historical systems that aim to act by following huge sets of rules (deductive reasoning) to Machine Learning techniques that allow to learn things simply from examples (inductive reasoning). Over the Machine Learning techniques, a major role is being played by Neural Networks that try to imitate the physical and chemical structure of human brain. The fields where Machine Learning has achieved recent outstanding results are many. Here we try to provide some examples of them.

With respect to image processing, Machine Learning can be able to detect, segment and recognize objects and regions. For these tasks Deep Convolutional Networks are used~\cite{lecun2015deep}. Their objective is to abstract from pixel-level features to more complex features such as lines, borders and shapes. In this way, they can successfully categorize images based on the contents: identify objects, recognize written text (OCR), detect faces, recognize people, detect facial expressions~\cite{tian2005facial}, find new planets.\footnote{\url{https://www.wired.com/story/new-kepler-exoplanet-90i-discovery-fueled-by-ai/}}

Instead on the field of language a lot of different tasks are addressed: automated translation,\footnote{\url{http://bit.ly/google-translate-introduces-nmt}} sentiment analysis, entity recognition, Natural Language Understanding. For this set of tasks, more than Convolutional Networks, the best results are provided by Recurrent Neural Networks that are able to process order-sensitive sequences. These approaches usually make use of distributed representation of words~\cite{bengio2003neural}, known as Word Embeddings, that following the Distributional Hypothesis: ``linguistic items with similar distributions have similar meanings''~\cite{sahlgren2008distributional} (see \ref{soaWordEmbeddings}). Those approaches, labeled as Natural Language Processing, can be used for different goals: analysis of text from online sources or social media, provide services through Conversational Interfaces, entertain the user with chit-chat dialogues.

Machine Learning also applies well to the field of games. In 2016 AlphaGo~\cite{chouard2016go} managed to beat the world champion of the Go game. The interesting part of this story is that the AI trained for the match by not only observing a lot of previous matches but also by playing against itself. Other studies focus on other games such as Texas hold'em~\cite{brown2017superhuman}, or on video games~\cite{mnih2015human}. Using Reinforcement Learning techniques, those systems are able to learn the consequences of performing actions and adaptively learn with time to optimize their goals.

Those different fields of application have also been combined together: for example generating textual descriptions of images~\cite{xu2015show}, or providing multimodal interaction (voice, text, visual) with smart assistants (Alexa, Siri, Cortana).

\subsubsection{The evolution of machine learning}
All those results have been achieved thanks to the evolution of the Machine Learning techniques.
A first division of them can be done by considering if the desired output is provided or not in the training set: the distinction is between supervised techniques (that have input-output pairs) and unsupervised ones (that have only inputs, and apply in tasks like clustering).

The principle at the basis of supervised techniques is to show examples to a system that learns how to obtain the desired output. Different approaches exist: decision trees, association rules, neural networks. But the trend nowadays is towards Artificial Neural Networks because of their power in being able to model non-linear relationships. Neural Networks have tunable parameters that are learned by using the Backpropagation algorithm~\cite{rumelhart1986learning}. From the errors on the predictions with respect to the truth values, the sources of errors are found by evaluating the backpropagation of the gradients, and the tunable parameters are modified in order to reduce this error by using some Gradient Descent techniques~\cite{bottou2010large}.

Given this mechanism to automatically learn from examples, there has been an evolution considering the structure itself of the network (adding more and more layers, of different kinds), on the training algorithms and on the kind of input that are fed.

Neural Networks come in the field of Artificial Intelligence as systems that can model some non-linear functions and substitute handcrafted rules in classification tasks. The first generation of this approach is based on a strong definition of input features, which are defined manually and inputs are annotated with a lot of them. This manual specification of input features requires a lot of effort in their design and in data annotation, and for this reason is not easily applicable to new problems. For some problems the features themselves can become quite complex and difficult to generate.

Given those reasons, and also considering the advances of hardware that allow more computation power at reduced costs, the trend of Machine Learning has gone towards Deep Learning that uses several layers of computation in the networks. This increase in number of layers and in complexity of the computational graph allows to reduce the work done on input features, allowing rawer data to be fed into the networks. In Deep Learning techniques, features at higher level layers are extracted by the lower levels of the network. This reduces the work of feature engineering but on the other hand requires more training samples in order to understand how to extract the relevant features.

This shift from simple networks with complex and elaborated features towards complex networks with simpler inputs can be observed also in the fields of Natural Language Understanding, which will be covered in Section~\ref{soaNLU}.

Having deep learning techniques, however, does not change the applied approach: it is always based on the observation of inputs and output pairs and learning a statistical model that will be able to predict something with sufficiently similar inputs. Random examples are presented to the computational graph that learns how to imitate them.

Another approach that is relevant to mention is the Reinforcement Learning that is more active also in the training epochs. Being able to act and observing the consequences on some rewarding functions, it is able to learn what is good and what is not towards a certain goal. This is the field where computers recently have defeated human opponents in popular games, such as AlphaGo~\cite{chouard2016go}. In this example the reinforcement was applied also to matches between two computers. The strength of this dynamic training is to be able to act and observe the consequences of actions. In this way, the machine is able to explore different choices and learn which one is better with respect to the rewards that have been established. For this mixed objective of learning new things but at the same time using the acquired knowledge, it is known as the \textit{exploration vs exploitation dilemma} and many solutions exist~\cite{tokic2010adaptive}.

A special kind of Reinforcement technique is the Adversarial ML, that by employing two components (the classifier and the adversary) with different goals builds a more robust classifier. This special training technique tries to overcome vulnerabilities of current Machine Learning systems, that may be exploited to generate wrong predictions if the inputs are preprocessed in some specific ways~\cite{athalye2018obfuscated}. This phenomenon of ``AI hallucinations''  is difficult to solve even with Obfuscated Gradients, and can make predict to the classifier any wrong output with only little but targeted changes on the inputs.\footnote{\url{https://www.wired.com/story/ai-has-a-hallucination-problem-thats-proving-tough-to-fix/}} The goal of Adversarial ML is to reduce the possible effects of adversarial examples by employing the adversary component abilities to reinforce the classifier via the trial-and-error procedure.

Even with Reinforcement Learning, the involved systems are unable to understand the ontological level of things: a system of this type is only able to work on a specific problem thanks to the optimization function that it learned to reduce. However what is unique to human intelligence is the ability to perform retrospective reasoning, in other words to truly understand the association between causes and consequences and being able to provide answer to associational questions.

As~\cite{pearl2018theoretical} analyzed, there are three levels of reasoning (seeing, doing, reasoning) that have strong dependencies between them and currently the Machine Learning approaches only reach the second level. An overview of building machines that are able to reach the third level (reasoning like humans) is provided in the next paragraph.

\subsubsection{Towards general AI}
\label{generalAI}

The term Artificial General Intelligence, with its meaning of being able to perform tasks like humans, ``\textit{involves the ability to reason, plan, solve problems, think abstractly, comprehend complex ideas, learn quickly and learn from experience. It is not merely book learning, a narrow academic skill, or test-taking smarts. Rather, it reflects a broader and deeper capability for comprehending our surroundings-'catching on', 'making sense' of things, or 'figuring out' what to do}'' .~\cite{gottfredson1997mainstream}

Looking at the current situation of Artificial Intelligence, the gap with respect to General AI can be felt: the examples shown previously belong to a narrow field, with strong definitions of inputs and outputs, while this definition requires a very dynamic behaviour, learning what to do in contexts that were not analyzed.

The main capabilities that are missing, according to~\cite{lake2017building}, are:

\begin{itemize}
	\item \textit{Building causal models} that can support real understanding of causes and consequences;
	\item \textit{Understanding principles} of physics and psychology, in order to enrich the knowledge that is acquired through ``senses''  with general principles that rule the world;
	\item \textit{Learning to learn} in new environments, generalizing knowledge.
\end{itemize}

The first point is the same that has been analyzed by~\cite{pearl2018theoretical} and it underlines the fact that mostly AI is applied to solving pattern recognition problems. Instead the human brain can build structured casual models thanks to the properties of the neocortex~\cite{hawkins2017theory}. This difference makes possible to recognize things also if they appear from different perspectives: for example in object recognition, rotated images can be misclassified simply because at training time the system was not trained from that point of view. Instead, having a richer model that simulates the 3D shape of objects helps to predict correctly also in those situations.

The second point extends this characteristics: a prior knowledge of some general rules, in the example of rotated objects those rules are geometrical, can help understand better and faster the observation done on the domain. For example, learning to track an object knowing that it is solid and coherent, can help focusing on more important features, such as the trajectory and stability~\cite{lerer2016learning}. Or for the psychological principles, observing a video gamer knowing that is trying to seek rewards while avoiding punishment can help focusing on his tactics and more advanced features.

For the third point, between human learning and machine learning we know that the first one is more efficient and faster to learn from few examples. The ability of learning to learn comes from the causal models that give form to rich structured knowledge. This knowledge is a set of concepts that belong to a higher level of abstraction.

From concepts, the human brain is able not only to do the pattern recognition task, but also to generate new examples and explain what are the major discriminator of the output classes. And those concepts are transferred along different problems thanks to the human memory.

Machine Learning instead suffers that for any different problem it is trained from scratch. Because the learning network is designed for the current narrow process, the learning is difficult to transfer to new applications. Humans, when trained on a completely new task, can exploit their previous experience on other problems and the rich structured knowledge they have been acquiring since their birth. This is also caused by the structure of the connections, that between artificial neurons are fixed. The brain instead, forming new synapses can quickly learn new things without affecting previous learnings.~\cite{hawkins2016neurons}

Following~\cite{pearl2018theoretical}, the way to achieve better AI is to include physical and psychological fundamentals and generate models that capture the causality of the world. With those models as basis, new concepts could be learned fast (thanks to Transfer Learning) and the training process could be really more efficient and require less examples.

A very important feature of future machine learning, in the opinion of the authors, is to use \textbf{compositionality} (to put together the learned models from different problems) to build a single causal model that is able to learn new things and generalize faster.

About machine reasoning, consider the \textit{MACNets} approach~\cite{hudson2018compositional} that with the proposition of a new recurrent cell, Memory Attention Composition (MAC), tries to model both control and attention propagation. The problem where this approach is applied is Question-Answering on synthesized images~\cite{johnson2017clevr}, and the focus is on language compositionality and visual reasoning based on simple objects. The approach is able to answer questions such as ``How many objects are either small cylinders or metal things?''.

The scientific communities are quite divided in opinion about if and when AGI will be reached but from some surveys, completed by participants of conferences (\textit{Philosophy and Theory of AI 2011}, \textit{AGI 12}) and members of associations like EETN and ``The 100 Top authors in artificial intelligence by citation in all years'' , the opinion seems to be that it will be reached in this century~\cite{muller2016future}.

The opinions can be so positive about the reachability of this objective thanks to all the research that has been done in order to understand how the human brain works. Different projects exist today, both European\footnote{\url{https://www.humanbrainproject.eu/en/}} and American\footnote{\url{https://www.nsf.gov/awardsearch/showAward?AWD\_ID=0134732}} that aim to explore this topic in neuroscience, computing and medicine. Models are built that can simulate brains and researchers can use them to do experiments in different fields: robotics, medicine, cognition.

However, nowadays the term AI is being used commercially as a fuzzy word, and it is unfortunately generating a lot of examples that use the expression simply because they employ some statistical models but are far from the processes that would be necessary to achieve AGI. The goal is simply to exploit the trending topic and generate some hype to promote products that are mostly based on rules or highly handcrafted features. It is the example of some Humanoid robots, that are remarkable good examples of progresses in human appearance and movements, but from the intelligent point of view are only well featured programs, far from the general intelligence.

As humanoid examples of robots, we can start with Sophia, developed by Hanson Robotics, which has been granted citizenship by Saudi Arabia recently.\footnote{\url{http://bit.ly/forbes_stone_sophia_robot_citizen}} This is an examples that is linked to many interesting topics. For some, this action ``\textit{set a bad precedent for how we might treat robots in future}'' \footnote{\url{http://bit.ly/theverge_vincent_robot_citizen}} and this will be covered in the next subsection \ref{aiGuidelines} about some guidelines. Moreover, it is not clear how much of this robot is real Artificial Intelligence and how much is just the emblematic representation of AI hype.

Another example is Nadine,\footnote{\url{http://bit.ly/telegraph_knapton_nadine}} built at the Nanyang Technological University, that is able to recognize people and resume conversations based on previous chats.

Those kind of robots are a combination of a wide number of AI methods: face tracking, emotion recognition, and robotic movements generated by deep neural networks.\footnote{\url{http://bit.ly/theverge_vincent_sophia_not_true_ai}} But anyway all those examples are far from general AI, because they work only in the specific field they have been designed to. The advances have been done on expanding the things that can be done, but purists of AI argue that in this way the human intelligence cannot be reached~\cite{pearl2018theoretical}. Human intelligence is more dynamic and general because it learns how to do new things in unbounded fields.

Also considering the actual reinforcement learning, which learns from itself how to do tasks, is misleading because the fields in which this is applied are always defined a-priori and the learning always occurs following a predefined rule. For instance, on the task of playing a specific game, a rewarding function is defined and the machine learns how to play better by challenging itself.

AGI can be reached only by effectively performing machine reasoning, enabling compositional and physical based thinking.

\subsection{AI Guidelines}
\label{aiGuidelines}

Also with the noticed distance from AGI, we can think about some general guidelines that should be followed when designing autonomous systems that are interacting with humans. Those guidelines can be seen as part of the AI ethics discussion~\cite{hibbard2014ethical,moor2009four}, and are really needed because we agree with the idea that ``\textit{Technology is neither good nor bad; nor is it neutral}''  as said in~\cite{kranzberg1986technology}. Because it is not neutral on itself, it is better to define some rules to avoid threats.

First of all, there are threats linked to the human-machine fight: physical threats. To avoid them, we can think of some simple principles, like the basic Three Laws of Robotics~\cite{clarke2011asimov}. Those rules, firstly expressed in a short story~\cite{asimov1942runaround} by Asimov, quoted from  the ``Handbook of Robotics, 56th Edition, 2058 A.D.'' , are:

\begin{enumerate}
	\item \textit{A robot may not injure a human being or, through inaction, allow a human being to come to harm;}
	\item \textit{A robot must obey the orders given it by human beings except where such orders would conflict with the First Law;}
	\item \textit{A robot must protect its own existence as long as such protection does not conflict with the First or Second Laws.}
\end{enumerate}

By observing those three rules, the main physical threats can be avoided. However, due to their historical birth, they may be a bit outdated and not consider all the possible consequences that may arise. This happens because the fields where autonomous systems can be applied vary dynamically with time and the potential consequences change with them.

More recently AI and robotics researchers, experts and other endorsers have produced a set of principles - the Asilomar AI Principles\footnote{\url{https://futureoflife.org/ai-principles/}} - that analyze issues that may arise in research (how to establish the goals, funding and cooperation of AI research towards beneficial intelligence), or in the fields of ethics and values (safety, human values, privacy, control), or also longer-term issues (like superintelligence for the benefit of humanity).

About the ethics of Artificial Intelligence, there is a lot of discussion on the topic. James H. Moor on the relation between robots and ethics defines ``\textit{Four Kinds of Ethical Robots}''~\cite{moor2009four}:

\begin{description}
	\item[Ethical impact agents:] those systems, intentionally or not, can have an ethical impact by the actions they perform;
	\item[Implicit ethical agents:] are designed to avoid unethical outcomes, the strategy is to prevent them by limitations of the system capabilities;
	\item[Explicit ethical agents:] have algorithms to act ethically, they can identify and process ethical information and act accordingly;
	\item[Full ethical agents:] are as ethical as humans, thanks to features such \textit{free wil}, \textit{consciousness} and \textit{intentionality}.
\end{description}

Previously it has been mentioned that keeping a good distinction between the human intelligence and the intelligence provided by artificial systems, and understanding the advantages of both, could help focusing on the desired scope of the technology. Following this vision, we can say that artificial intelligence can provide an improvement to the society of today by being a set of useful tools. This is the idea expressed with the expression ``utilitarian ethics for AI''~\cite{hibbard2014ethical}.

Letting the machines do the stuff they can do better such as automatic and repetitive tasks is needed in a word where fast production is a crucial element, as it has been done in factories by substituting human employees with robots in tasks like assembling of products. Fields where this substitution is happening now is in call centers, where the work is most repetitive and can be automated. While the automation in previous years has been focused on mechanical fields, now it is shifting to conversational fields. While machines do this works, the humans can focus on works they like more and on decisional processes.

All where there is a distinction there is a point of contact, an interface, where the two sides meet and interact. Specifically for conversational agents, here are presented some boundaries that should be present on different dimensions that are examined in the following subsections: \textit{distinguishability}, \textit{autonomy} and \textit{personality}.

\subsubsection{Distinguishability}
One of the reasons that lead to the development of Conversational Agents is to emulate the human. The progress of those system is evaluated with measures of how seamless the interaction is and how likely the system can be confused with a human (see the Turing Test~\cite{turing1950computing}, or the Loebner Prize\footnote{\url{http://www.aisb.org.uk/events/loebner-prize}}). However, going in the opposite direction, we can put as a first general rule the distinguishability of autonomous systems. Following the ``\textit{Turing's red flag law}''  analyzed by~\cite{walsh2016turing}, an autonomous system should be designed in a way to make clear that is not a human, and identify itself at the start of any interaction with other agents. In this article, the author is taking the expression ``red flag law''  from the Red Flag Act contained in the Locomotive Act~\cite{rickards1817statutes}, that stated that a self-propelled vehicle had to be led by a pedestrian waving a red flag or carrying a lantern to warn bystanders of the vehicle's approach. The term is modified by adding the artificial intelligence topic by referencing the author of the so largely known test.

Walsh stands this principle in two parts. In the first one, the design itself of the system should be done by keeping in mind that the product is unlikely to be mistaken for human. This applies to the case of self-driving vehicles, that should be recognizable so that other actors on the road can have a more precise knowledge of the surrounding environment. This is crucial because the behaviour of human drivers and autonomous ones can be very different: both sides can make errors but of different kinds, a human can be distracted or fall asleep while a bot can do mistakes in situations it has not been designed to work.

The second part is about stating the nature of the agent at the beginning of every conversation. This has to be done to be distinguished and put the interlocutor in the right setting and mood. Knowing the source of words is very important.

The article also reports some examples for this specific part of the law. First of all with virtual assistants, that nowadays are so popular. Walsh observes that this rule is not always respected: if you ask Siri\footnote{\url{https://www.apple.com/ios/siri/}} if she is human or not, the answer is not so clear. The playwright did that in order to keep a funny and unpredictable character of the assistant, but pretending to be human is a dangerous precedent. Now it is clear that they are AI, but with technological progress the difference could become unnoticed. Another example is with online games: bots can have some advantages and disadvantages, but user should know what kind of player they are playing with. Also when reading computer-generated text it should be explicit that the writer is not human: depending on the domain, this can impact the emotions of the reader.

This criterion of distinguishability is not itself limiting the expressive power that the autonomous systems can have. It is simply asking to make clear what species belongs the interlocutor.

\subsubsection{Autonomy}
Another dimension in which there should exist a fixed barrier is the decisional one. On this dimension there should be a limit on what can be decided autonomously by the artificial agents in a way to establish on the one side the control of humans and their safety, and on the other one to allow some smart actions that improve the experience for humans. The goal of all the systems should be of this nature: being useful to the human user. Then the problems falls on how to decide what is good in an environment composed of different people with different goals. This problem is intrinsic in the society, also when no autonomous systems exist. Adding those presences, their function of autonomy must be aligned with the values and rules of the society.

If their autonomy is none, there is no risk for the humans but there may be no advantages at all. With some degrees of freedom, regulamented in the right way (for example always letting human overrides and never hurting anyone), a good improvement can be done.
A good regulamentation by the governments, maybe in a internationalized context, can help defining the boundaries in this dimension. This guideline is very similar to the first two Laws of Robotics~\cite{asimov1942runaround}.

As an example, let us take again the self-driving car. In the majority of the world this technology is not allowed to drive independently. Advanced cruise controls with lane assist can operate only if a driver with a licence is sitting on the driving place, ready to intervene in danger situations. Those kind of rules are necessary because those technology may not be ready and in general can find unexpected situations where they may fail.

However, in some cases there can be conflicts between what the user asks and the principle the system was designed with. This should be the only situation in which the system must disregard a command. If the system decides things on his own, also without conflicting with commands, this can be seen as a threat to human superiority on machines. The autonomy can be seen as menacing the safety and well-being of humans or also material resources~\cite{zlotowski2017can}. It is a hierarchical problem. The humans should stay in control even if the AI becomes smarter. For this reason it would be better if autonomous systems in decisional processes are used only as advisors to empower humans to take smarter decision, not directly deciding on their own.

\subsubsection{Personality and Emotions}
AI can be seen as a threat to humans not only as direct menace, but also in more symbolic and subtle ways. Without establishing a strong discrimination between human and machine, the concept of identity and distinctiveness are threatened~\cite{zlotowski2017can}. Both kinds of threat lead to negative attitudes towards robots and robotics research. But beyond a possible negative attitude towards robotics generated from the recognition of those threats, we want to shortly describe the positive and negative effects on the individuals and society.

We can take as examples the app Replika\footnote{\url{https://replika.ai/}} that resembles the Black Mirror episode ``Be Right Back'' : the goal of the app is to be ``your best friend'', by learning from the conversation how to interact with the specific user and make him happy (rewarding function is the user feedback expressed via text or thumbs buttons).

On the positive effect, having an artificial intelligence with some personality makes interactions more interesting and seamless for users. This makes the user feel more comfortable and can make the system to use emotions to express the vicinity to the user, using the Affective Computing concept~\cite{picard2000affective}.

However, this emotional attachment to machines can cause also negative effects, both on the individual and on society. On the individual those systems can cause addiction\footnote{\url{http://bit.ly/thekernel_coca_apps_designed_addiction}} and isolation. The addiction is caused by the apparent relief given by interacting with something that agrees with things we say and its availability to listen to us. The isolation is the consequence of this addiction, and could be disruptive especially for the youngest ones who, already under the effect of mobile addiction, have a high decrease on empathic abilities~\cite{konrath2011changes} and can sometimes fall in a very problematic isolation situation like \textit{hikikomori}~\cite{furlong2008japanese}. And these changes on the individual and his personality can also affect society, weakening the human relations.

For these reasons, a boundary needs to be defined in order to avoid emotional attachment and other inappropriate feelings towards machines. From the societal point of view, people need to keep their life in real world with real people, and use only the autonomous systems not as targets of happiness but as means to achieve wellness in the human only environment of feelings and emotions.

This dimension of personality should also not be invaded under the light of excessive personalization. Knowing the users' preferences is ethically good if actually the objective is an improvement on his side. Commercial recommender systems should not be creepy for the users. A regulation of what can be collected, analyzed and sold to advertising companies should be imposed in more strict forms that the current ones, to avoid some \textit{Big Brother}-like dystopian scenarios.

These guidelines should put the design of autonomous systems oriented towards the well-being of humans, helping with repetitive tasks and providing powerful services and enhancing the connection between humans. All this should interface in a easy natural way for the humans, using natural language both in spoken and in written forms.

\section{Remainder}
The next Chapter~\ref{soa} gives a deep analysis on the topic of Conversational Agents. Starting from a classification of agents based on the contents of the dialogue (Section~\ref{soaClassification}), a description is given of the available approaches that best fit different contents. Focusing on the Goal-oriented agents, the available Machine Learning approaches are analyzed (Section~\ref{soaNLU}) focusing on Neural Networks (Recurrent ones) that achieve State of the Art condition for the specific goals: sentence classification (intent recognition) and extraction of parameters (slot filling). The chapter ends with an analysis of the personalization techniques (Section~\ref{soaPersonalization}), describing how the user features are used both in content-based and collaborative filtering and what are the available options to avoid the cold start problem.

Chapter~\ref{approach} is divided into two parts. The Section~\ref{approachNLU} describes the chosen approach for managing different interaction environments: the State of the Art approach for single-turn described in~\cite{liu2016attention} is analyzed, and the proposed modifications for multi-turn interactions are exposed in Subsection~\ref{approachMultiTurn}. The Section~\ref{approachPrototype} instead focuses on the specific Chatbot use case: the bot for bike sharing information. The scenarios are delineated (Subsection~\ref{approachScenarios}) and the high level model of the system is described (Subsection~\ref{approachModel}) by also giving the motivation for having two different components that interact. Then a description is given of the intent and entities chosen for a simple interaction model (Subsection~\ref{approachTypes}). On the topic of personalization (Subsection~\ref{approachPersonalization}) two main needs emerge: provide content recommendation under the form of places near the path of the cyclist, and performing a tailored communication with the user in terms of a customized behaviour, personal preferences and different linguistic style. At the end of that subsection, a description of the information retrieval techniques needed as dependencies to the presented approach are given (Subsection~\ref{approachIR}).

Chapter~\ref{implementation} focuses on the implementation of the Bot prototype. This includes the interaction mechanism with chat platforms in Section~\ref{implementationInteraction} and the Neural Network computational graph in Section~\ref{implementationNLU}. The latter describes the exploitation of an online NLU provider in the initial stage of the prototype (Subsection~\ref{implementationWit}), followed by the choice of the Neural Network framework (Subsection~\ref{implementationNN}) and the implementation details (Subsection~\ref{implementationNNDetails}). Two subsections close the discussion: one describes word embeddings (Subsection~\ref{implementationWV}) computed for the Italian language and retrieved for the English one, and the other reports the collection of the single and multi-turn datasets (Subsection~\ref{implementationDatasets}).

Chapter~\ref{validation} contains the evaluation of the system, both for the Natural Language Understanding performance in Section~\ref{validationNLU} and for the prototype in Section~\ref{validationPrototype}. For the NLU, Subsection~\ref{validationDatasets} describes the datasets used both for single-turn and multi-turn interactions, then a description of the measures and the obtained results follows in Subsection~\ref{validationMeasures}, comparing the effects of different choices.

The Chapter~\ref{conclusion} concludes this work by underlying the reached objectives, both as objective results and as acquired competences, and prospecting possible future works.
